<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Secure Databricks cluster with vNet injection and access resources via Azure private endpoint | blacklabnz | Data | DevOps</title><meta name=keywords content="terraform,databricks,private endpoints,databricks secure connectivity,databricks vNet injection,keyvault backed secret scope"><meta name=description content="What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.
This configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.
In this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it."><meta name=author content="Neil Xu"><link rel=canonical href=https://blacklabnz.github.io/posts/databricks-secure/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.807602952edefbeaea61e27642028a93d45bdbd0b38187ce18386944bb940374.css integrity="sha256-gHYClS7e++rqYeJ2QgKKk9Rb29CzgYfOGDhpRLuUA3Q=" rel="preload stylesheet" as=style><link rel=icon href=https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://blacklabnz.github.io/favicon16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blacklabnz.github.io/favicon32x32.png><link rel=mask-icon href=https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous><script async src="https://www.googletagmanager.com/gtag/js?id=G-FDV12HCDKK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-FDV12HCDKK",{anonymize_ip:!1})}</script><meta property="og:title" content="Secure Databricks cluster with vNet injection and access resources via Azure private endpoint"><meta property="og:description" content="What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.
This configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.
In this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it."><meta property="og:type" content="article"><meta property="og:url" content="https://blacklabnz.github.io/posts/databricks-secure/"><meta property="og:image" content="https://blacklabnz.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-28T23:39:13+13:00"><meta property="article:modified_time" content="2022-04-11T00:00:00+00:00"><meta property="og:site_name" content="blacklabnz"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blacklabnz.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Secure Databricks cluster with vNet injection and access resources via Azure private endpoint"><meta name=twitter:description content="What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.
This configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.
In this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blacklabnz.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Secure Databricks cluster with vNet injection and access resources via Azure private endpoint","item":"https://blacklabnz.github.io/posts/databricks-secure/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Secure Databricks cluster with vNet injection and access resources via Azure private endpoint","name":"Secure Databricks cluster with vNet injection and access resources via Azure private endpoint","description":"What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.\nThis configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.\nIn this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it.","keywords":["terraform","databricks","private endpoints","databricks secure connectivity","databricks vNet injection","keyvault backed secret scope"],"articleBody":"What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.\nThis configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.\nIn this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it. For all infra related deployment I am going to use Terraform as much as possible to avoid any “ClickOps” efforts. Once Infra is done, we will run a quick notebook to validate the connectivity as well as DNS resolution\nPart 1 Overall Architecture of the lab In this lab we will use databricks with secured connectivity and vNet injection to access some data residing in the ADLS with private endpoint configuration. We will also create keyvault for keeping the secrets involved using private endpoint. In the end we will use a simply python notebook to validate two things against the private Ip of the private endpoint:\n the network connectivity DNS resolution  As revealed on the diagram, we are simulating a well adopted hub and spoke topology. In our case we don’t have any hub for this lab, but two spokes that connects to each other via vNet peering. For the storage account private endpoint, well, we could have deployed that into the databricks vNet and they would naturally have connectivity(and secured by NSG if needed). But that removes the fun of dealing with Networking and sometime there are enough reason on the design to keep the private endpoint virtual network separate from other virtual network. We will also create keyvault with private endpoint and use that for a KV backed secret scope in databricks. In a nutshell a secret scope is a way to securely store secrets such as service principal key or a storage account key or any other type of secrets as such, so that we do hard code and secrets but are able to use “dbutil” to retrieve the secrets in a notebook run. Simple but yet important principle right ? no hard coded secret in your source code !!!!\nProject setup ├── README.md ├── data.tf ├── main.tf ├── output.tf ├── providers.tf ├── values.tfvars ├── variables.tf └── .gitignore  For simplifying our demo, we are going to put all our terraform resources in the main.tf file. In real world development, the best practice would be modularize any components that are reusable. We are also simplifying the terraform variables with locals, you will find the only the authentication related values are kept in variables and passed in with values.tfvars file.\nFor your convenience, you can download tf module required for this lab from my github repo: secure-databricks-terraform\nFirst thing before carry on to the next part is to setup the providers.\n providers.tf\n terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"2.97.0\" } databricks = { source = \"databrickslabs/databricks\" version = \"0.5.1\" } external = { source = \"hashicorp/external\" version = \"2.2.2\" } } } provider \"azurerm\" { features {} subscription_id = var.subscription_id client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id } provider \"databricks\" { host = format(\"https://%s\", data.azurerm_databricks_workspace.dbr.workspace_url) azure_workspace_resource_id = data.azurerm_databricks_workspace.dbr.id azure_client_id = var.client_id azure_client_secret = var.client_secret azure_tenant_id = var.tenant_id }  Quick explanation: line 16 to line 19 we use service principal to authenticate to azure, in here we use variables to represent the value so that later on we can specify the values.tfvars file and pass the value in. This is really handy if you need to deal with multiple Azure environment that has different auth service principals. Refer the official doc here.\nTo be able to use variables we will need to add the following to the variables file:\n variable.tf\n variable \"subscription_id\" { type = string default = \"default\" } variable \"client_id\" { type = string default = \"default\" } variable \"client_secret\" { type = string default = \"default\" } variable \"tenant_id\" { type = string default = \"default\" }  line 23 to 27 similar deal here. One thing to note is that we are retrieving the databricks workspace url and resource id by using tf data object. The following needs to be added in the data.tf to achieve this.\n data.tf\n data \"azurerm_databricks_workspace\" \"dbr\" { name = azurerm_databricks_workspace.dbr.name resource_group_name = local.rg }  Lastly you will need to create a service principal that has contributor access to the subscription that you are about to deploy to. You can find more details in this office doc. Once Service principal is created you will need to create a secret and use that alone side with the application id to authentication to Azure. When finished, please create a values.tfvars file where you could input those required auth information for terraform to pass in as variables. The information required are:\n subscription id service principal client id service principal client secret azure tenant id.   values.tfvars\n subscription_id = \"xxxxxxxxx\" client_id = \"xxxxxxxxx\" client_secret = \"xxxxxxxxx\" tenant_id = \"xxxxxxxxx\"  At this point we will need to initiate our terraform workspace by running:\nterraform init  With that all set, lets move on to next part of creating actual resources.\nPart 2 Virtual networks and peering First we will add some variables to main.tf that we can use in the main file to the locals block\n main.tf\n locals { org = \"blk\" rg = \"${local.org}-secure-dbr\" rg_location = \"australiaeast\" vnet_dbr = \"${local.org}-spoke-dbr\" vnet_resource = \"${local.org}-spoke-resource\" vnet_dbr_address_space = \"10.0.0.0/16\" vnet_dbr_subnet_prv = \"private\" vnet_dbr_subnet_prv_prefix = \"10.0.1.0/24\" vnet_dbr_subnet_pub = \"public\" vnet_dbr_subnet_pub_prefix = \"10.0.2.0/24\" vnet_resource_address_space = \"10.1.0.0/16\" vnet_resource_subnet_stor = \"stor\" vnet_resource_subnet_stor_prefix = \"10.1.1.0/24\" vnet_resource_subnet_kv = \"keyvault\" vnet_resource_subnet_kv_prefix = \"10.1.2.0/24\" }  Line 2 to line 6 defines some local variables that we can reuse to build up the name of vNets. Line 8 to 14 defines the databricks vNet. Line 16 to 22 defines the resource vNet which is used by storage account and keyvault.\n2.1 Spoke vNet for databricks Pretty standard configuration here, one thing to note is that the subnets name is dictated to be private and public. There are also requirements to the minimum size of the subnets, please refere to this doc for more details.\nAll we need to do is to add the following to main.tf next to the locals block.\n main.tf\n resource \"azurerm_resource_group\" \"secure_dbr\" { name = local.rg location = local.rg_location } resource \"azurerm_virtual_network\" \"vnet_dbr\" { name = local.vnet_dbr location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name address_space = [local.vnet_dbr_address_space] depends_on = [azurerm_resource_group.secure_dbr] } resource \"azurerm_subnet\" \"dbr_prv\" { name = local.vnet_dbr_subnet_prv resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name address_prefixes = [local.vnet_dbr_subnet_prv_prefix] delegation { name = \"dbr_prv_dlg\" service_delegation { name = \"Microsoft.Databricks/workspaces\" actions = [ \"Microsoft.Network/virtualNetworks/subnets/join/action\", \"Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\", \"Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action\", ] } } depends_on = [azurerm_virtual_network.vnet_dbr] } resource \"azurerm_subnet\" \"dbr_pub\" { name = local.vnet_dbr_subnet_pub resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name address_prefixes = [local.vnet_dbr_subnet_pub_prefix] delegation { name = \"dbr_pub_dlg\" service_delegation { name = \"Microsoft.Databricks/workspaces\" actions = [ \"Microsoft.Network/virtualNetworks/subnets/join/action\", \"Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\", \"Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action\", ] } } depends_on = [azurerm_virtual_network.vnet_dbr] }  Quick explanation: Line 1-4 defines the resource group. Line 6-13 defines the dbr vNet. Line 15-53 defines the private and public subnets for the clusters.  Note in each of the subnet there is subnet delegation properties that needs to be added. The delegation block is not mandatory as Azure will take care of that, but if you don't add that, next time when you do terraform plan, the delegation property will be recognized as change even though there is nothing changed. \n2.2 Spoke vNet for storage account and keyvault Nothing complicated here, just need a separate subnets for storage account and keyvault private endpoints. Add the following to main.tf file.\nresource \"azurerm_virtual_network\" \"vnet_resource\" { name = local.vnet_resource location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name address_space = [local.vnet_resource_address_space] depends_on = [azurerm_resource_group.secure_dbr] } resource \"azurerm_subnet\" \"resource_stor\" { name = local.vnet_resource_subnet_stor resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name address_prefixes = [local.vnet_resource_subnet_stor_prefix] enforce_private_link_endpoint_network_policies = true depends_on = [azurerm_virtual_network.vnet_resource] } resource \"azurerm_subnet\" \"resource_kv\" { name = local.vnet_resource_subnet_kv resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name address_prefixes = [local.vnet_resource_subnet_kv_prefix] enforce_private_link_endpoint_network_policies = true depends_on = [azurerm_virtual_network.vnet_resource] }  Quick explanation Note the difference on the subnet compare to the above. Subnets provisioned for storage account and keyvault does not require subnet delegation. However it need the “Private endpoint network policy” to be enabled before any private endpoint can be deployed in it.\n2.3 vNet peering Virtual network peering ensures the connectivity between two virtual networks. In some environments direct spoke to spoke communication via peering is disable, all traffic between vNet peers goes via hub network, filtered and processed by firewall. However purpose of our lab is to focus on Databricks and private endpoints, we will use peering, a simpler setup to achieve spoke to spoke connectivity. I will write a separate article to talk about best practice of Hub-Spoke topology.\nresource \"azurerm_virtual_network_peering\" \"dbr_to_resource\" { name = \"dbr-vent-to-resource-vnet\" resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name remote_virtual_network_id = azurerm_virtual_network.vnet_resource.id } resource \"azurerm_virtual_network_peering\" \"resource_to_dbr\" { name = \"resource-vent-to-dbr-vnet\" resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name remote_virtual_network_id = azurerm_virtual_network.vnet_dbr.id }  Quick explanation The vNet peering needs to be setup from both directions, hence you find that there are two peering blocks required to achieve this.\nOnce you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars  followed by:\nterraform apply -auto-approve -var-file=values.tfvars  Let quickly go to the portal and validate that our vNets are created and peering is all connected. Part 3 Databricks workspace and cluster Lets create Databricks workspace and clusters in this part.\n3.1 Databricks secure connectivity + vNet injection To remove the exposure to public internet traffic, clusters can be deployed with no-pubip configuration and deployed into pre-defined vNet. First need to add the following to locals block:\ndbr = \"${local.org}-secure-dbr\" dbr_sku = \"premium\" dbr_mgmt_rg = \"${local.dbr}-mgmt-rg\" dbr_nsg = \"${local.org}-nsg\" dbr_cluster = \"${local.org}-cluster\"  Then add the following to main.tf\nresource \"azurerm_network_security_group\" \"dbr_nsg\" { name = local.dbr_nsg location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name } resource \"azurerm_subnet_network_security_group_association\" \"dbr_prv\" { subnet_id = azurerm_subnet.dbr_prv.id network_security_group_id = azurerm_network_security_group.dbr_nsg.id } resource \"azurerm_subnet_network_security_group_association\" \"dbr_pub\" { subnet_id = azurerm_subnet.dbr_pub.id network_security_group_id = azurerm_network_security_group.dbr_nsg.id } resource \"azurerm_databricks_workspace\" \"dbr\" { name = local.dbr resource_group_name = local.rg location = local.rg_location sku = local.dbr_sku managed_resource_group_name = local.dbr_mgmt_rg custom_parameters { no_public_ip = true virtual_network_id = azurerm_virtual_network.vnet_dbr.id public_subnet_name = azurerm_subnet.dbr_pub.name private_subnet_name = azurerm_subnet.dbr_prv.name public_subnet_network_security_group_association_id = azurerm_subnet_network_security_group_association.dbr_pub.id private_subnet_network_security_group_association_id = azurerm_subnet_network_security_group_association.dbr_prv.id } }  Quick explanation Line 1-15 defines the network security group and the association, this is mandatory for databricks cluster vNet injection. Line 17-34 defines the databricks workspace with existing vNet and subnets. Line 26 configures databricks cluster to have not public IP.  Please customize your NSG if you are using this in higher environments, current configuration is minimal so that we could focus on the lab. Please always go with Security first principle. \n3.2 Cluster creation We will use terraform to create clusters, note we will need to add databricks provider to terraform which was already performed in earlier steps.\nresource \"databricks_cluster\" \"cluster\" { cluster_name = local.dbr_cluster spark_version = data.databricks_spark_version.latest.id node_type_id = data.databricks_node_type.smallest.id autotermination_minutes = 20 spark_conf = { \"spark.databricks.cluster.profile\" : \"singleNode\" \"spark.master\" : \"local[*]\" } custom_tags = { \"ResourceClass\" = \"SingleNode\" } }  you will also need to add the following to the data.tf file so that databricks provide can retrieve some information dynamically for the provisioning.\n data.tf\n data \"databricks_spark_version\" \"latest\" {} data \"databricks_node_type\" \"smallest\" { local_disk = true }  Quick explanation The cluster we provisioned is single cluster which is capable enough to help to complete the lab.\nOnce you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars  followed by:\nterraform apply -auto-approve -var-file=values.tfvars  Lets login to our databricks instance and validate the cluster is in place. Part 4 ADLS + Keyvault with Private endpoint Lets create ADLS account and Keyvault in this part with private endpoints and private DNS zones. private DNS zone is required so that a Azure private link DNS address could be resolved to the private IP address for the private endpoint.\n4.1 Storage account with private endpoint and private DNS zone This part shouldnt be too bad, a storage account with private endpoint deployed to the spoke network.\nFirst need to add the following to the locals block:\nstor = \"${local.org}stor\" stor_pe = \"${local.stor}pe\" stor_prv_con = \"${local.stor}prvcon\" kv = \"${local.org}-kv\" kv_pe = \"${local.kv}pe\" kv_prv_con = \"${local.kv}prvcon\"  Then add the following to main.tf file：\nresource \"azurerm_storage_account\" \"stor\" { name = local.stor resource_group_name = local.rg location = local.rg_location account_tier = \"Standard\" account_replication_type = \"LRS\" is_hns_enabled = true network_rules { default_action = \"Deny\" ip_rules = [data.external.my_ip.result.ip] } } resource \"azurerm_storage_container\" \"container\" { name = \"land\" storage_account_name = azurerm_storage_account.stor.name container_access_type = \"private\" } resource \"azurerm_private_endpoint\" \"stor_pe\" { name = local.stor_pe location = local.rg_location resource_group_name = local.rg subnet_id = azurerm_subnet.resource_stor.id private_service_connection { name = local.stor_prv_con private_connection_resource_id = azurerm_storage_account.stor.id is_manual_connection = false subresource_names = [\"dfs\"] } } resource \"azurerm_private_dns_zone\" \"stor_dfs\" { name = \"privatelink.dfs.core.windows.net\" resource_group_name = local.rg } resource \"azurerm_private_dns_zone_virtual_network_link\" \"dbr_vnet_link_stor\" { name = \"dbr_vnet_link\" resource_group_name = local.rg private_dns_zone_name = azurerm_private_dns_zone.stor_dfs.name virtual_network_id = azurerm_virtual_network.vnet_dbr.id } resource \"azurerm_private_dns_a_record\" \"storpe_dns\" { name = local.stor zone_name = azurerm_private_dns_zone.stor_dfs.name resource_group_name = local.rg ttl = 300 records = [azurerm_private_endpoint.stor_pe.private_service_connection.0.private_ip_address] }  Quick explanation Line 11-14 defined the firewall rules so that we are not open our storage account to public by default. Note the “data.external.my_ip.result.ip”, because we still want to be operate the storage account from out local machine hence the data block retrieve your current public ip and inject that to network policy. You will also need to add the following to your data.tf file：\ndata \"external\" \"my_ip\" { program = [\"curl\", \"https://api.ipify.org?format=json\"] }  Line 23-35 created the private endpoint targeting the storage account resource. Line 37-47 creates a Azure private dns zone for ADLS, and link that to the dbr network, so that DNS request from the dbr network could use this for name resolution. Line 38 specifies the top level domain name for the storage account in the private DNZ zone. Line 49-55 creates a private DNS record using the private ip address of the network interface as part of private endpoint provisioning.  Note when creating private endpoint, there will be a network interface attached to it which gets a private ip address allocated in the subnet range. \n4.2 Keyvault with private endpoint and private DNS zone This one is not too bad either, keyvualt with private endpoint deployed to the same spoke network with similar setup on the private DNS zone, vNet linkage and dns records\nresource \"azurerm_key_vault\" \"kv\" { name = local.kv location = local.rg_location resource_group_name = local.rg enabled_for_disk_encryption = true tenant_id = data.azurerm_client_config.current.tenant_id soft_delete_retention_days = 7 purge_protection_enabled = false sku_name = \"standard\" network_acls { default_action = \"Deny\" bypass = \"AzureServices\" ip_rules = [data.external.my_ip.result.ip] } } resource \"azurerm_private_endpoint\" \"kv_pe\" { name = local.kv_pe location = local.rg_location resource_group_name = local.rg subnet_id = azurerm_subnet.resource_kv.id private_service_connection { name = local.kv_prv_con private_connection_resource_id = azurerm_key_vault.kv.id is_manual_connection = false subresource_names = [\"Vault\"] } } resource \"azurerm_private_dns_zone\" \"kv\" { name = \"privatelink.vaultcore.azure.net\" resource_group_name = local.rg } resource \"azurerm_private_dns_zone_virtual_network_link\" \"dbr_vnet_link_kv\" { name = \"dbr_vnet_link_kv\" resource_group_name = local.rg private_dns_zone_name = azurerm_private_dns_zone.kv.name virtual_network_id = azurerm_virtual_network.vnet_dbr.id } resource \"azurerm_private_dns_a_record\" \"kvpe_dns\" { name = local.kv zone_name = azurerm_private_dns_zone.kv.name resource_group_name = local.rg ttl = 300 records = [azurerm_private_endpoint.kv_pe.private_service_connection.0.private_ip_address] }  Once you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars  followed by:\nterraform apply -auto-approve -var-file=values.tfvars  Part 5 Databricks configuration - Keyvault backed secret scope To create the keyvault backed secret scope, terraform provides a handy resource to do this. add the following to the main.tf files to achieve this.\nresource \"databricks_secret_scope\" \"kv\" { name = \"keyvault-managed\" keyvault_metadata { resource_id = azurerm_key_vault.kv.id dns_name = azurerm_key_vault.kv.vault_uri } }  The tricky part here is that according to terraform this is only supported by azure cli authentication but NOT with service principal authentication. You will see error like this if service principal auth is used: what you will need to do is to comment out the azurerm authentication part in the providers.tf and then login to Azure using\naz login  If you haven’t installed Az cli following this to install and read more details of this limitation here.\nPart 6 Run notebook to validate Let put some python code in and test the connectivity and DNS resolutions. The first and most important task is to validate the connectivity and DNS resolution. You can run this code snippet in a notebook to check them. 1 2 3 4 5  import socket stor = socket.gethostbyname_ex(\"blkstor.dfs.core.windows.net\") print (\"\\n\\nThe IP Address of the Domain Name is: \" + repr(stor)) kv = socket.gethostbyname_ex(\"blk-kv.vault.azure.net\") print (\"\\n\\nThe IP Address of the Domain Name is: \" + repr(kv))  \nIf all setup correctly you should be able to see that the DNS name is resolved with a private IP address. If you are interest in further testing to read some file from ADLS you could use the following snippet to do so.\ndbutils.widgets.text(\"adls_name\", \"blkstor\", \"adls_name\") dbutils.widgets.text(\"adls_container\", \"land\", \"adls_container\") # COMMAND ---------- adls_name = dbutils.widgets.get(\"adls_name\") adls_container = dbutils.widgets.get(\"adls_container\") # COMMAND ---------- spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\") spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") spark.conf.set(\"fs.azure.account.oauth2.client.id\", dbutils.secrets.get(scope=\"keyvault-managed\",key=\"sp-id\")) spark.conf.set(\"fs.azure.account.oauth2.client.secret\", dbutils.secrets.get(scope=\"keyvault-managed\",key=\"sp-secret\")) spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"https://login.microsoftonline.com/xxxx-xxx-xxxxxx-xxxxxx/oauth2/token\") # COMMAND ---------- df = spark.read.json(f\"abfss://{adls_container}@{adls_name}.dfs.core.windows.net/pubapis.json\") # COMMAND ---------- df.show()  What this notebook does is quite simple, it grabs the secret of a service principal from keyvault secret scope and use that to auth to Azure. Once authorized it will read a json file into dataframe. Please seethe outcome of the notebook run in the following screen shot. To be able to achieve this, you will need the following:\n another service principal that has the role “Storage blob contributor” on the storage account having the application id and secret saved in the Keyvault we created as “sp-id” and “sp-secret”  I have included these two notebooks in the repo, you will find them in the notebooks folder.\nTo deploy the notebooks to our databricks workspace, you could add the following to your main.tf file as part of the deployment.\nresource \"databricks_notebook\" \"notbooks\" { for_each = fileset(\"${path.module}/notebooks\", \"*\") source = \"${path.module}/notebooks/${each.key}\" path = \"/validation/${element(split(\".\", each.key), 0)}\" language = \"PYTHON\" }  This block iterate through the notebooks folder and deploy the notebook in a folder called “validated” in the workspace.  Note usually you would not deploy your notebooks as part of IaC efforts, these would otherwise be regarded as the \"application\" artifacts. But for simplicity of our demo, they are deployed in the same IaC code flow. \nAt this point our lab has reached the end, hope you enjoyed the extra efforts needed to make your databricks cluster more secure.\nIf like the content please leave your comments below, if you fine issues with the content, please also comment below. Thanks for your time and patience with me !!\n","wordCount":"3078","inLanguage":"en","datePublished":"2022-03-28T23:39:13+13:00","dateModified":"2022-04-11T00:00:00Z","author":{"@type":"Person","name":"Neil Xu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blacklabnz.github.io/posts/databricks-secure/"},"publisher":{"@type":"Organization","name":"blacklabnz | Data | DevOps","logo":{"@type":"ImageObject","url":"https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><span class=header__inner><span class=logo><a href=https://blacklabnz.github.io/ accesskey=h title="blacklabnz (Alt + H)"><span><i class="fas fa-terminal fa-sm"></i></span>
<span class=logo_text>blacklabnz</span>
<span id=cursor></span></a></span>
<span class=header__right><div class=dropdown><span class=dropbtn><i class="fas fa-bars"></i></span><nav id=dropdonwBox class=dropdown-content><ul id=menu-mobile><li><a href=https://blacklabnz.github.io/posts title=blogs><span>blogs</span></a></li><li><a href=https://blacklabnz.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://blacklabnz.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://blacklabnz.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav><script src=/assets/js/menu.min.js></script></div><nav class=nav><ul id=menu><li><a href=https://blacklabnz.github.io/posts title=blogs><span>blogs</span></a></li><li><a href=https://blacklabnz.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://blacklabnz.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://blacklabnz.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav><ul class=logo-switches></ul><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></span></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blacklabnz.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://blacklabnz.github.io/posts/>Posts</a></div><h1 class=post-title>Secure Databricks cluster with vNet injection and access resources via Azure private endpoint</h1><div class=post-meta><span title="2022-03-28 23:39:13 +1300 +1300">Created March 28, 2022</span>&nbsp;·&nbsp;Updated Apr 11, 2022&nbsp;·&nbsp;15 min&nbsp;·&nbsp;Neil Xu&nbsp;|&nbsp;<a href=https://github.com/blacklabnz/blacklabnz.github.io/blob/main/content/posts/databricks-secure.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#part-1-overall-architecture-of-the-lab aria-label="Part 1 Overall Architecture of the lab">Part 1 Overall Architecture of the lab</a><ul><li><a href=#project-setup aria-label="Project setup">Project setup</a></li><li><a href=#quick-explanation aria-label="Quick explanation:">Quick explanation:</a></li></ul></li><li><a href=#part-2-virtual-networks-and-peering aria-label="Part 2 Virtual networks and peering">Part 2 Virtual networks and peering</a><ul><li><a href=#21-spoke-vnet-for-databricks aria-label="2.1 Spoke vNet for databricks">2.1 Spoke vNet for databricks</a></li><li><a href=#quick-explanation-1 aria-label="Quick explanation:">Quick explanation:</a></li><li><a href=#22-spoke-vnet-for-storage-account-and-keyvault aria-label="2.2 Spoke vNet for storage account and keyvault">2.2 Spoke vNet for storage account and keyvault</a></li><li><a href=#quick-explanation-2 aria-label="Quick explanation">Quick explanation</a></li><li><a href=#23-vnet-peering aria-label="2.3 vNet peering">2.3 vNet peering</a></li><li><a href=#quick-explanation-3 aria-label="Quick explanation">Quick explanation</a></li></ul></li><li><a href=#part-3-databricks-workspace-and-cluster aria-label="Part 3 Databricks workspace and cluster">Part 3 Databricks workspace and cluster</a><ul><li><a href=#31-databricks-secure-connectivity--vnet-injection aria-label="3.1 Databricks secure connectivity + vNet injection">3.1 Databricks secure connectivity + vNet injection</a></li><li><a href=#quick-explanation-4 aria-label="Quick explanation">Quick explanation</a></li><li><a href=#32-cluster-creation aria-label="3.2 Cluster creation">3.2 Cluster creation</a></li><li><a href=#quick-explanation-5 aria-label="Quick explanation">Quick explanation</a></li></ul></li><li><a href=#part-4-adls--keyvault-with-private-endpoint aria-label="Part 4 ADLS + Keyvault with Private endpoint">Part 4 ADLS + Keyvault with Private endpoint</a><ul><li><a href=#41-storage-account-with-private-endpoint-and-private-dns-zone aria-label="4.1 Storage account with private endpoint and private DNS zone">4.1 Storage account with private endpoint and private DNS zone</a></li><li><a href=#quick-explanation-6 aria-label="Quick explanation">Quick explanation</a></li><li><a href=#42-keyvault-with-private-endpoint-and-private-dns-zone aria-label="4.2 Keyvault with private endpoint and private DNS zone">4.2 Keyvault with private endpoint and private DNS zone</a></li></ul></li><li><a href=#part-5-databricks-configuration---keyvault-backed-secret-scope aria-label="Part 5 Databricks configuration - Keyvault backed secret scope">Part 5 Databricks configuration - Keyvault backed secret scope</a></li><li><a href=#part-6-run-notebook-to-validate aria-label="Part 6 Run notebook to validate">Part 6 Run notebook to validate</a></li></ul></div></details></div><div class=post-content><p>What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.</p><p>This configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.</p><p>In this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it.
For all infra related deployment I am going to use Terraform as much as possible to avoid any &ldquo;ClickOps&rdquo; efforts. Once Infra is done, we will run a quick notebook to validate the connectivity as well as DNS resolution</p><h2 id=part-1-overall-architecture-of-the-lab>Part 1 Overall Architecture of the lab<a hidden class=anchor aria-hidden=true href=#part-1-overall-architecture-of-the-lab>#</a></h2><p><img loading=lazy src=/posts/databricks-secure/dbr-secure.png alt=architecture>
In this lab we will use databricks with <a href=https://docs.microsoft.com/en-us/azure/databricks/security/secure-cluster-connectivity>secured connectivity and vNet injection</a> to access some data residing in the ADLS with <a href=https://docs.microsoft.com/en-us/azure/storage/common/storage-private-endpoints>private endpoint</a> configuration.
We will also create keyvault for keeping the secrets involved using private endpoint.
In the end we will use a simply python notebook to validate two things against the private Ip of the private endpoint:</p><ol><li>the network connectivity</li><li>DNS resolution</li></ol><p>As revealed on the diagram, we are simulating a well adopted hub and spoke topology. In our case we don&rsquo;t have any hub for this lab, but two spokes that connects to each other via vNet peering.
For the storage account private endpoint, well, we could have deployed that into the databricks vNet and they would naturally have connectivity(and secured by NSG if needed). But that removes the fun of dealing with Networking and sometime there are enough reason on the design to keep the private endpoint virtual network separate from other virtual network. We will also create keyvault with private endpoint and use that for a <a href=https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes>KV backed secret scope</a> in databricks. In a nutshell a secret scope is a way to securely store secrets such as service principal key or a storage account key or any other type of secrets as such, so that we do hard code and secrets but are able to use &ldquo;dbutil&rdquo; to retrieve the secrets in a notebook run. Simple but yet important principle right ? no hard coded secret in your source code !!!!</p><h3 id=project-setup>Project setup<a hidden class=anchor aria-hidden=true href=#project-setup>#</a></h3><pre><code class=language-bash>├── README.md
├── data.tf
├── main.tf
├── output.tf
├── providers.tf
├── values.tfvars
├── variables.tf
└── .gitignore
</code></pre><p>For simplifying our demo, we are going to put all our terraform resources in the main.tf file. In real world development, the best practice would be modularize any components that are reusable.
We are also simplifying the terraform variables with locals, you will find the only the authentication related values are kept in variables and passed in with values.tfvars file.</p><p>For your convenience, you can download tf module required for this lab from my github repo: <a href=https://github.com/blacklabnz/secure-databricks-terraform>secure-databricks-terraform</a></p><p>First thing before carry on to the next part is to setup the providers.</p><blockquote><p>providers.tf</p></blockquote><pre><code class=language-bash>terraform {
  required_providers {
    azurerm = {
      source  = &quot;hashicorp/azurerm&quot;
      version = &quot;2.97.0&quot;
    }
    databricks = {
      source  = &quot;databrickslabs/databricks&quot;
      version = &quot;0.5.1&quot;
    }
    external = {
      source  = &quot;hashicorp/external&quot;
      version = &quot;2.2.2&quot;
    }
  }
}

provider &quot;azurerm&quot; {
  features {}
  subscription_id = var.subscription_id
  client_id       = var.client_id
  client_secret   = var.client_secret
  tenant_id       = var.tenant_id
}

provider &quot;databricks&quot; {
  host                        = format(&quot;https://%s&quot;, data.azurerm_databricks_workspace.dbr.workspace_url)
  azure_workspace_resource_id = data.azurerm_databricks_workspace.dbr.id
  azure_client_id             = var.client_id
  azure_client_secret         = var.client_secret
  azure_tenant_id             = var.tenant_id
}
</code></pre><h3 id=quick-explanation>Quick explanation:<a hidden class=anchor aria-hidden=true href=#quick-explanation>#</a></h3><p>line 16 to line 19 we use service principal to authenticate to azure, in here we use variables to represent the value so that later on we can specify the values.tfvars file and pass the value in. This is really handy if you need to deal with multiple Azure environment that has different auth service principals. Refer the official doc <a href=https://www.terraform.io/language/values/variables>here</a>.</p><p>To be able to use variables we will need to add the following to the variables file:</p><blockquote><p>variable.tf</p></blockquote><pre><code class=language-bash>variable &quot;subscription_id&quot; {
  type = string
  default = &quot;default&quot;
} 
variable &quot;client_id&quot; {
  type = string
  default = &quot;default&quot;
}
variable &quot;client_secret&quot; {
  type = string
  default = &quot;default&quot;
}   
variable &quot;tenant_id&quot; {
  type = string
  default = &quot;default&quot;
}       
</code></pre><p>line 23 to 27 similar deal here. One thing to note is that we are retrieving the databricks workspace url and resource id by using tf data object. The following needs to be added in the data.tf to achieve this.</p><blockquote><p>data.tf</p></blockquote><pre><code class=language-bash>data &quot;azurerm_databricks_workspace&quot; &quot;dbr&quot; {
  name                = azurerm_databricks_workspace.dbr.name
  resource_group_name = local.rg
}
</code></pre><p>Lastly you will need to create a service principal that has contributor access to the subscription that you are about to deploy to. You can find more details in this office <a href=https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli>doc</a>.
Once Service principal is created you will need to create a secret and use that alone side with the application id to authentication to Azure. When finished, please create a values.tfvars file where you could input those required auth information for terraform to pass in as variables. The information required are:</p><ol><li>subscription id</li><li>service principal client id</li><li>service principal client secret</li><li>azure tenant id.</li></ol><blockquote><p>values.tfvars</p></blockquote><pre><code class=language-bash>subscription_id = &quot;xxxxxxxxx&quot;
client_id       = &quot;xxxxxxxxx&quot;
client_secret   = &quot;xxxxxxxxx&quot;
tenant_id       = &quot;xxxxxxxxx&quot;
</code></pre><p>At this point we will need to initiate our terraform workspace by running:</p><pre><code class=language-bash>terraform init
</code></pre><p>With that all set, lets move on to next part of creating actual resources.</p><h2 id=part-2-virtual-networks-and-peering>Part 2 Virtual networks and peering<a hidden class=anchor aria-hidden=true href=#part-2-virtual-networks-and-peering>#</a></h2><p>First we will add some variables to main.tf that we can use in the main file to the locals block</p><blockquote><p>main.tf</p></blockquote><pre><code class=language-bash>locals {
  org           = &quot;blk&quot;
  rg            = &quot;${local.org}-secure-dbr&quot;
  rg_location   = &quot;australiaeast&quot;
  vnet_dbr      = &quot;${local.org}-spoke-dbr&quot;
  vnet_resource = &quot;${local.org}-spoke-resource&quot;

  vnet_dbr_address_space = &quot;10.0.0.0/16&quot;

  vnet_dbr_subnet_prv        = &quot;private&quot;
  vnet_dbr_subnet_prv_prefix = &quot;10.0.1.0/24&quot;

  vnet_dbr_subnet_pub        = &quot;public&quot;
  vnet_dbr_subnet_pub_prefix = &quot;10.0.2.0/24&quot;

  vnet_resource_address_space = &quot;10.1.0.0/16&quot;

  vnet_resource_subnet_stor        = &quot;stor&quot;
  vnet_resource_subnet_stor_prefix = &quot;10.1.1.0/24&quot;

  vnet_resource_subnet_kv        = &quot;keyvault&quot;
  vnet_resource_subnet_kv_prefix = &quot;10.1.2.0/24&quot;
}
</code></pre><p>Line 2 to line 6 defines some local variables that we can reuse to build up the name of vNets.
Line 8 to 14 defines the databricks vNet.
Line 16 to 22 defines the resource vNet which is used by storage account and keyvault.</p><h3 id=21-spoke-vnet-for-databricks>2.1 Spoke vNet for databricks<a hidden class=anchor aria-hidden=true href=#21-spoke-vnet-for-databricks>#</a></h3><p>Pretty standard configuration here, one thing to note is that the subnets name is dictated to be private and public. There are also requirements to the minimum size of the subnets, please refere to this <a href="https://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject#:~:text=Region%3A%20The%20VNet%20must%20reside,subnet%20and%20a%20host%20subnet.">doc</a> for more details.</p><p>All we need to do is to add the following to main.tf next to the locals block.</p><blockquote><p>main.tf</p></blockquote><pre><code class=language-bash>resource &quot;azurerm_resource_group&quot; &quot;secure_dbr&quot; {
  name     = local.rg
  location = local.rg_location
}

resource &quot;azurerm_virtual_network&quot; &quot;vnet_dbr&quot; {
  name                = local.vnet_dbr
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.secure_dbr.name
  address_space       = [local.vnet_dbr_address_space]

  depends_on = [azurerm_resource_group.secure_dbr]
}

resource &quot;azurerm_subnet&quot; &quot;dbr_prv&quot; {
  name                 = local.vnet_dbr_subnet_prv
  resource_group_name  = local.rg
  virtual_network_name = azurerm_virtual_network.vnet_dbr.name
  address_prefixes     = [local.vnet_dbr_subnet_prv_prefix]

  delegation {
    name = &quot;dbr_prv_dlg&quot;
    service_delegation {
      name = &quot;Microsoft.Databricks/workspaces&quot;
      actions = [
        &quot;Microsoft.Network/virtualNetworks/subnets/join/action&quot;,
        &quot;Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action&quot;,
        &quot;Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action&quot;,
      ]
    }
  }
  depends_on = [azurerm_virtual_network.vnet_dbr]
}

resource &quot;azurerm_subnet&quot; &quot;dbr_pub&quot; {
  name                 = local.vnet_dbr_subnet_pub
  resource_group_name  = local.rg
  virtual_network_name = azurerm_virtual_network.vnet_dbr.name
  address_prefixes     = [local.vnet_dbr_subnet_pub_prefix]

  delegation {
    name = &quot;dbr_pub_dlg&quot;
    service_delegation {
      name = &quot;Microsoft.Databricks/workspaces&quot;
      actions = [
        &quot;Microsoft.Network/virtualNetworks/subnets/join/action&quot;,
        &quot;Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action&quot;,
        &quot;Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action&quot;,
      ]
    }
  }
  depends_on = [azurerm_virtual_network.vnet_dbr]
}
</code></pre><h3 id=quick-explanation-1>Quick explanation:<a hidden class=anchor aria-hidden=true href=#quick-explanation-1>#</a></h3><p>Line 1-4 defines the resource group.
Line 6-13 defines the dbr vNet.
Line 15-53 defines the private and public subnets for the clusters.<div class=Note style=background-color:var(--note);font-size:15px;margin-bottom:10px;margin-top:10px;padding:10px;border-radius:10px><i class="fas fa-exclamation-triangle"></i>
Note in each of the subnet there is subnet delegation properties that needs to be added. The delegation block is not mandatory as Azure will take care of that, but if you don't add that, next time when you do terraform plan, the delegation property will be recognized as change even though there is nothing changed.</div></p><h3 id=22-spoke-vnet-for-storage-account-and-keyvault>2.2 Spoke vNet for storage account and keyvault<a hidden class=anchor aria-hidden=true href=#22-spoke-vnet-for-storage-account-and-keyvault>#</a></h3><p>Nothing complicated here, just need a separate subnets for storage account and keyvault private endpoints. Add the following to main.tf file.</p><pre><code class=language-bash>resource &quot;azurerm_virtual_network&quot; &quot;vnet_resource&quot; {
  name                = local.vnet_resource
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.secure_dbr.name
  address_space       = [local.vnet_resource_address_space]

  depends_on = [azurerm_resource_group.secure_dbr]
}

resource &quot;azurerm_subnet&quot; &quot;resource_stor&quot; {
  name                                           = local.vnet_resource_subnet_stor
  resource_group_name                            = local.rg
  virtual_network_name                           = azurerm_virtual_network.vnet_resource.name
  address_prefixes                               = [local.vnet_resource_subnet_stor_prefix]
  enforce_private_link_endpoint_network_policies = true

  depends_on = [azurerm_virtual_network.vnet_resource]
}

resource &quot;azurerm_subnet&quot; &quot;resource_kv&quot; {
  name                                           = local.vnet_resource_subnet_kv
  resource_group_name                            = local.rg
  virtual_network_name                           = azurerm_virtual_network.vnet_resource.name
  address_prefixes                               = [local.vnet_resource_subnet_kv_prefix]
  enforce_private_link_endpoint_network_policies = true

  depends_on = [azurerm_virtual_network.vnet_resource]
}
</code></pre><h3 id=quick-explanation-2>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-2>#</a></h3><p>Note the difference on the subnet compare to the above. Subnets provisioned for storage account and keyvault does not require subnet delegation. However it need the &ldquo;Private endpoint network policy&rdquo; to be enabled before any private endpoint can be deployed in it.</p><h3 id=23-vnet-peering>2.3 vNet peering<a hidden class=anchor aria-hidden=true href=#23-vnet-peering>#</a></h3><p>Virtual network peering ensures the connectivity between two virtual networks. In some environments direct spoke to spoke communication via peering is disable, all traffic between vNet peers goes via hub network, filtered and processed by firewall. However purpose of our lab is to focus on Databricks and private endpoints, we will use peering, a simpler setup to achieve spoke to spoke connectivity. I will write a separate article to talk about best practice of Hub-Spoke topology.</p><pre><code class=language-bash>resource &quot;azurerm_virtual_network_peering&quot; &quot;dbr_to_resource&quot; {
  name                      = &quot;dbr-vent-to-resource-vnet&quot;
  resource_group_name       = local.rg
  virtual_network_name      = azurerm_virtual_network.vnet_dbr.name
  remote_virtual_network_id = azurerm_virtual_network.vnet_resource.id
}

resource &quot;azurerm_virtual_network_peering&quot; &quot;resource_to_dbr&quot; {
  name                      = &quot;resource-vent-to-dbr-vnet&quot;
  resource_group_name       = local.rg
  virtual_network_name      = azurerm_virtual_network.vnet_resource.name
  remote_virtual_network_id = azurerm_virtual_network.vnet_dbr.id
}
</code></pre><h3 id=quick-explanation-3>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-3>#</a></h3><p>The vNet peering needs to be setup from both directions, hence you find that there are two peering blocks required to achieve this.</p><p>Once you have all the code in, lets create resources in a iterative fashion by running</p><pre><code class=language-bash>terraform plan -var-file=values.tfvars
</code></pre><p>followed by:</p><pre><code class=language-bash>terraform apply -auto-approve -var-file=values.tfvars
</code></pre><p>Let quickly go to the portal and validate that our vNets are created and peering is all connected.
<img loading=lazy src=/posts/databricks-secure/vNet.png alt=vNet></p><h2 id=part-3-databricks-workspace-and-cluster>Part 3 Databricks workspace and cluster<a hidden class=anchor aria-hidden=true href=#part-3-databricks-workspace-and-cluster>#</a></h2><p>Lets create Databricks workspace and clusters in this part.</p><h3 id=31-databricks-secure-connectivity--vnet-injection>3.1 Databricks secure connectivity + vNet injection<a hidden class=anchor aria-hidden=true href=#31-databricks-secure-connectivity--vnet-injection>#</a></h3><p>To remove the exposure to public internet traffic, clusters can be deployed with no-pubip configuration and deployed into pre-defined vNet.
First need to add the following to locals block:</p><pre><code class=language-bash>dbr         = &quot;${local.org}-secure-dbr&quot;
dbr_sku     = &quot;premium&quot;
dbr_mgmt_rg = &quot;${local.dbr}-mgmt-rg&quot;
dbr_nsg     = &quot;${local.org}-nsg&quot;
dbr_cluster = &quot;${local.org}-cluster&quot;
</code></pre><p>Then add the following to main.tf</p><pre><code class=language-bash>resource &quot;azurerm_network_security_group&quot; &quot;dbr_nsg&quot; {
  name                = local.dbr_nsg
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.secure_dbr.name
}

resource &quot;azurerm_subnet_network_security_group_association&quot; &quot;dbr_prv&quot; {
  subnet_id                 = azurerm_subnet.dbr_prv.id
  network_security_group_id = azurerm_network_security_group.dbr_nsg.id
}

resource &quot;azurerm_subnet_network_security_group_association&quot; &quot;dbr_pub&quot; {
  subnet_id                 = azurerm_subnet.dbr_pub.id
  network_security_group_id = azurerm_network_security_group.dbr_nsg.id
}

resource &quot;azurerm_databricks_workspace&quot; &quot;dbr&quot; {
  name                = local.dbr
  resource_group_name = local.rg
  location            = local.rg_location
  sku                 = local.dbr_sku

  managed_resource_group_name = local.dbr_mgmt_rg

  custom_parameters {
    no_public_ip        = true
    virtual_network_id  = azurerm_virtual_network.vnet_dbr.id
    public_subnet_name  = azurerm_subnet.dbr_pub.name
    private_subnet_name = azurerm_subnet.dbr_prv.name

    public_subnet_network_security_group_association_id  = azurerm_subnet_network_security_group_association.dbr_pub.id
    private_subnet_network_security_group_association_id = azurerm_subnet_network_security_group_association.dbr_prv.id
  }
}
</code></pre><h3 id=quick-explanation-4>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-4>#</a></h3><p>Line 1-15 defines the network security group and the association, this is mandatory for databricks cluster vNet injection.
Line 17-34 defines the databricks workspace with existing vNet and subnets.
Line 26 configures databricks cluster to have not public IP.<div class=Note style=background-color:var(--note);font-size:15px;margin-bottom:10px;margin-top:10px;padding:10px;border-radius:10px><i class="fas fa-exclamation-triangle"></i>
Please customize your NSG if you are using this in higher environments, current configuration is minimal so that we could focus on the lab. Please always go with Security first principle.</div></p><h3 id=32-cluster-creation>3.2 Cluster creation<a hidden class=anchor aria-hidden=true href=#32-cluster-creation>#</a></h3><p>We will use terraform to create clusters, note we will need to add <a href=https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs>databricks provider</a> to terraform which was already performed in earlier steps.</p><pre><code class=language-bash>resource &quot;databricks_cluster&quot; &quot;cluster&quot; {
  cluster_name            = local.dbr_cluster
  spark_version           = data.databricks_spark_version.latest.id
  node_type_id            = data.databricks_node_type.smallest.id
  autotermination_minutes = 20
  spark_conf = {
    &quot;spark.databricks.cluster.profile&quot; : &quot;singleNode&quot;
    &quot;spark.master&quot; : &quot;local[*]&quot;
  }

  custom_tags = {
    &quot;ResourceClass&quot; = &quot;SingleNode&quot;
  }
}
</code></pre><p>you will also need to add the following to the data.tf file so that databricks provide can retrieve some information dynamically for the provisioning.</p><blockquote><p>data.tf</p></blockquote><pre><code class=language-bash>data &quot;databricks_spark_version&quot; &quot;latest&quot; {}

data &quot;databricks_node_type&quot; &quot;smallest&quot; {
  local_disk = true
}
</code></pre><h3 id=quick-explanation-5>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-5>#</a></h3><p>The cluster we provisioned is single cluster which is capable enough to help to complete the lab.</p><p>Once you have all the code in, lets create resources in a iterative fashion by running</p><pre><code class=language-bash>terraform plan -var-file=values.tfvars
</code></pre><p>followed by:</p><pre><code class=language-bash>terraform apply -auto-approve -var-file=values.tfvars
</code></pre><p>Lets login to our databricks instance and validate the cluster is in place.
<img loading=lazy src=/posts/databricks-secure/cluster.png alt=cluster></p><h2 id=part-4-adls--keyvault-with-private-endpoint>Part 4 ADLS + Keyvault with Private endpoint<a hidden class=anchor aria-hidden=true href=#part-4-adls--keyvault-with-private-endpoint>#</a></h2><p>Lets create ADLS account and Keyvault in this part with private endpoints and private DNS zones.
<a href=https://docs.microsoft.com/en-us/azure/private-link/private-endpoint-dns>private DNS zone</a> is required so that a Azure private link DNS address could be resolved to the private IP address for the private endpoint.</p><h3 id=41-storage-account-with-private-endpoint-and-private-dns-zone>4.1 Storage account with private endpoint and private DNS zone<a hidden class=anchor aria-hidden=true href=#41-storage-account-with-private-endpoint-and-private-dns-zone>#</a></h3><p>This part shouldnt be too bad, a storage account with private endpoint deployed to the spoke network.</p><p>First need to add the following to the locals block:</p><pre><code class=language-bash>stor         = &quot;${local.org}stor&quot;
stor_pe      = &quot;${local.stor}pe&quot;
stor_prv_con = &quot;${local.stor}prvcon&quot;

kv         = &quot;${local.org}-kv&quot;
kv_pe      = &quot;${local.kv}pe&quot;
kv_prv_con = &quot;${local.kv}prvcon&quot;
</code></pre><p>Then add the following to main.tf file：</p><pre><code class=language-bash>resource &quot;azurerm_storage_account&quot; &quot;stor&quot; {
  name                = local.stor
  resource_group_name = local.rg

  location                 = local.rg_location
  account_tier             = &quot;Standard&quot;
  account_replication_type = &quot;LRS&quot;

  is_hns_enabled = true

  network_rules {
    default_action = &quot;Deny&quot;
    ip_rules       = [data.external.my_ip.result.ip]
  }
}

resource &quot;azurerm_storage_container&quot; &quot;container&quot; {
  name                  = &quot;land&quot;
  storage_account_name  = azurerm_storage_account.stor.name
  container_access_type = &quot;private&quot;
}

resource &quot;azurerm_private_endpoint&quot; &quot;stor_pe&quot; {
  name                = local.stor_pe
  location            = local.rg_location
  resource_group_name = local.rg
  subnet_id           = azurerm_subnet.resource_stor.id

  private_service_connection {
    name                           = local.stor_prv_con
    private_connection_resource_id = azurerm_storage_account.stor.id
    is_manual_connection           = false
    subresource_names              = [&quot;dfs&quot;]
  }
}

resource &quot;azurerm_private_dns_zone&quot; &quot;stor_dfs&quot; {
  name                = &quot;privatelink.dfs.core.windows.net&quot;
  resource_group_name = local.rg
}

resource &quot;azurerm_private_dns_zone_virtual_network_link&quot; &quot;dbr_vnet_link_stor&quot; {
  name                  = &quot;dbr_vnet_link&quot;
  resource_group_name   = local.rg
  private_dns_zone_name = azurerm_private_dns_zone.stor_dfs.name
  virtual_network_id    = azurerm_virtual_network.vnet_dbr.id
}

resource &quot;azurerm_private_dns_a_record&quot; &quot;storpe_dns&quot; {
  name                = local.stor
  zone_name           = azurerm_private_dns_zone.stor_dfs.name
  resource_group_name = local.rg
  ttl                 = 300
  records             = [azurerm_private_endpoint.stor_pe.private_service_connection.0.private_ip_address]
}
</code></pre><h3 id=quick-explanation-6>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-6>#</a></h3><p>Line 11-14 defined the firewall rules so that we are not open our storage account to public by default. Note the &ldquo;data.external.my_ip.result.ip&rdquo;, because we still want to be operate the storage account from out local machine hence the data block retrieve your current public ip and inject that to network policy. You will also need to add the following to your data.tf file：</p><pre><code class=language-bash>data &quot;external&quot; &quot;my_ip&quot; {
  program = [&quot;curl&quot;, &quot;https://api.ipify.org?format=json&quot;]
}
</code></pre><p>Line 23-35 created the private endpoint targeting the storage account resource.
Line 37-47 creates a Azure private dns zone for ADLS, and link that to the dbr network, so that DNS request from the dbr network could use this for name resolution.
Line 38 specifies the top level domain name for the storage account in the private DNZ zone.
Line 49-55 creates a private DNS record using the private ip address of the network interface as part of private endpoint provisioning.<div class=Note style=background-color:var(--note);font-size:15px;margin-bottom:10px;margin-top:10px;padding:10px;border-radius:10px><i class="fas fa-exclamation-triangle"></i>
Note when creating private endpoint, there will be a network interface attached to it which gets a private ip address allocated in the subnet range.</div></p><h3 id=42-keyvault-with-private-endpoint-and-private-dns-zone>4.2 Keyvault with private endpoint and private DNS zone<a hidden class=anchor aria-hidden=true href=#42-keyvault-with-private-endpoint-and-private-dns-zone>#</a></h3><p>This one is not too bad either, keyvualt with private endpoint deployed to the same spoke network with similar setup on the private DNS zone, vNet linkage and dns records</p><pre><code class=language-bash>resource &quot;azurerm_key_vault&quot; &quot;kv&quot; {
  name                        = local.kv
  location                    = local.rg_location
  resource_group_name         = local.rg
  enabled_for_disk_encryption = true
  tenant_id                   = data.azurerm_client_config.current.tenant_id
  soft_delete_retention_days  = 7
  purge_protection_enabled    = false

  sku_name = &quot;standard&quot;

  network_acls {
    default_action = &quot;Deny&quot;
    bypass = &quot;AzureServices&quot;
    ip_rules = [data.external.my_ip.result.ip]
  }
}

resource &quot;azurerm_private_endpoint&quot; &quot;kv_pe&quot; {
  name                = local.kv_pe
  location            = local.rg_location
  resource_group_name = local.rg
  subnet_id           = azurerm_subnet.resource_kv.id

  private_service_connection {
    name                           = local.kv_prv_con
    private_connection_resource_id = azurerm_key_vault.kv.id
    is_manual_connection           = false
    subresource_names              = [&quot;Vault&quot;]
  }
}

resource &quot;azurerm_private_dns_zone&quot; &quot;kv&quot; {
  name                = &quot;privatelink.vaultcore.azure.net&quot;
  resource_group_name = local.rg
}

resource &quot;azurerm_private_dns_zone_virtual_network_link&quot; &quot;dbr_vnet_link_kv&quot; {
  name                  = &quot;dbr_vnet_link_kv&quot;
  resource_group_name   = local.rg
  private_dns_zone_name = azurerm_private_dns_zone.kv.name
  virtual_network_id    = azurerm_virtual_network.vnet_dbr.id
}

resource &quot;azurerm_private_dns_a_record&quot; &quot;kvpe_dns&quot; {
  name                = local.kv
  zone_name           = azurerm_private_dns_zone.kv.name
  resource_group_name = local.rg
  ttl                 = 300
  records             = [azurerm_private_endpoint.kv_pe.private_service_connection.0.private_ip_address]
}
</code></pre><p>Once you have all the code in, lets create resources in a iterative fashion by running</p><pre><code class=language-bash>terraform plan -var-file=values.tfvars
</code></pre><p>followed by:</p><pre><code class=language-bash>terraform apply -auto-approve -var-file=values.tfvars
</code></pre><h2 id=part-5-databricks-configuration---keyvault-backed-secret-scope>Part 5 Databricks configuration - Keyvault backed secret scope<a hidden class=anchor aria-hidden=true href=#part-5-databricks-configuration---keyvault-backed-secret-scope>#</a></h2><p>To create the keyvault backed secret scope, terraform provides a handy resource to do this. add the following to the main.tf files to achieve this.</p><pre><code class=language-bash>resource &quot;databricks_secret_scope&quot; &quot;kv&quot; {
  name = &quot;keyvault-managed&quot;

  keyvault_metadata {
    resource_id = azurerm_key_vault.kv.id
    dns_name    = azurerm_key_vault.kv.vault_uri
  }
}
</code></pre><p>The tricky part here is that according to terraform this is only supported by azure cli authentication but NOT with service principal authentication. You will see error like this if service principal auth is used:
<img loading=lazy src=/posts/databricks-secure/error.png alt=error>
what you will need to do is to comment out the azurerm authentication part in the providers.tf and then login to Azure using</p><pre><code class=language-bash>az login
</code></pre><p>If you haven&rsquo;t installed Az cli following <a href=https://docs.microsoft.com/en-us/cli/azure/install-azure-cli>this</a> to install and read more details of this limitation <a href=https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/secret_scope>here</a>.</p><h2 id=part-6-run-notebook-to-validate>Part 6 Run notebook to validate<a hidden class=anchor aria-hidden=true href=#part-6-run-notebook-to-validate>#</a></h2><p>Let put some python code in and test the connectivity and DNS resolutions.
The first and most important task is to validate the connectivity and DNS resolution. You can run this code snippet in a notebook to check them.<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kn>import</span> <span class=nx>socket</span>
</span></span><span class=line><span class=cl><span class=nx>stor</span> <span class=p>=</span> <span class=nx>socket</span><span class=p>.</span><span class=nf>gethostbyname_ex</span><span class=p>(</span><span class=s>&#34;blkstor.dfs.core.windows.net&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nf>print</span> <span class=p>(</span><span class=s>&#34;\n\nThe IP Address of the Domain Name is: &#34;</span> <span class=o>+</span> <span class=nf>repr</span><span class=p>(</span><span class=nx>stor</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nx>kv</span> <span class=p>=</span> <span class=nx>socket</span><span class=p>.</span><span class=nf>gethostbyname_ex</span><span class=p>(</span><span class=s>&#34;blk-kv.vault.azure.net&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nf>print</span> <span class=p>(</span><span class=s>&#34;\n\nThe IP Address of the Domain Name is: &#34;</span> <span class=o>+</span> <span class=nf>repr</span><span class=p>(</span><span class=nx>kv</span><span class=p>))</span></span></span></code></pre></td></tr></table></div></div></p><p>If all setup correctly you should be able to see that the DNS name is resolved with a private IP address.
<img loading=lazy src=/posts/databricks-secure/notebook1.png alt=notebook></p><p>If you are interest in further testing to read some file from ADLS you could use the following snippet to do so.</p><pre><code>dbutils.widgets.text(&quot;adls_name&quot;,  &quot;blkstor&quot;, &quot;adls_name&quot;)
dbutils.widgets.text(&quot;adls_container&quot;,  &quot;land&quot;, &quot;adls_container&quot;)
# COMMAND ----------
adls_name = dbutils.widgets.get(&quot;adls_name&quot;)
adls_container = dbutils.widgets.get(&quot;adls_container&quot;)
# COMMAND ----------
spark.conf.set(&quot;fs.azure.account.auth.type&quot;, &quot;OAuth&quot;)
spark.conf.set(&quot;fs.azure.account.oauth.provider.type&quot;, &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;)
spark.conf.set(&quot;fs.azure.account.oauth2.client.id&quot;, dbutils.secrets.get(scope=&quot;keyvault-managed&quot;,key=&quot;sp-id&quot;))
spark.conf.set(&quot;fs.azure.account.oauth2.client.secret&quot;, dbutils.secrets.get(scope=&quot;keyvault-managed&quot;,key=&quot;sp-secret&quot;))
spark.conf.set(&quot;fs.azure.account.oauth2.client.endpoint&quot;, &quot;https://login.microsoftonline.com/xxxx-xxx-xxxxxx-xxxxxx/oauth2/token&quot;)

# COMMAND ----------
df = spark.read.json(f&quot;abfss://{adls_container}@{adls_name}.dfs.core.windows.net/pubapis.json&quot;)
# COMMAND ----------
df.show()
</code></pre><p>What this notebook does is quite simple, it grabs the secret of a service principal from keyvault secret scope and use that to auth to Azure.
Once authorized it will read a json file into dataframe. Please seethe outcome of the notebook run in the following screen shot.
<img loading=lazy src=/posts/databricks-secure/notebook2.png alt=notebook2></p><p>To be able to achieve this, you will need the following:</p><ol><li>another service principal that has the role &ldquo;Storage blob contributor&rdquo; on the storage account</li><li>having the application id and secret saved in the Keyvault we created as &ldquo;sp-id&rdquo; and &ldquo;sp-secret&rdquo;</li></ol><p>I have included these two notebooks in the repo, you will find them in the notebooks folder.</p><p>To deploy the notebooks to our databricks workspace, you could add the following to your main.tf file as part of the deployment.</p><pre><code class=language-bash>resource &quot;databricks_notebook&quot; &quot;notbooks&quot; {
  for_each = fileset(&quot;${path.module}/notebooks&quot;, &quot;*&quot;)
  source = &quot;${path.module}/notebooks/${each.key}&quot;
  path   = &quot;/validation/${element(split(&quot;.&quot;, each.key), 0)}&quot;
  language = &quot;PYTHON&quot;
}
</code></pre><p>This block iterate through the notebooks folder and deploy the notebook in a folder called &ldquo;validated&rdquo; in the workspace.<div class=Note style=background-color:var(--note);font-size:15px;margin-bottom:10px;margin-top:10px;padding:10px;border-radius:10px><i class="fas fa-exclamation-triangle"></i>
Note usually you would not deploy your notebooks as part of IaC efforts, these would otherwise be regarded as the "application" artifacts. But for simplicity of our demo, they are deployed in the same IaC code flow.</div></p><p>At this point our lab has reached the end, hope you enjoyed the extra efforts needed to make your databricks cluster more secure.</p><p>If like the content please leave your comments below, if you fine issues with the content, please also comment below. Thanks for your time and patience with me !!</p></div><footer class=post-footer><ul class=post-tags><li><i class="fas fa-tags fa-sm"></i></li><li><a href=https://blacklabnz.github.io/tags/terraform/>terraform</a></li><li><a href=https://blacklabnz.github.io/tags/databricks/>databricks</a></li><li><a href=https://blacklabnz.github.io/tags/private-endpoints/>private endpoints</a></li><li><a href=https://blacklabnz.github.io/tags/databricks-secure-connectivity/>databricks secure connectivity</a></li><li><a href=https://blacklabnz.github.io/tags/databricks-vnet-injection/>databricks vNet injection</a></li><li><a href=https://blacklabnz.github.io/tags/keyvault-backed-secret-scope/>keyvault backed secret scope</a></li></ul><nav class=paginav><a class=prev href=https://blacklabnz.github.io/posts/websocket-prometheus/><span class=title>« Prev Page</span><br><span>Consume Websocket stream and send to Prometheus in Python</span></a></nav><div class=share-buttons><a>Share via:</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Secure Databricks cluster with vNet injection and access resources via Azure private endpoint on twitter" href="https://twitter.com/intent/tweet/?text=Secure%20Databricks%20cluster%20with%20vNet%20injection%20and%20access%20resources%20via%20Azure%20private%20endpoint&url=https%3a%2f%2fblacklabnz.github.io%2fposts%2fdatabricks-secure%2f&hashtags=terraform%2cdatabricks%2cprivateendpoints%2cdatabrickssecureconnectivity%2cdatabricksvNetinjection%2ckeyvaultbackedsecretscope"><i class="fab fa-twitter fa-lg"></i></a>
<a target=_blank rel="noopener noreferrer" aria-label="share Secure Databricks cluster with vNet injection and access resources via Azure private endpoint on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fblacklabnz.github.io%2fposts%2fdatabricks-secure%2f&title=Secure%20Databricks%20cluster%20with%20vNet%20injection%20and%20access%20resources%20via%20Azure%20private%20endpoint&summary=Secure%20Databricks%20cluster%20with%20vNet%20injection%20and%20access%20resources%20via%20Azure%20private%20endpoint&source=https%3a%2f%2fblacklabnz.github.io%2fposts%2fdatabricks-secure%2f"><i class="fab fa-linkedin-in fa-lg"></i></a></div></footer><div id=page-comments><script src=https://utteranc.es/client.js repo=blacklabnz/blacklabnz.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div><script src=/assets/js/utterance-theme.min.js></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://blacklabnz.github.io/>blacklabnz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><i class="fas fa-chevron-circle-up fa-2x"></i></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>