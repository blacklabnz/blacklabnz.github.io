<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Purview Lineage: Part A Databricks Manual Lineage | blacklabnz | Data | DevOps</title><meta name=keywords content="purview,lineage,data catalogue,pyapacheatlas"><meta name=description content="Purview has been published by Microsoft as a unified data governance solution to help manage and govern your multi-cloud, SaaS and on prem data. You can create a holistic and up-to-date view of your data landscape with automated data discovery, data classification and end to end lineage. This provides data users with valuable, trustworthy data management. While the auto scanned lineage is useful most of the times, there are always cases where you need to manually generate your lineage graph."><meta name=author content="Neil Xu"><link rel=canonical href=https://blacklabnz.github.io/posts/purview-lineage-manual/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.ed8304e9a7131d4ed812fbc7df21d2959f9ef0525fcc90f168c1e8f506768abc.css integrity="sha256-7YME6acTHU7YEvvH3yHSlZ+e8FJfzJDxaMHo9QZ2irw=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://blacklabnz.github.io/favicon16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blacklabnz.github.io/favicon32x32.png><link rel=mask-icon href=https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous><script async src="https://www.googletagmanager.com/gtag/js?id=G-FDV12HCDKK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-FDV12HCDKK",{anonymize_ip:!1})}</script><meta property="og:title" content="Purview Lineage: Part A Databricks Manual Lineage"><meta property="og:description" content="Purview has been published by Microsoft as a unified data governance solution to help manage and govern your multi-cloud, SaaS and on prem data. You can create a holistic and up-to-date view of your data landscape with automated data discovery, data classification and end to end lineage. This provides data users with valuable, trustworthy data management. While the auto scanned lineage is useful most of the times, there are always cases where you need to manually generate your lineage graph."><meta property="og:type" content="article"><meta property="og:url" content="https://blacklabnz.github.io/posts/purview-lineage-manual/"><meta property="og:image" content="https://blacklabnz.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-30T00:39:13+13:00"><meta property="article:modified_time" content="2022-10-30T00:39:13+13:00"><meta property="og:site_name" content="blacklabnz"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blacklabnz.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Purview Lineage: Part A Databricks Manual Lineage"><meta name=twitter:description content="Purview has been published by Microsoft as a unified data governance solution to help manage and govern your multi-cloud, SaaS and on prem data. You can create a holistic and up-to-date view of your data landscape with automated data discovery, data classification and end to end lineage. This provides data users with valuable, trustworthy data management. While the auto scanned lineage is useful most of the times, there are always cases where you need to manually generate your lineage graph."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blacklabnz.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Purview Lineage: Part A Databricks Manual Lineage","item":"https://blacklabnz.github.io/posts/purview-lineage-manual/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Purview Lineage: Part A Databricks Manual Lineage","name":"Purview Lineage: Part A Databricks Manual Lineage","description":"Purview has been published by Microsoft as a unified data governance solution to help manage and govern your multi-cloud, SaaS and on prem data. You can create a holistic and up-to-date view of your data landscape with automated data discovery, data classification and end to end lineage. This provides data users with valuable, trustworthy data management. While the auto scanned lineage is useful most of the times, there are always cases where you need to manually generate your lineage graph.","keywords":["purview","lineage","data catalogue","pyapacheatlas"],"articleBody":" Purview has been published by Microsoft as a unified data governance solution to help manage and govern your multi-cloud, SaaS and on prem data. You can create a holistic and up-to-date view of your data landscape with automated data discovery, data classification and end to end lineage. This provides data users with valuable, trustworthy data management. While the auto scanned lineage is useful most of the times, there are always cases where you need to manually generate your lineage graph. Apache Spark is one of those examples where you might need to build the lineage manually from time to time.\nIn this post, let’s explore how we can do that with pyapacheatlas in python, alternatively if you are planning to build you custom solution, you might want to refer to Purview REST API. We will start with using Databricks to run some sample transformation, then we will create a Purview instance and do some data sources scanning. Once completed, we will create the lineage using python which you could either run in Databricks notebook or in python local environment. So let dive into it！\nSetup Databircks and ADLS account to run sample transformation First we need to setup Databricks and Azure ADLS (storage account) in order to do some sample transformation. Obviously our focus in the lab is not IaC and the easiest way is via manual efforts. However if you are interested in IaC, you can refer to my other post Secure Databricks where I focused on setting up everything using terraform.\nCreate Databricks clusters You could easily create cluster using the interactive wizard in Azure portal and I hope that is smooth experience for you.\nCreate ADLS account The next step is to create a ADLS account. It can be easily created in the portal with catch to remember to tick the ADLS option in the wizard. The namespace/folder structure inside ADLS that I had for this lab are as the following:\nadls account - land (container) - dbr (container) - ingested (folder) - curated (folder) Load and transform sample data We can grab some data from Databricks sample data store that is available when the Databricks cluster is created. If we do file list at the following location, we could see the files that we use.\n/databricks-datasets/nyctaxi Lets use one of the green taxi file with the taxi zone lookup file. Please refer to the following for the path of the file we sue\n/databricks-datasets/nyctaxi/taxizone/taxi_zone_lookup.csv /databricks-datasets/nyctaxi/tripdata/green/green_tripdata_2009-05.csv.gz First I will copy the sample csv “green_tripdata_2009-05.csv.gz” into the landing zone by running the following in my notebook:\ndbutils.fs.cp( \"/databricks-datasets/nyctaxi/tripdata/green/green_tripdata_2019-12.csv.gz\", \"abfss://land@blkdatalake.dfs.core.windows.net/trip_data/green_tripdata_2019-12.csv.gz\") dbutils.fs.cp( \"/databricks-datasets/nyctaxi/taxizone/taxi_zone_lookup.csv\", \"abfss://land@blkdatalake.dfs.core.windows.net/zone_lookup/taxi_zone_lookup.csv\") Once file is copied, I can then read it and write it to the ingested folder to simulate data ingestion using the following:\ndf_trip = spark.read .options(header=\"True\") .csv(\"abfss://land@blkdatalake.dfs.core.windows.net/trip_data/green_tripdata_2009-05.csv.gz\") df_trip.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://dbr@blkdatalake.dfs.core.windows.net/ingested/trip_data\") df_zone = (spark.read .options(header=\"True\") .csv(\"abfss://land@blkdatalake.dfs.core.windows.net/zone_lookup/taxi_zone_lookup.csv\")) df_zone.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://dbr@blkdatalake.dfs.core.windows.net/ingested/zone_data\") Lets inspect both taxi data and zone data to see what transformation we could do for this lab.\nIn the following green taxi data, the PULocationId and DOLocationId columns indicates the zone code for the location. The zone data in the following is a reference table where we could look up the zone name by its id. So here is an idea for transformation, lets replace the zone id with zone name using the lookup table. So lets start ! First lets create some tables using the data we have ingested.\n%sql create table if not exists nyctaxi_trip using delta location 'abfss://dbr@blkdatalake.dfs.core.windows.net/ingested/trip_data'; create table if not exists nyctaxi_zone using delta location 'abfss://dbr@blkdatalake.dfs.core.windows.net/ingested/zone_data'; Now that we have the data in the delta table, lets look at our transformation SQL script. The transformation logic is fairly simple, create CTE by lookup up the zone id from zone table, join it with trip data and then select the columns we are interested to form the nyctaxi_curated_tbl table. So lets run that in the notebook.\n%sql create or replace table nyctaxi_curated_tbl using delta location 'abfss://dbr@blkdatalake.dfs.core.windows.net/curated/trip_data' as ( with temp as ( select lpep_pickup_datetime as pickup_time, lpep_dropoff_datetime as dropoff_time, PULocationID, DOLocationID, z.Zone as pickup_zone, z.LocationID as pickup_zone_id, x.Zone as dropoff_zone, x.LocationID as dropoff_zone_id from nyctaxi_trip t left join nyctaxi_zone z on t.PULocationID = z.LocationID left join nyctaxi_zone x on t.DOLocationID = x.LocationID ) select pickup_time, dropoff_time, pickup_zone, dropoff_zone from temp); Now we should have the sample data in place so lets move to next part where we use purview to scan those assets.\nSetup Purview and scan the assets In this section we need to create Purview instance manually and scan the our ADLS account.\nCreate Purview instance The process of creating Purview instance is straight forward, one can follow this doc for more details.\nScan ADLS assets Once Purview is created, we can start scanning our ADLS account. First lets create some logical construct to keep our assets what is so called a collection in Purview. Its pretty effortless to create a collection, the quick doc has all you need to create the collection.\nWith some collection created, now its time to register our assets. Follow screenshots below and click continue to proceed to asset registration. Follow the prompt and select the ADLS (storage account) that your used in the previous steps and hit register. The guide will take you to the next part to create scan, in this part you can also test the connection between Purview and ADLS. If you are working on a fresh setup you will need to do a role assignment so that you Purview system managed identity has the “Storage blob contributor” access to you ADLS. Once all set, please click continue, this will take you to another screen where you select the folders to be scanned. The folder structure should reflect to what we have done in Databricks in previous steps. Click “continue” and we will see the prompt ask us to select the scan rule set, please select the default rule. Alternatively you can also create custom rule to include/exclude certain file types here. At this window we could setup the recurrence for the scan. Or, for our lab purpose we can select “once” and hit continue. In the last window, lets review our setup and if all correct let click “save \u0026 run”. Allow the scan to run for a few minute before you see the complete status. Navigate to your assets and click “details” in your registered assets, you will see the scan history. At this point when we navigate to “Data catalogue” and start browsing, we should be able to see some assets there. Create manual lineage using python With all the assets setup in Purview, now lets start the fun and create some manual lineage using python. This is little bit painful compare to the auto scan, but hey, thats where the value of automation kicks in to Data governance right ?! The final outcome of the lineage looks pretty good, we can see the ingestion step as well as the join action of two datasets to form the final curated dataset. Though its missing the column level lineage, but one step at a time ! Lets dive into some python code snippet and find out how we can easily create manual lineage!\nSetup python environment and dependencies The python package we use to do this can be found here in pypi. Thanks to all the contributors.\nThere are couple of ways to run the python script, either in your local python virtual environment or in databricks notebook. I have chosen to run the python scripts locally, therefore I have created a virtual env and installed the required packages.\nCode deep dive The first block sets up the import and create a client using service principal authentication.\nfrom html.entities import entitydefs from pyapacheatlas.auth import ServicePrincipalAuthentication from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess import json import uuid import logging logging.basicConfig() logging.getLogger().setLevel(logging.DEBUG) auth = ServicePrincipalAuthentication( tenant_id = \"your azure tenant id\", client_id = \"your service principal id\", client_secret = \"your service principal secret\" ) client = PurviewClient( account_name = \"you purview account\", authentication = auth ) This part assumes that you have already created a service principal and completed the role assignment to it in purview with “Collection Admins” in the root collection. For carrying on our lab we can start with “Collection Admins” followed by a security hardening process to reduce the permissions for the service principal later on. Since our successful scan has brought us some registered assets, therefore we can retrieve them from Purview and use them in the lineage graph.\nThe concept of lineage graph can be be interpreted as simple as that you have a process that takes an input and generates output. Therefore the logic would be [list of inputs] -\u003e process -\u003e [list of outputs]. I have some inline documentation to explain what does each part do.\n# This block uses the client to retrieve a entity(asset) create by the scan using its guid. # I will explain how to find the guid in the portal very soon. zone_data_land = client.get_entity( guid=\"0c75617c-xxxx-4e1f-b972-655e0a09e0b7\" ) # Create a Atlas entity using the information of the asset retrieved by its id in the previous block. # In this case we are creating the input entity input_zone = AtlasEntity( name=zone_data_land[\"entities\"][0][\"attributes\"][\"name\"], typeName=zone_data_land[\"entities\"][0][\"typeName\"], qualified_name=zone_data_land[\"entities\"][0][\"attributes\"][\"qualifiedName\"], guid=\"0c75617c-xxxx-4e1f-b972-655e0a09e0b7\" ) # This block uses the client to retrieve a entity(asset) create by the scan using its guid. zone_data_ingested = client.get_entity( guid=\"d714587f-xxxx-4190-8f5f-54dfe27a58b5\" ) # Create a Atlas entity using the information of the asset retrieved by in id in the previous block. # In this case we are creating the output entity generated by the process output_zone = AtlasEntity( name=zone_data_ingested[\"entities\"][0][\"attributes\"][\"name\"], typeName=zone_data_ingested[\"entities\"][0][\"typeName\"], qualified_name=zone_data_ingested[\"entities\"][0][\"attributes\"][\"qualifiedName\"], guid=\"d714587f-xxxx-4190-8f5f-54dfe27a58b5\" ) # Create a Atlas process that links the input and the output # Note the input and output parameter of AtlasProcess class, it is list type but at the moment there is only one object in it. zone_process = AtlasProcess( name=\"zone_ingestion\", typeName=\"Process\", qualified_name=\"pyspark://zone_ingestion\", inputs=[input_zone], outputs=[output_zone], guid=f\"-{uuid.uuid4()}\" ) # Create the process and upload it. results = client.upload_entities( batch=[input_zone, output_zone, zone_process] ) The above code requires you to know the guid of the entity that you wanted to bind to the process. It is fairly simple to get by navigating to purview, simply go the asset that you are interested in and you can find the guid in the browser link. This seems to be a bit “anti-automation” but with some good investigation to the pyapacheatlas, you would be able to retrieve the entity by its “name” and “typeName”. The official documentation is your best friend to investigate the automation for this part. For our lab purpose lets find it from the browser link. You can now see these two datasets are the one used in the input and output. Note the guid which was used in looking up the input dataset When finished with python setup simply run the code snippet we’v got so far. If everything is happening as intended, you should be able to see the lineage when you navigate to the lineage tab of your zone data asset. The following code is pretty much the same to the previous block, only difference is that it is building the lineage for the other trip dataset that we are going to use in the join. So I will skip explanation for this part.\ntrip_data_land=client.get_entity( guid=\"5e0a1aeb-xxxx-4deb-86aa-f1ce8945e6db\" ) input_trip = AtlasEntity( name=trip_data_land[\"entities\"][0][\"attributes\"][\"name\"], typeName=trip_data_land[\"entities\"][0][\"typeName\"], qualified_name=trip_data_land[\"entities\"][0][\"attributes\"][\"qualifiedName\"], guid=\"5e0a1aeb-xxxx-4deb-86aa-f1ce8945e6db\" ) trip_data_ingested=client.get_entity( guid=\"d68138fe-xxxx-454f-9d26-581af01787f2\" ) output_trip = AtlasEntity( name=trip_data_ingested[\"entities\"][0][\"attributes\"][\"name\"], typeName=trip_data_ingested[\"entities\"][0][\"typeName\"], qualified_name=trip_data_ingested[\"entities\"][0][\"attributes\"][\"qualifiedName\"], guid=\"d68138fe-xxxx-454f-9d26-581af01787f2\" ) trip_process = AtlasProcess( name=\"trip_ingestion\", typeName=\"Process\", qualified_name=\"pyspark://trip_ingestion\", inputs=[input_trip], outputs=[output_trip], guid=f\"-{uuid.uuid4()}\" ) results = client.upload_entities( batch=[input_trip, output_trip, trip_process] ) When completed adding the snippet, please run the python script again and inspect to make sure you’v got lineage for the trip dataset.\nThe very last bit is to build the lineage graph for the join action. The logic behind this is the same, this time we are using the output entity as the input entity for the next join process. Note the “input” and “output” parameter in the “AtalasProcess” object, this time it includes two items to reflect a join action. When you finish the coding, run the completed script again and you should be able to see the lineage graph you see at the beginning of the section.\ntrip_data_curated=client.get_entity( guid=\"6bb1ed88-xxxx-4b9a-bb0f-96c37c35b57f\" ) output_trip_curated=AtlasEntity( name=trip_data_curated[\"entities\"][0][\"attributes\"][\"name\"], typeName=trip_data_curated[\"entities\"][0][\"typeName\"], qualified_name=trip_data_curated[\"entities\"][0][\"attributes\"][\"qualifiedName\"], guid=\"6bb1ed88-xxxx-4b9a-bb0f-96c37c35b57f\" ) input_trip_curation = AtlasEntity( name=trip_data_ingested[\"entities\"][0][\"attributes\"][\"name\"], typeName=trip_data_ingested[\"entities\"][0][\"typeName\"], qualified_name=trip_data_ingested[\"entities\"][0][\"attributes\"][\"qualifiedName\"], guid=\"d68138fe-xxxx-454f-9d26-581af01787f2\" ) input_zone_curation = AtlasEntity( name=zone_data_ingested[\"entities\"][0][\"attributes\"][\"name\"], typeName=zone_data_ingested[\"entities\"][0][\"typeName\"], qualified_name=zone_data_ingested[\"entities\"][0][\"attributes\"][\"qualifiedName\"], guid=\"d714587f-xxxx-4190-8f5f-54dfe27a58b5\" ) trip_process_curated = AtlasProcess( name=\"trip_curation\", typeName=\"Process\", qualified_name=\"pyspark://trip_curation\", inputs=[input_trip_curation, input_zone_curation], outputs=[output_trip_curated], guid=f\"-{uuid.uuid4()}\" ) results = client.upload_entities( batch=[input_trip_curation, input_zone_curation, output_trip_curated, trip_process_curated] ) Closing Note Congrats ! You’v reached to the end of this post on manual lineage. Thank you for your patience too ! The code in this lab is only intended to be used as snippets, you will obviously need some careful scheme to turn it into production ready code. But you get the gist right ?! It only take “little bit” more efforts to make it perfect ! Hope by now you have a good idea of building manual lineage in such situation when the auto scan does not suite.\nIn part B of this topic on Purview lineage, I will explore how we can connect Purview to Databricks Hive Metadata store and see if we can capture the lineage by scanning it. Stay tuned and keep safe !\n","wordCount":"2190","inLanguage":"en","datePublished":"2022-10-30T00:39:13+13:00","dateModified":"2022-10-30T00:39:13+13:00","author":{"@type":"Person","name":"Neil Xu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blacklabnz.github.io/posts/purview-lineage-manual/"},"publisher":{"@type":"Organization","name":"blacklabnz | Data | DevOps","logo":{"@type":"ImageObject","url":"https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><span class=header__inner><span class=logo><a href=https://blacklabnz.github.io/ accesskey=h title="blacklabnz (Alt + H)"><span><i class="fas fa-terminal fa-sm"></i></span>
<span class=logo_text>blacklabnz</span>
<span id=cursor></span></a></span>
<span class=header__right><div class=dropdown><span class=dropbtn><i class="fas fa-bars"></i></span><nav id=dropdonwBox class=dropdown-content><ul id=menu-mobile><li><a href=https://blacklabnz.github.io/posts title=blogs><span>blogs</span></a></li><li><a href=https://blacklabnz.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://blacklabnz.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://blacklabnz.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav><script src=/assets/js/menu.min.js></script></div><nav class=nav><ul id=menu><li><a href=https://blacklabnz.github.io/posts title=blogs><span>blogs</span></a></li><li><a href=https://blacklabnz.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://blacklabnz.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://blacklabnz.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav><ul class=logo-switches></ul><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></span></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blacklabnz.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://blacklabnz.github.io/posts/>Posts</a></div><h1 class=post-title>Purview Lineage: Part A Databricks Manual Lineage</h1><div class=post-meta><span title='2022-10-30 00:39:13 +1300 +1300'>Created October 30, 2022</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Neil Xu&nbsp;|&nbsp;<a href=https://github.com/blacklabnz/blacklabnz.github.io/blob/main/content/posts/purview-lineage-manual.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#setup-databircks-and-adls-account-to-run-sample-transformation aria-label="Setup Databircks and ADLS account to run sample transformation">Setup Databircks and ADLS account to run sample transformation</a><ul><li><a href=#create-databricks-clusters aria-label="Create Databricks clusters">Create Databricks clusters</a></li><li><a href=#create-adls-account aria-label="Create ADLS account">Create ADLS account</a></li><li><a href=#load-and-transform-sample-data aria-label="Load and transform sample data">Load and transform sample data</a></li></ul></li><li><a href=#setup-purview-and-scan-the-assets aria-label="Setup Purview and scan the assets">Setup Purview and scan the assets</a><ul><li><a href=#create-purview-instance aria-label="Create Purview instance">Create Purview instance</a></li><li><a href=#scan-adls-assets aria-label="Scan ADLS assets">Scan ADLS assets</a></li></ul></li><li><a href=#create-manual-lineage-using-python aria-label="Create manual lineage using python">Create manual lineage using python</a><ul><li><a href=#setup-python-environment-and-dependencies aria-label="Setup python environment and dependencies">Setup python environment and dependencies</a></li><li><a href=#code-deep-dive aria-label="Code deep dive">Code deep dive</a></li></ul></li><li><a href=#closing-note aria-label="Closing Note">Closing Note</a></li></ul></div></details></div><div class=post-content><p><img loading=lazy src=/posts/purview-lineage-manual/lineage_main.png alt=lineage_main>
Purview has been published by Microsoft as a unified data governance solution to help manage and govern your multi-cloud, SaaS and on prem data. You can create a holistic and up-to-date view of your data landscape with automated data discovery, data classification and end to end lineage. This provides data users with valuable, trustworthy data management.
While the auto scanned lineage is useful most of the times, there are always cases where you need to manually generate your lineage graph. Apache Spark is one of those examples where you might need to build the lineage manually from time to time.</p><p>In this post, let&rsquo;s explore how we can do that with <a href=https://github.com/wjohnson/pyapacheatlas>pyapacheatlas</a> in python, alternatively if you are planning to build you custom solution, you might want to refer to <a href=https://learn.microsoft.com/en-us/azure/purview/tutorial-using-rest-apis>Purview REST API</a>. We will start with using Databricks to run some sample transformation, then we will create a Purview instance and do some data sources scanning. Once completed, we will create the lineage using python which you could either run in Databricks notebook or in python local environment. So let dive into it！</p><h2 id=setup-databircks-and-adls-account-to-run-sample-transformation>Setup Databircks and ADLS account to run sample transformation<a hidden class=anchor aria-hidden=true href=#setup-databircks-and-adls-account-to-run-sample-transformation>#</a></h2><p>First we need to setup Databricks and Azure ADLS (storage account) in order to do some sample transformation. Obviously our focus in the lab is not IaC and the easiest way is via manual efforts. However if you are interested in IaC, you can refer to my other post <a href=http://localhost:1313/posts/databricks-secure/>Secure Databricks</a> where I focused on setting up everything using terraform.</p><h3 id=create-databricks-clusters>Create Databricks clusters<a hidden class=anchor aria-hidden=true href=#create-databricks-clusters>#</a></h3><p>You could easily create cluster using the interactive wizard in Azure portal and I hope that is smooth experience for you.</p><h3 id=create-adls-account>Create ADLS account<a hidden class=anchor aria-hidden=true href=#create-adls-account>#</a></h3><p>The next step is to create a ADLS account. It can be easily created in the portal with catch to remember to tick the ADLS option in the wizard. The namespace/folder structure inside ADLS that I had for this lab are as the following:</p><pre><code>adls account
    - land (container)
    - dbr (container)
        - ingested (folder)
        - curated (folder)
</code></pre><h3 id=load-and-transform-sample-data>Load and transform sample data<a hidden class=anchor aria-hidden=true href=#load-and-transform-sample-data>#</a></h3><p>We can grab some data from Databricks sample data store that is available when the Databricks cluster is created.
If we do file list at the following location, we could see the files that we use.</p><pre><code>/databricks-datasets/nyctaxi
</code></pre><p>Lets use one of the green taxi file with the taxi zone lookup file. Please refer to the following for the path of the file we sue</p><pre><code>/databricks-datasets/nyctaxi/taxizone/taxi_zone_lookup.csv
/databricks-datasets/nyctaxi/tripdata/green/green_tripdata_2009-05.csv.gz
</code></pre><p>First I will copy the sample csv &ldquo;green_tripdata_2009-05.csv.gz&rdquo; into the landing zone by running the following in my notebook:</p><pre><code>dbutils.fs.cp(
    &quot;/databricks-datasets/nyctaxi/tripdata/green/green_tripdata_2019-12.csv.gz&quot;, 
    &quot;abfss://land@blkdatalake.dfs.core.windows.net/trip_data/green_tripdata_2019-12.csv.gz&quot;)

dbutils.fs.cp(
    &quot;/databricks-datasets/nyctaxi/taxizone/taxi_zone_lookup.csv&quot;, 
    &quot;abfss://land@blkdatalake.dfs.core.windows.net/zone_lookup/taxi_zone_lookup.csv&quot;)
</code></pre><p>Once file is copied, I can then read it and write it to the ingested folder to simulate data ingestion using the following:</p><pre><code>df_trip = spark.read
    .options(header=&quot;True&quot;)
    .csv(&quot;abfss://land@blkdatalake.dfs.core.windows.net/trip_data/green_tripdata_2009-05.csv.gz&quot;)

df_trip.write.mode(&quot;overwrite&quot;).format(&quot;delta&quot;).save(&quot;abfss://dbr@blkdatalake.dfs.core.windows.net/ingested/trip_data&quot;)

df_zone = (spark.read
    .options(header=&quot;True&quot;)
    .csv(&quot;abfss://land@blkdatalake.dfs.core.windows.net/zone_lookup/taxi_zone_lookup.csv&quot;))

df_zone.write.mode(&quot;overwrite&quot;).format(&quot;delta&quot;).save(&quot;abfss://dbr@blkdatalake.dfs.core.windows.net/ingested/zone_data&quot;)
</code></pre><p>Lets inspect both taxi data and zone data to see what transformation we could do for this lab.</p><p>In the following green taxi data, the PULocationId and DOLocationId columns indicates the zone code for the location.
<img loading=lazy src=/posts/purview-lineage-manual/green_taxi.png alt=green_taxi>
The zone data in the following is a reference table where we could look up the zone name by its id.
<img loading=lazy src=/posts/purview-lineage-manual/zone.png alt=zone>
So here is an idea for transformation, lets replace the zone id with zone name using the lookup table. So lets start !
First lets create some tables using the data we have ingested.</p><pre><code>%sql
create table if not exists nyctaxi_trip using delta location 'abfss://dbr@blkdatalake.dfs.core.windows.net/ingested/trip_data';
create table if not exists nyctaxi_zone using delta location 'abfss://dbr@blkdatalake.dfs.core.windows.net/ingested/zone_data';
</code></pre><p>Now that we have the data in the delta table, lets look at our transformation SQL script.
The transformation logic is fairly simple, create CTE by lookup up the zone id from zone table, join it with trip data and then select the columns we are interested to form the nyctaxi_curated_tbl table.
So lets run that in the notebook.</p><pre><code>%sql
create or replace table nyctaxi_curated_tbl using delta location 'abfss://dbr@blkdatalake.dfs.core.windows.net/curated/trip_data'
as (
    with temp as (
        select 
        lpep_pickup_datetime as pickup_time, 
        lpep_dropoff_datetime as dropoff_time,
        PULocationID, 
        DOLocationID, 
        z.Zone as pickup_zone, 
        z.LocationID as pickup_zone_id,
        x.Zone as dropoff_zone,
        x.LocationID as dropoff_zone_id
        from nyctaxi_trip t left join nyctaxi_zone z on t.PULocationID = z.LocationID left join nyctaxi_zone x on t.DOLocationID = x.LocationID
    )
    select pickup_time, dropoff_time, pickup_zone, dropoff_zone from temp);
</code></pre><p>Now we should have the sample data in place so lets move to next part where we use purview to scan those assets.</p><h2 id=setup-purview-and-scan-the-assets>Setup Purview and scan the assets<a hidden class=anchor aria-hidden=true href=#setup-purview-and-scan-the-assets>#</a></h2><p>In this section we need to create Purview instance manually and scan the our ADLS account.</p><h3 id=create-purview-instance>Create Purview instance<a hidden class=anchor aria-hidden=true href=#create-purview-instance>#</a></h3><p>The process of creating Purview instance is straight forward, one can follow this <a href=https://learn.microsoft.com/en-us/azure/purview/create-microsoft-purview-portal>doc</a> for more details.</p><h3 id=scan-adls-assets>Scan ADLS assets<a hidden class=anchor aria-hidden=true href=#scan-adls-assets>#</a></h3><p>Once Purview is created, we can start scanning our ADLS account.
First lets create some logical construct to keep our assets what is so called a collection in Purview.
Its pretty effortless to create a collection, the quick <a href=https://learn.microsoft.com/en-us/azure/purview/quickstart-create-collection>doc</a> has all you need to create the collection.</p><p>With some collection created, now its time to register our assets.
Follow screenshots below and click continue to proceed to asset registration.
<img loading=lazy src=/posts/purview-lineage-manual/scan1.png alt=scan1>
Follow the prompt and select the ADLS (storage account) that your used in the previous steps and hit register.
<img loading=lazy src=/posts/purview-lineage-manual/scan2.png alt=scan2>
The guide will take you to the next part to create scan, in this part you can also test the connection between Purview and ADLS.
If you are working on a fresh setup you will need to do a role assignment so that you Purview system managed identity has the &ldquo;Storage blob contributor&rdquo; access to you ADLS.
<img loading=lazy src=/posts/purview-lineage-manual/scan3.png alt=scan3>
Once all set, please click continue, this will take you to another screen where you select the folders to be scanned. The folder structure should reflect to what we have done in Databricks in previous steps.
<img loading=lazy src=/posts/purview-lineage-manual/scan4.png alt=scan4>
Click &ldquo;continue&rdquo; and we will see the prompt ask us to select the scan rule set, please select the default rule. Alternatively you can also create custom rule to include/exclude certain file types here.
<img loading=lazy src=/posts/purview-lineage-manual/scan5.png alt=scan5>
At this window we could setup the recurrence for the scan. Or, for our lab purpose we can select &ldquo;once&rdquo; and hit continue.
<img loading=lazy src=/posts/purview-lineage-manual/scan6.png alt=scan6>
In the last window, lets review our setup and if all correct let click &ldquo;save & run&rdquo;.
<img loading=lazy src=/posts/purview-lineage-manual/scan7.png alt=scan7>
Allow the scan to run for a few minute before you see the complete status. Navigate to your assets and click &ldquo;details&rdquo; in your registered assets, you will see the scan history.
<img loading=lazy src=/posts/purview-lineage-manual/scan8.png alt=scan8>
At this point when we navigate to &ldquo;Data catalogue&rdquo; and start browsing, we should be able to see some assets there.
<img loading=lazy src=/posts/purview-lineage-manual/scan9.png alt=scan9></p><h2 id=create-manual-lineage-using-python>Create manual lineage using python<a hidden class=anchor aria-hidden=true href=#create-manual-lineage-using-python>#</a></h2><p>With all the assets setup in Purview, now lets start the fun and create some manual lineage using python.
This is little bit painful compare to the auto scan, but hey, thats where the value of automation kicks in to Data governance right ?!
The final outcome of the lineage looks pretty good, we can see the ingestion step as well as the join action of two datasets to form the final curated dataset. Though its missing the column level lineage, but one step at a time !
<img loading=lazy src=/posts/purview-lineage-manual/lineage.png alt=lineage>
Lets dive into some python code snippet and find out how we can easily create manual lineage!</p><h3 id=setup-python-environment-and-dependencies>Setup python environment and dependencies<a hidden class=anchor aria-hidden=true href=#setup-python-environment-and-dependencies>#</a></h3><p>The python package we use to do this can be found <a href=https://pypi.org/project/pyapacheatlas/>here</a> in pypi. Thanks to all the contributors.</p><p>There are couple of ways to run the python script, either in your local python virtual environment or in databricks notebook.
I have chosen to run the python scripts locally, therefore I have created a virtual env and installed the required packages.</p><h3 id=code-deep-dive>Code deep dive<a hidden class=anchor aria-hidden=true href=#code-deep-dive>#</a></h3><p>The first block sets up the import and create a client using service principal authentication.</p><pre><code class=language-python>from html.entities import entitydefs
from pyapacheatlas.auth import ServicePrincipalAuthentication
from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess
import json
import uuid
import logging

logging.basicConfig()
logging.getLogger().setLevel(logging.DEBUG)

auth = ServicePrincipalAuthentication(
    tenant_id = &quot;your azure tenant id&quot;, 
    client_id = &quot;your service principal id&quot;,
    client_secret = &quot;your service principal secret&quot;
)

client = PurviewClient(
    account_name = &quot;you purview account&quot;,
    authentication = auth
)
</code></pre><p>This part assumes that you have already created a service principal and completed the role assignment to it in purview with &ldquo;Collection Admins&rdquo; in the root collection.
For carrying on our lab we can start with &ldquo;Collection Admins&rdquo; followed by a security hardening process to reduce the permissions for the service principal later on.
<img loading=lazy src=/posts/purview-lineage-manual/sp_role.png alt=sp_role>
Since our successful scan has brought us some registered assets, therefore we can retrieve them from Purview and use them in the lineage graph.</p><p>The concept of lineage graph can be be interpreted as simple as that you have a process that takes an input and generates output.
Therefore the logic would be [list of inputs] -> process -> [list of outputs].
I have some inline documentation to explain what does each part do.</p><pre><code># This block uses the client to retrieve a entity(asset) create by the scan using its guid.
# I will explain how to find the guid in the portal very soon.
zone_data_land = client.get_entity(
    guid=&quot;0c75617c-xxxx-4e1f-b972-655e0a09e0b7&quot;
)

# Create a Atlas entity using the information of the asset retrieved by its id in the previous block.
# In this case we are creating the input entity
input_zone = AtlasEntity(
    name=zone_data_land[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;name&quot;],
    typeName=zone_data_land[&quot;entities&quot;][0][&quot;typeName&quot;],
    qualified_name=zone_data_land[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;qualifiedName&quot;],
    guid=&quot;0c75617c-xxxx-4e1f-b972-655e0a09e0b7&quot;
)

# This block uses the client to retrieve a entity(asset) create by the scan using its guid.
zone_data_ingested = client.get_entity(
    guid=&quot;d714587f-xxxx-4190-8f5f-54dfe27a58b5&quot;
)

# Create a Atlas entity using the information of the asset retrieved by in id in the previous block.
# In this case we are creating the output entity generated by the process
output_zone = AtlasEntity(
    name=zone_data_ingested[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;name&quot;],
    typeName=zone_data_ingested[&quot;entities&quot;][0][&quot;typeName&quot;],
    qualified_name=zone_data_ingested[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;qualifiedName&quot;],
    guid=&quot;d714587f-xxxx-4190-8f5f-54dfe27a58b5&quot;
)

# Create a Atlas process that links the input and the output
# Note the input and output parameter of AtlasProcess class, it is list type but at the moment there is only one object in it.
zone_process = AtlasProcess(
    name=&quot;zone_ingestion&quot;,
    typeName=&quot;Process&quot;,
    qualified_name=&quot;pyspark://zone_ingestion&quot;,
    inputs=[input_zone],
    outputs=[output_zone],
    guid=f&quot;-{uuid.uuid4()}&quot;
)

# Create the process and upload it.
results = client.upload_entities(
    batch=[input_zone, output_zone, zone_process]
)
</code></pre><p>The above code requires you to know the guid of the entity that you wanted to bind to the process.
It is fairly simple to get by navigating to purview, simply go the asset that you are interested in and you can find the guid in the browser link.
This seems to be a bit &ldquo;anti-automation&rdquo; but with some good investigation to the pyapacheatlas, you would be able to retrieve the entity by its &ldquo;name&rdquo; and &ldquo;typeName&rdquo;. The official documentation is your best friend to investigate the automation for this part.
For our lab purpose lets find it from the browser link.
You can now see these two datasets are the one used in the input and output.
<img loading=lazy src=/posts/purview-lineage-manual/guid1.png alt=guid1>
Note the guid which was used in looking up the input dataset
<img loading=lazy src=/posts/purview-lineage-manual/guid2.png alt=guid2>
When finished with python setup simply run the code snippet we&rsquo;v got so far.
If everything is happening as intended, you should be able to see the lineage when you navigate to the lineage tab of your zone data asset.
<img loading=lazy src=/posts/purview-lineage-manual/lineage1.png alt=lineage1>
The following code is pretty much the same to the previous block, only difference is that it is building the lineage for the other trip dataset that we are going to use in the join. So I will skip explanation for this part.</p><pre><code>trip_data_land=client.get_entity(
    guid=&quot;5e0a1aeb-xxxx-4deb-86aa-f1ce8945e6db&quot;
)

input_trip = AtlasEntity(
    name=trip_data_land[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;name&quot;],
    typeName=trip_data_land[&quot;entities&quot;][0][&quot;typeName&quot;],
    qualified_name=trip_data_land[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;qualifiedName&quot;],
    guid=&quot;5e0a1aeb-xxxx-4deb-86aa-f1ce8945e6db&quot;
)

trip_data_ingested=client.get_entity(
    guid=&quot;d68138fe-xxxx-454f-9d26-581af01787f2&quot;
)

output_trip = AtlasEntity(
    name=trip_data_ingested[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;name&quot;],
    typeName=trip_data_ingested[&quot;entities&quot;][0][&quot;typeName&quot;],
    qualified_name=trip_data_ingested[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;qualifiedName&quot;],
    guid=&quot;d68138fe-xxxx-454f-9d26-581af01787f2&quot;
)

trip_process = AtlasProcess(
    name=&quot;trip_ingestion&quot;,
    typeName=&quot;Process&quot;,
    qualified_name=&quot;pyspark://trip_ingestion&quot;,
    inputs=[input_trip],
    outputs=[output_trip],
    guid=f&quot;-{uuid.uuid4()}&quot;
)

results = client.upload_entities(
    batch=[input_trip, output_trip, trip_process]
)
</code></pre><p>When completed adding the snippet, please run the python script again and inspect to make sure you&rsquo;v got lineage for the trip dataset.</p><p>The very last bit is to build the lineage graph for the join action.
The logic behind this is the same, this time we are using the output entity as the input entity for the next join process.
Note the &ldquo;input&rdquo; and &ldquo;output&rdquo; parameter in the &ldquo;AtalasProcess&rdquo; object, this time it includes two items to reflect a join action.
When you finish the coding, run the completed script again and you should be able to see the lineage graph you see at the beginning of the section.</p><pre><code>
trip_data_curated=client.get_entity(
    guid=&quot;6bb1ed88-xxxx-4b9a-bb0f-96c37c35b57f&quot;
)

output_trip_curated=AtlasEntity(
    name=trip_data_curated[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;name&quot;],
    typeName=trip_data_curated[&quot;entities&quot;][0][&quot;typeName&quot;],
    qualified_name=trip_data_curated[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;qualifiedName&quot;],
    guid=&quot;6bb1ed88-xxxx-4b9a-bb0f-96c37c35b57f&quot;
)

input_trip_curation = AtlasEntity(
    name=trip_data_ingested[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;name&quot;],
    typeName=trip_data_ingested[&quot;entities&quot;][0][&quot;typeName&quot;],
    qualified_name=trip_data_ingested[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;qualifiedName&quot;],
    guid=&quot;d68138fe-xxxx-454f-9d26-581af01787f2&quot;
)

input_zone_curation = AtlasEntity(
    name=zone_data_ingested[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;name&quot;],
    typeName=zone_data_ingested[&quot;entities&quot;][0][&quot;typeName&quot;],
    qualified_name=zone_data_ingested[&quot;entities&quot;][0][&quot;attributes&quot;][&quot;qualifiedName&quot;],
    guid=&quot;d714587f-xxxx-4190-8f5f-54dfe27a58b5&quot;
)

trip_process_curated = AtlasProcess(
    name=&quot;trip_curation&quot;,
    typeName=&quot;Process&quot;,
    qualified_name=&quot;pyspark://trip_curation&quot;,
    inputs=[input_trip_curation, input_zone_curation],
    outputs=[output_trip_curated],
    guid=f&quot;-{uuid.uuid4()}&quot;
)

results = client.upload_entities(
    batch=[input_trip_curation, input_zone_curation, output_trip_curated, trip_process_curated]
)
</code></pre><h2 id=closing-note>Closing Note<a hidden class=anchor aria-hidden=true href=#closing-note>#</a></h2><p>Congrats ! You&rsquo;v reached to the end of this post on manual lineage. Thank you for your patience too !
The code in this lab is only intended to be used as snippets, you will obviously need some careful scheme to turn it into production ready code. But you get the gist right ?! It only take &ldquo;little bit&rdquo; more efforts to make it perfect ! Hope by now you have a good idea of building manual lineage in such situation when the auto scan does not suite.</p><p>In part B of this topic on Purview lineage, I will explore how we can connect Purview to Databricks Hive Metadata store and see if we can capture the lineage by scanning it. Stay tuned and keep safe !</p></div><footer class=post-footer><ul class=post-tags><li><i class="fas fa-tags fa-sm"></i></li><li><a href=https://blacklabnz.github.io/tags/purview/>purview</a></li><li><a href=https://blacklabnz.github.io/tags/lineage/>lineage</a></li><li><a href=https://blacklabnz.github.io/tags/data-catalogue/>data catalogue</a></li><li><a href=https://blacklabnz.github.io/tags/pyapacheatlas/>pyapacheatlas</a></li></ul><nav class=paginav><a class=next href=https://blacklabnz.github.io/posts/databricks-row-security/><span class=title>Next Page »</span><br><span>Databricks row and column level security</span></a></nav><div class=share-buttons><a>Share via:</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Purview Lineage: Part A Databricks Manual Lineage on twitter" href="https://twitter.com/intent/tweet/?text=Purview%20Lineage%3a%20Part%20A%20Databricks%20Manual%20Lineage&url=https%3a%2f%2fblacklabnz.github.io%2fposts%2fpurview-lineage-manual%2f&hashtags=purview%2clineage%2cdatacatalogue%2cpyapacheatlas"><i class="fab fa-twitter fa-lg"></i></a>
<a target=_blank rel="noopener noreferrer" aria-label="share Purview Lineage: Part A Databricks Manual Lineage on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fblacklabnz.github.io%2fposts%2fpurview-lineage-manual%2f&title=Purview%20Lineage%3a%20Part%20A%20Databricks%20Manual%20Lineage&summary=Purview%20Lineage%3a%20Part%20A%20Databricks%20Manual%20Lineage&source=https%3a%2f%2fblacklabnz.github.io%2fposts%2fpurview-lineage-manual%2f"><i class="fab fa-linkedin-in fa-lg"></i></a></div></footer><div id=page-comments></div><script>let currentTheme=document.getElementsByClassName("dark")[0],utterance=document.createElement("script");utterance.src="https://utteranc.es/client.js",utterance.setAttribute("repo","blacklabnz/blacklabnz.github.io"),utterance.setAttribute("issue-term","pathname"),utterance.setAttribute("crossorigin","anonymous"),utterance.setAttribute("async",""),currentTheme?utterance.setAttribute("theme","photon-dark"):utterance.setAttribute("theme","github-light"),document.querySelector("#page-comments").innerHTML="",document.querySelector("#page-comments").appendChild(utterance)</script><script src=/assets/js/utterance-theme.min.js></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://blacklabnz.github.io/>blacklabnz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><i class="fas fa-chevron-circle-up fa-2x"></i></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>