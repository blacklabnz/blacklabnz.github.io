<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Azure networking: Hub and spoke topology with terraform | blacklabnz | Data | DevOps</title><meta name=keywords content="terraform,hub and spoke,private endpoints,azure firewall,forced tunneling"><meta name=description content="The hub and spoke topology has been widely adopted for enterprise production deployment. In this lab, let put on our network/infrastructure engineer hat and get our hand dirty on Azure Hub and spoke topology with one of the popular IaC &ndash; Terraform. Lets have a look at the high level architecture first.
Overall architecture of the lab The essence of the topology is, by the name of it, having all traffic routed to hub before it gets forwarded to spoke."><meta name=author content="Neil Xu"><link rel=canonical href=https://blacklabnz.github.io/posts/hub-spoke/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.ed8304e9a7131d4ed812fbc7df21d2959f9ef0525fcc90f168c1e8f506768abc.css integrity="sha256-7YME6acTHU7YEvvH3yHSlZ+e8FJfzJDxaMHo9QZ2irw=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://blacklabnz.github.io/favicon16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blacklabnz.github.io/favicon32x32.png><link rel=mask-icon href=https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous><script async src="https://www.googletagmanager.com/gtag/js?id=G-FDV12HCDKK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-FDV12HCDKK",{anonymize_ip:!1})}</script><meta property="og:title" content="Azure networking: Hub and spoke topology with terraform"><meta property="og:description" content="The hub and spoke topology has been widely adopted for enterprise production deployment. In this lab, let put on our network/infrastructure engineer hat and get our hand dirty on Azure Hub and spoke topology with one of the popular IaC &ndash; Terraform. Lets have a look at the high level architecture first.
Overall architecture of the lab The essence of the topology is, by the name of it, having all traffic routed to hub before it gets forwarded to spoke."><meta property="og:type" content="article"><meta property="og:url" content="https://blacklabnz.github.io/posts/hub-spoke/"><meta property="og:image" content="https://blacklabnz.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-11T00:00:00+00:00"><meta property="article:modified_time" content="2022-04-11T00:00:00+00:00"><meta property="og:site_name" content="blacklabnz"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blacklabnz.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Azure networking: Hub and spoke topology with terraform"><meta name=twitter:description content="The hub and spoke topology has been widely adopted for enterprise production deployment. In this lab, let put on our network/infrastructure engineer hat and get our hand dirty on Azure Hub and spoke topology with one of the popular IaC &ndash; Terraform. Lets have a look at the high level architecture first.
Overall architecture of the lab The essence of the topology is, by the name of it, having all traffic routed to hub before it gets forwarded to spoke."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blacklabnz.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Azure networking: Hub and spoke topology with terraform","item":"https://blacklabnz.github.io/posts/hub-spoke/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Azure networking: Hub and spoke topology with terraform","name":"Azure networking: Hub and spoke topology with terraform","description":"The hub and spoke topology has been widely adopted for enterprise production deployment. In this lab, let put on our network/infrastructure engineer hat and get our hand dirty on Azure Hub and spoke topology with one of the popular IaC \u0026ndash; Terraform. Lets have a look at the high level architecture first.\nOverall architecture of the lab The essence of the topology is, by the name of it, having all traffic routed to hub before it gets forwarded to spoke.","keywords":["terraform","hub and spoke","private endpoints","azure firewall","forced tunneling"],"articleBody":"The hub and spoke topology has been widely adopted for enterprise production deployment. In this lab, let put on our network/infrastructure engineer hat and get our hand dirty on Azure Hub and spoke topology with one of the popular IaC – Terraform. Lets have a look at the high level architecture first.\nOverall architecture of the lab The essence of the topology is, by the name of it, having all traffic routed to hub before it gets forwarded to spoke. In hub network Azure firewall will be deployed, processing and logging all the traffic. In spoke user defined route is configured to route the traffic to hub, one the traffic arrives hub and processed by firewall, policy will be applied depending on the requirement to either allow or deny. We don’t have the luxury to have a on-prem VPN device with a VPN connection or express route, hence we are using another vNet and connect that to the hub via vNet gateway to simulate traffic passing through a gateway. Please refer to the following table for address spaces for each of the vent.\n   vnet address space subnet address prefix subnet name can be customized     blk-hub 10.0.0.0/16 GatewaySubnet 10.0.1.0/24 no     AzureFirewallSubnet 10.0.2.0/24 no     AzureFirewallManagementSubnet 10.0.3.0/24 no     jump 10.0.3.0/24 yes   blk-spoke-a 10.1.0.0/16 app 10.1.1.0/24 yes   blk-spoke-b 10.2.0.0/16 app 10.2.1.0/24 yes   blk-spoke-onprem 192.168.1.0/24 GatewaySubnet 192.168.1.0/26 no     GatewaySubnet 192.168.1.64/26 no     Please note Azure dictates some of the subnet names internally when trying to create resources such as firewalls and gateways. The table will tell you whether the name can be customized or not.  Once we have the network all setup we will manually create some VMs to do some ping tests and the analyze the firewall logs using Azure Log analytics. Without further ado let dive into each part.\nProject setup The following represents the file structures we have in the project.\n├── README.md ├── data.tf ├── main.tf ├── output.tf ├── providers.tf ├── values.tfvars ├── variables.tf └── .gitignore  For simplifying our demo, we are going to put all our terraform resources in the main.tf file. In real world development, the best practice would be modularize any components that are reusable. We are also simplifying the terraform variables with locals, you will find the only the authentication related values are kept in variables whose values get passed in from values.tfvars file.\nFor your convenience, you can download tf module required for this lab from my github repo: networking-hub-spoke\nFirst thing before carry on to the next part is to setup the providers.\n providers.tf\n terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"2.97.0\" } databricks = { source = \"databrickslabs/databricks\" version = \"0.5.1\" } external = { source = \"hashicorp/external\" version = \"2.2.2\" } } } provider \"azurerm\" { features {} subscription_id = var.subscription_id client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id }  Quick explanation We use service principal to authenticate to azure, in here we use variables to represent the value so that later on we can specify the values.tfvars file and pass the value in. This is really handy if you need to deal with multiple Azure environment that has different auth service principals. Refer the official doc here.\nTo be able to use variables we will need to add the following to the variables file:\n variable.tf\n variable \"subscription_id\" { type = string default = \"default\" } variable \"client_id\" { type = string default = \"default\" } variable \"client_secret\" { type = string default = \"default\" } variable \"tenant_id\" { type = string default = \"default\" }  Lastly you will need to create a service principal that has contributor access to the subscription that you are about to deploy to. You can find more details in this office doc. Once Service principal is created you will need to create a secret and use that alone side with the application id to authentication to Azure. When finished, please create a values.tfvars file where you could input those required auth information for terraform to pass in as variables. The information required are:\n subscription id service principal client id service principal client secret azure tenant id.   values.tfvars\n subscription_id = \"xxxxxxxxx\" client_id = \"xxxxxxxxx\" client_secret = \"xxxxxxxxx\" tenant_id = \"xxxxxxxxx\"  At this point we will need to initiate our terraform workspace by running:\nterraform init  With that all set, lets move on to next part of creating actual resources.\nPart 1 Create all vNets and subnets In this part we will create all the vNets and subnets associated with it. We will also create the peerings for each of the vNets.\n1.1 Create vNet First we will need to add some variables in the locals block as mentioned previously.  To simplify our lab setup, I am not creating any NSG, I'd encourage to embrace Security and add NSG if you are implementing this in a work environment.  Because some of our resources are repetitive in nature, and I could confess that I am lazy, I will use tf for_loop instead of copying/pasting similar code.\nPlease add the first block of locals in your file.\n main.tf\n locals { org = \"blk\" rg = \"${local.org}-hub-spoke\" rg_location = \"australiaeast\" vnets = { vnet_hub = { name = \"${local.org}-hub\", address_space = \"10.0.0.0/16\" } vnet_a = { name = \"${local.org}-spoke-a\", address_space = \"10.1.0.0/16\" } vnet_b = { name = \"${local.org}-spoke-b\", address_space = \"10.2.0.0/16\" } vnet_onprem = { name = \"${local.org}-spoke-onprem\", address_space = \"192.168.1.0/24\" } } }  Quick explanation Just wanted to quick go through how I setup the variables for all vNets. The vnets is a map object, essentially a key-value pair structure. If we look at the information of vent name and address spaces as the value of each of the vent properties, then the key would be the reference name of each of the vent property object. The information about each of the vent is grouped in such way that it can be iterated through the map object they form. when tf uses the vnet variable it would simply go into the value of eac key-value pair and retrieve the data from there.\nLets add the following to your main.tf and start creating first the resource group and the vNets\nresource \"azurerm_resource_group\" \"hub_spoke\" { name = local.rg location = local.rg_location } resource \"azurerm_virtual_network\" \"hub_spoke\" { for_each = local.vnets name = each.value[\"name\"] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name address_space = [each.value[\"address_space\"]] depends_on = [azurerm_resource_group.hub_spoke] }  Quick explanation In the second block we assign local.vnets which is map object to for_each expression, the expression is able to take each of the key-value pair and starting using them in the properties following it. You could then refer to each of the object in map by referring to key words each with syntax as the following:\nname = each.value[\"name\"]  The “each” keyword effectively points to every single key-value pair, the first one being\nvnet_hub = { name = \"${local.org}-hub\", address_space = \"10.0.0.0/16\" }  In this “each” object, the “key” is “vnet_hub” and the “value” is “{ name = “${local.org}-hub”, address_space = “10.0.0.0/16” }”, therefore if I need the “name” property in the “value” inside each of the object, I would have “each.value[“name”]”. Please refer to tf doc for more information on for_each.\nWith this all set, lets run our tf and deploy the resource for the first time. Simply go:\nterraform plan -var-file=values.tfvars  followed by\nterraform apply -auto-approve -var-file=values.tfvars  If all sets up correctly, you should be able to see all the required vNet being created in the resource group.\n1.2 Create subnets They way subnets are created is pretty much similar to how vents were created, using for_each expression. add the following to you locals in you main.tf file\n main.tf\n subnets = { hub_gw = { vnet = \"${local.org}-hub\", name = \"GatewaySubnet\", address_prefixes = \"10.0.1.0/24\" } hub_firewall = { vnet = \"${local.org}-hub\", name = \"AzureFirewallSubnet\", address_prefixes = \"10.0.2.0/24\" } hub_firewall_mgmt = { vnet = \"${local.org}-hub\", name = \"AzureFirewallManagementSubnet\", address_prefixes = \"10.0.3.0/24\" } hub_jumphost = { vnet = \"${local.org}-hub\", name = \"jump\", address_prefixes = \"10.0.4.0/24\" } a_app = { vnet = \"${local.org}-spoke-a\", name = \"app\", address_prefixes = \"10.1.1.0/24\" } b_app = { vnet = \"${local.org}-spoke-b\", name = \"app\", address_prefixes = \"10.2.1.0/24\" } onprem_gw = { vnet = \"${local.org}-spoke-onprem\", name = \"GatewaySubnet\", address_prefixes = \"192.168.1.0/26\" } onprem_app = { vnet = \"${local.org}-spoke-onprem\", name = \"app\", address_prefixes = \"192.168.1.64/26\" } }  Add the following resources to our main.tf as well\n main.tf\n resource \"azurerm_subnet\" \"hub-spoke\" { for_each = local.subnets name = each.value[\"name\"] resource_group_name = local.rg virtual_network_name = each.value[\"vnet\"] address_prefixes = [each.value[\"address_prefixes\"]] enforce_private_link_endpoint_network_policies = true depends_on = [azurerm_virtual_network.hub_spoke] }  Again run:\nterraform plan -var-file=values.tfvars  followed by\nterraform apply -auto-approve -var-file=values.tfvars  If all sets up correctly, you should be able to see all the required subnets created inside our vnets.\n1.3 Create peerings between vNets Lets create the peerings as well to wire up the spoke vents with hub vnets. Add the following vars in you locals\n main.tf\n peering = { hub_to_spoke_a = { name = \"${local.vnets.vnet_hub.name}-to-${local.vnets.vnet_a.name}\", vnet = \"vnet_hub\", remote = \"vnet_a\", use_remote_gw = false } spoke_a_to_hub = { name = \"${local.vnets.vnet_a.name}-to-${local.vnets.vnet_hub.name}\", vnet = \"vnet_a\", remote = \"vnet_hub\", use_remote_gw = true } hub_to_spoke_b = { name = \"${local.vnets.vnet_hub.name}-to-${local.vnets.vnet_b.name}\", vnet = \"vnet_hub\", remote = \"vnet_b\", use_remote_gw = false } spoke_b_to_hub = { name = \"${local.vnets.vnet_b.name}-to-${local.vnets.vnet_hub.name}\", vnet = \"vnet_b\", remote = \"vnet_hub\", use_remote_gw = true } }  as well as the following resources:\n main.tf\n resource \"azurerm_virtual_network_peering\" \"hub_to_spoke_a\" { for_each = local.peering name = each.value[\"name\"] resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.hub_spoke[each.value[\"vnet\"]].name remote_virtual_network_id = azurerm_virtual_network.hub_spoke[each.value[\"remote\"]].id allow_virtual_network_access = true allow_forwarded_traffic = true allow_gateway_transit = true use_remote_gateways = each.value[\"use_remote_gw\"] }  Let now run terraform plan and terraform apply to create the peering, please follow steps in previous part for the code snippet. Once done we should be able to validate the peering connection by inspecting the connection status. 1.3 Create onprem vnet to hub vnet VPN connection VNet peering will help us wiring up spokes with hub, for the part of simulating onprem network connecting to hub, we will need to setup the vNet to vNet VPN connection. The real world vNet to vNet VPN connection is commonly used to establish connectivity between vNet along side vNet peering. For the use case where use need to decide which one to choose, please refer to this doc. Lets add some vars to our locals block again.\ngw = { hub_gw = { name = \"${local.org}-hub-gw\", ip = \"${local.org}-hub-gw-ip\", vpn_name = \"${local.org}-onprem-hub\", peer = \"onprem_gw\" } onprem_gw = { name = \"${local.org}-onprem-gw\", ip = \"${local.org}-onprem-gw-ip\", vpn_name = \"${local.org}-hub-onprem\", peer = \"hub_gw\" } }  Quick explanation Again all the information needed for creating VPN connection is in the key-value pair. We will need to create a public IPs for the gateway resource, and once gateway resources are provisioned we will setup the connection between them. Add the following resources to your main.tf file\nresource \"azurerm_public_ip\" \"gw_ip\" { for_each = local.gw name = each.value[\"ip\"] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name allocation_method = \"Dynamic\" sku = \"Basic\" } resource \"azurerm_virtual_network_gateway\" \"gw\" { for_each = local.gw name = each.value[\"name\"] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name type = \"Vpn\" vpn_type = \"RouteBased\" active_active = false enable_bgp = false sku = \"VpnGw1\" ip_configuration { name = \"vnetGatewayConfig\" public_ip_address_id = azurerm_public_ip.gw_ip[each.key].id private_ip_address_allocation = \"Dynamic\" subnet_id = azurerm_subnet.hub-spoke[each.key].id } } resource \"azurerm_virtual_network_gateway_connection\" \"hub_onprem\" { for_each = local.gw name = each.value[\"vpn_name\"] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name type = \"Vnet2Vnet\" virtual_network_gateway_id = azurerm_virtual_network_gateway.gw[each.key].id peer_virtual_network_gateway_id = azurerm_virtual_network_gateway.gw[each.value[\"peer\"]].id shared_key = \"microsoft\" }  Quick explanation The first block of resources iteratively define the public IP addresses that are going to be used by the gateway resource. The second block of resources are the gateways, which will need to be deployed inside the GatewaySubnets of both onprem vNet and hub vNet. The third block of resources are the VPN connections, please not the the type is “Vnet2Vnet”. In a real world scenario you would setup your onprem to azure connectivity via site to site VPN or ideally express route.\nAt this point we should have our VPN all connected which can be verified by inspecting the connection status in the portal.\n1.4 Create firewall instance Next up lets create the firewall instance. The firewall instance we created has the forced tunneling option enabled. Please refer to Microsoft doc for more information on the tunneling option. With this option you will need a additional subnets in the hub vnet with is designated as the management subnet. So lets add more vars to our locals block:\nfw_name = \"${local.org}-azfw\" fw_policy = \"${local.org}-fw-policy\" fw_ip = { fw_ip = { name = \"${local.org}-fw-ip\" } fw_mgmt_ip = { name = \"${local.org}-fw-mgmt-ip\" } }  add add the following firewall to our main.tf as well.\nresource \"azurerm_firewall_policy\" \"fw_policy\" { name = local.fw_policy resource_group_name = azurerm_resource_group.hub_spoke.name location = local.rg_location } resource \"azurerm_public_ip\" \"fw_ip\" { for_each = local.fw_ip name = each.value[\"name\"] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name allocation_method = \"Static\" sku = \"Standard\" } resource \"azurerm_firewall\" \"fw\" { name = local.fw_name location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name firewall_policy_id = azurerm_firewall_policy.fw_policy.id ip_configuration { name = \"ipconfig\" subnet_id = azurerm_subnet.hub-spoke[\"hub_firewall\"].id public_ip_address_id = azurerm_public_ip.fw_ip[\"fw_ip\"].id } management_ip_configuration { name = \"mgmt_ipconfig\" subnet_id = azurerm_subnet.hub-spoke[\"hub_firewall_mgmt\"].id public_ip_address_id = azurerm_public_ip.fw_ip[\"fw_mgmt_ip\"].id } }  Quick explanation The first code block create a new firewall policy object. With The new policy resource sits outside of the firewall resource, if you need to redeploy the firewall for configuration changes, the policy resource remains untouched. Here we decided to create black policy object and attach to the firewall at time of creation. The second block of resources creates the public IPs used by the firewall. The third block of resource is obviously the firewall resource with reference to the firewall policy and the public Ips.\nLets also run terraform plan as well as terraform apply to get the firewall in our vnet.\n1.5 Create user defined route and firewall policies With all the resources setup and ready to go, the next step would be create some user defined routes and associated them to the vNet and subnets. For simplicity we will just focus on what needed to get the lab going, in a production environment you will have more complicated routes and firewall policies to meet your organization’s networking and security requirements. Let first add some vars to the locals. To begin with, we will need to create baseline policy rule collection which denies all traffice. If you need to understand the hierarchical strucutre of policy rules and policy rule collections, please refer to Microsoft doc.\nresource \"azurerm_firewall_policy_rule_collection_group\" \"fw_rules_deny\" { name = local.fw_rules_group_deny firewall_policy_id = azurerm_firewall_policy.fw_policy.id priority = 1000 network_rule_collection { name = \"deny_netowrk_rule_coll\" priority = 1000 action = \"Deny\" rule { name = \"deny_all\" protocols = [\"TCP\", \"UDP\", \"ICMP\"] source_addresses = [\"*\"] destination_addresses = [\"*\"] destination_ports = [\"*\"] } } }  secondly lets selectively apply some allow rules for our lab. Lets put our rules in plain words first and then create policy rules based on that. I have the following table to help me formulate my policies provide justification. Again when doing this in your production consult your network and security engineers.\n   from to action protocol reasoning     * * deny all Baseline policy to deny all traffic   my workstation * allow TCP If I create any jumphost in the hub, we need to be able to rdp into it from our work station   jumphost * allow TCP,UDP,ICMP Jumphost need to be able to connect to VMs in the spoke   vnet a vnet b/onprem allow TCP,UDP,ICMP This is a arbitrary rule that allow communication from vNet a to vNet b and onprem network   vnet b vnet a allow TCP,UDP,ICMP arbitrary rule that allows communication from vnet b to vnet a only   onprem * allow TCP,UDP,ICMP rule to allow onprem to connect to all vms in spokes    with all these in place, lets start building the firewall rules. Add the following to the main.tf\nresource \"azurerm_firewall_policy_rule_collection_group\" \"fw_rules_allow\" { name = local.fw_rules_group_allow firewall_policy_id = azurerm_firewall_policy.fw_policy.id priority = 500 network_rule_collection { name = \"allow_network_rule_coll\" priority = 500 action = \"Allow\" rule { name = \"allow_blk\" protocols = [\"TCP\"] source_addresses = [data.external.my_ip.result.ip] destination_addresses = [\"*\"] destination_ports = [\"*\"] } rule { name = \"allow_hub_jumphost\" protocols = [\"TCP\"] source_addresses = [local.subnets.hub_jumphost.address_prefixes] destination_addresses = [\"*\"] destination_ports = [\"*\"] } rule { name = \"allow_a_to_b_and_onprem\" protocols = [\"TCP\", \"UDP\", \"ICMP\"] source_addresses = [local.subnets.a_app.address_prefixes] destination_addresses = [local.subnets.b_app.address_prefixes, local.subnets.onprem_app.address_prefixes] destination_ports = [\"*\"] } rule { name = \"allow_b_to_a\" protocols = [\"TCP\", \"UDP\", \"ICMP\"] source_addresses = [local.subnets.b_app.address_prefixes] destination_addresses = [local.subnets.a_app.address_prefixes] destination_ports = [\"*\"] } rule { name = \"allow_onprem_to_all\" protocols = [\"TCP\", \"UDP\", \"ICMP\"] source_addresses = [local.subnets.onprem_app.address_prefixes] destination_addresses = [\"*\"] destination_ports = [\"*\"] } } nat_rule_collection { name = \"nat_rule_coll\" priority = 400 action = \"Dnat\" rule { name = \"jumphost_rdp\" protocols = [\"TCP\"] source_addresses = [data.external.my_ip.result.ip] destination_address = \"20.227.0.88\" destination_ports = [\"3387\"] translated_address = \"10.0.4.4\" translated_port = \"3389\" } } }  Quick explanation Each of the rule we had in the table will have a rule block to reflect what was planned. The last block is a nat rule where the firewall IP address can be used to NATted that to the jumphost private IP.\nLets run terraform plan and terraform apply.\nPart 3 Create some VMs to validate the traffic Lets create some vm in each of the app subnets and test the connectivity. For simplicity, create the VM with a public IP and access from your workstation.\n3.1 Create some VMs Manually create the VMs in the app network, for creating VMs with integration to subnet please refer to this doc.\n3.2 Ping tests Test 1 Connectivity from vNet_a. We are able to that VM in app subnet in vNet_a is able to ping VM in vNet_b and onprem.\nTest 2 Connectivity from onpmrem network. Part 4 Azure log analytics analysis We are also able to inspect the traffic once we enable Log Analytics for firewall. Follow this to enable diagnostic loggings. As shown on the screen shot we have firewall logging all the traffic including allowed and denied traffic. VM from vNet_a has connectivity to VMs in vNet_b and VMs in onprem network. VM from vNet_b has connectivity to VMs in vNet_b but not VMs in onprem network.\nCongrats! you’v reached the end of this post, thanks for your patience! Please leave your comments at the bottom if you find this post helpful! Cheers !\n","wordCount":"3024","inLanguage":"en","datePublished":"2022-04-11T00:00:00Z","dateModified":"2022-04-11T00:00:00Z","author":{"@type":"Person","name":"Neil Xu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blacklabnz.github.io/posts/hub-spoke/"},"publisher":{"@type":"Organization","name":"blacklabnz | Data | DevOps","logo":{"@type":"ImageObject","url":"https://blacklabnz.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><span class=header__inner><span class=logo><a href=https://blacklabnz.github.io/ accesskey=h title="blacklabnz (Alt + H)"><span><i class="fas fa-terminal fa-sm"></i></span>
<span class=logo_text>blacklabnz</span>
<span id=cursor></span></a></span>
<span class=header__right><div class=dropdown><span class=dropbtn><i class="fas fa-bars"></i></span><nav id=dropdonwBox class=dropdown-content><ul id=menu-mobile><li><a href=https://blacklabnz.github.io/posts title=blogs><span>blogs</span></a></li><li><a href=https://blacklabnz.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://blacklabnz.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://blacklabnz.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav><script src=/assets/js/menu.min.js></script></div><nav class=nav><ul id=menu><li><a href=https://blacklabnz.github.io/posts title=blogs><span>blogs</span></a></li><li><a href=https://blacklabnz.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://blacklabnz.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://blacklabnz.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav><ul class=logo-switches></ul><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></span></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blacklabnz.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://blacklabnz.github.io/posts/>Posts</a></div><h1 class=post-title>Azure networking: Hub and spoke topology with terraform</h1><div class=post-meta><span title="2022-04-11 00:00:00 +0000 UTC">Created April 11, 2022</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;Neil Xu&nbsp;|&nbsp;<a href=https://github.com/blacklabnz/blacklabnz.github.io/blob/main/content/posts/hub-spoke.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#overall-architecture-of-the-lab aria-label="Overall architecture of the lab">Overall architecture of the lab</a><ul><li><a href=#project-setup aria-label="Project setup">Project setup</a></li><li><a href=#quick-explanation aria-label="Quick explanation">Quick explanation</a></li></ul></li><li><a href=#part-1-create-all-vnets-and-subnets aria-label="Part 1 Create all vNets and subnets">Part 1 Create all vNets and subnets</a><ul><li><a href=#11-create-vnet aria-label="1.1 Create vNet">1.1 Create vNet</a></li><li><a href=#quick-explanation-1 aria-label="Quick explanation">Quick explanation</a></li><li><a href=#quick-explanation-2 aria-label="Quick explanation">Quick explanation</a></li><li><a href=#12-create-subnets aria-label="1.2 Create subnets">1.2 Create subnets</a></li><li><a href=#13-create-peerings-between-vnets aria-label="1.3 Create peerings between vNets">1.3 Create peerings between vNets</a></li><li><a href=#13-create-onprem-vnet-to-hub-vnet-vpn-connection aria-label="1.3 Create onprem vnet to hub vnet VPN connection">1.3 Create onprem vnet to hub vnet VPN connection</a></li><li><a href=#quick-explanation-3 aria-label="Quick explanation">Quick explanation</a></li><li><a href=#quick-explanation-4 aria-label="Quick explanation">Quick explanation</a></li><li><a href=#14-create-firewall-instance aria-label="1.4 Create firewall instance">1.4 Create firewall instance</a></li><li><a href=#quick-explanation-5 aria-label="Quick explanation">Quick explanation</a></li><li><a href=#15-create-user-defined-route-and-firewall-policies aria-label="1.5 Create user defined route and firewall policies">1.5 Create user defined route and firewall policies</a></li><li><a href=#quick-explanation-6 aria-label="Quick explanation">Quick explanation</a></li></ul></li><li><a href=#part-3-create-some-vms-to-validate-the-traffic aria-label="Part 3 Create some VMs to validate the traffic">Part 3 Create some VMs to validate the traffic</a><ul><li><a href=#31-create-some-vms aria-label="3.1 Create some VMs">3.1 Create some VMs</a></li><li><a href=#32-ping-tests aria-label="3.2 Ping tests">3.2 Ping tests</a></li><li><a href=#test-1-connectivity-from-vnet_a aria-label="Test 1 Connectivity from vNet_a.">Test 1 Connectivity from vNet_a.</a></li><li><a href=#test-2-connectivity-from-onpmrem-network aria-label="Test 2 Connectivity from onpmrem network.">Test 2 Connectivity from onpmrem network.</a></li></ul></li><li><a href=#part-4-azure-log-analytics-analysis aria-label="Part 4 Azure log analytics analysis">Part 4 Azure log analytics analysis</a></li></ul></div></details></div><div class=post-content><p>The hub and spoke topology has been widely adopted for enterprise production deployment.
In this lab, let put on our network/infrastructure engineer hat and get our hand dirty on Azure Hub and spoke topology with one of the popular IaC &ndash; Terraform.
Lets have a look at the high level architecture first.</p><h2 id=overall-architecture-of-the-lab>Overall architecture of the lab<a hidden class=anchor aria-hidden=true href=#overall-architecture-of-the-lab>#</a></h2><p><img loading=lazy src=/posts/hub-spoke/hub-spoke.png alt=hubandspoke>
The essence of the topology is, by the name of it, having all traffic routed to hub before it gets forwarded to spoke. In hub network Azure firewall will be deployed, processing and logging all the traffic.
In spoke user defined route is configured to route the traffic to hub, one the traffic arrives hub and processed by firewall, policy will be applied depending on the requirement to either allow or deny.
We don&rsquo;t have the luxury to have a on-prem VPN device with a VPN connection or express route, hence we are using another vNet and connect that to the hub via vNet gateway to simulate traffic passing through a gateway.
Please refer to the following table for address spaces for each of the vent.</p><table><thead><tr><th style=text-align:left>vnet</th><th style=text-align:left>address space</th><th style=text-align:left>subnet</th><th style=text-align:left>address prefix</th><th style=text-align:left>subnet name can be customized</th></tr></thead><tbody><tr><td style=text-align:left>blk-hub</td><td style=text-align:left>10.0.0.0/16</td><td style=text-align:left>GatewaySubnet</td><td style=text-align:left>10.0.1.0/24</td><td style=text-align:left>no</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>AzureFirewallSubnet</td><td style=text-align:left>10.0.2.0/24</td><td style=text-align:left>no</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>AzureFirewallManagementSubnet</td><td style=text-align:left>10.0.3.0/24</td><td style=text-align:left>no</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>jump</td><td style=text-align:left>10.0.3.0/24</td><td style=text-align:left>yes</td></tr><tr><td style=text-align:left>blk-spoke-a</td><td style=text-align:left>10.1.0.0/16</td><td style=text-align:left>app</td><td style=text-align:left>10.1.1.0/24</td><td style=text-align:left>yes</td></tr><tr><td style=text-align:left>blk-spoke-b</td><td style=text-align:left>10.2.0.0/16</td><td style=text-align:left>app</td><td style=text-align:left>10.2.1.0/24</td><td style=text-align:left>yes</td></tr><tr><td style=text-align:left>blk-spoke-onprem</td><td style=text-align:left>192.168.1.0/24</td><td style=text-align:left>GatewaySubnet</td><td style=text-align:left>192.168.1.0/26</td><td style=text-align:left>no</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left>GatewaySubnet</td><td style=text-align:left>192.168.1.64/26</td><td style=text-align:left>no</td></tr></tbody></table><p><div class=Note style=background-color:var(--note);font-size:15px;margin-bottom:10px;margin-top:10px;padding:10px;border-radius:10px><i class="fas fa-exclamation-triangle"></i>
Please note Azure dictates some of the subnet names internally when trying to create resources such as firewalls and gateways. The table will tell you whether the name can be customized or not.</div>Once we have the network all setup we will manually create some VMs to do some ping tests and the analyze the firewall logs using Azure Log analytics.
Without further ado let dive into each part.</p><h3 id=project-setup>Project setup<a hidden class=anchor aria-hidden=true href=#project-setup>#</a></h3><p>The following represents the file structures we have in the project.</p><pre><code class=language-bash>├── README.md
├── data.tf
├── main.tf
├── output.tf
├── providers.tf
├── values.tfvars
├── variables.tf
└── .gitignore
</code></pre><p>For simplifying our demo, we are going to put all our terraform resources in the main.tf file. In real world development, the best practice would be modularize any components that are reusable.
We are also simplifying the terraform variables with locals, you will find the only the authentication related values are kept in variables whose values get passed in from values.tfvars file.</p><p>For your convenience, you can download tf module required for this lab from my github repo: <a href=https://github.com/blacklabnz/networking-hub-spoke-terraform>networking-hub-spoke</a></p><p>First thing before carry on to the next part is to setup the providers.</p><blockquote><p>providers.tf</p></blockquote><pre><code class=language-bash>terraform {
  required_providers {
    azurerm = {
      source  = &quot;hashicorp/azurerm&quot;
      version = &quot;2.97.0&quot;
    }
    databricks = {
      source  = &quot;databrickslabs/databricks&quot;
      version = &quot;0.5.1&quot;
    }
    external = {
      source  = &quot;hashicorp/external&quot;
      version = &quot;2.2.2&quot;
    }
  }
}

provider &quot;azurerm&quot; {
  features {}
  subscription_id = var.subscription_id
  client_id       = var.client_id
  client_secret   = var.client_secret
  tenant_id       = var.tenant_id
}
</code></pre><h3 id=quick-explanation>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation>#</a></h3><p>We use service principal to authenticate to azure, in here we use variables to represent the value so that later on we can specify the values.tfvars file and pass the value in. This is really handy if you need to deal with multiple Azure environment that has different auth service principals. Refer the official doc <a href=https://www.terraform.io/language/values/variables>here</a>.</p><p>To be able to use variables we will need to add the following to the variables file:</p><blockquote><p>variable.tf</p></blockquote><pre><code class=language-bash>variable &quot;subscription_id&quot; {
  type = string
  default = &quot;default&quot;
} 
variable &quot;client_id&quot; {
  type = string
  default = &quot;default&quot;
}
variable &quot;client_secret&quot; {
  type = string
  default = &quot;default&quot;
}   
variable &quot;tenant_id&quot; {
  type = string
  default = &quot;default&quot;
}       
</code></pre><p>Lastly you will need to create a service principal that has contributor access to the subscription that you are about to deploy to. You can find more details in this office <a href=https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli>doc</a>.
Once Service principal is created you will need to create a secret and use that alone side with the application id to authentication to Azure. When finished, please create a values.tfvars file where you could input those required auth information for terraform to pass in as variables. The information required are:</p><ol><li>subscription id</li><li>service principal client id</li><li>service principal client secret</li><li>azure tenant id.</li></ol><blockquote><p>values.tfvars</p></blockquote><pre><code class=language-bash>subscription_id = &quot;xxxxxxxxx&quot;
client_id       = &quot;xxxxxxxxx&quot;
client_secret   = &quot;xxxxxxxxx&quot;
tenant_id       = &quot;xxxxxxxxx&quot;
</code></pre><p>At this point we will need to initiate our terraform workspace by running:</p><pre><code class=language-bash>terraform init
</code></pre><p>With that all set, lets move on to next part of creating actual resources.</p><h2 id=part-1-create-all-vnets-and-subnets>Part 1 Create all vNets and subnets<a hidden class=anchor aria-hidden=true href=#part-1-create-all-vnets-and-subnets>#</a></h2><p>In this part we will create all the vNets and subnets associated with it. We will also create the peerings for each of the vNets.</p><h3 id=11-create-vnet>1.1 Create vNet<a hidden class=anchor aria-hidden=true href=#11-create-vnet>#</a></h3><p>First we will need to add some variables in the locals block as mentioned previously.<div class=Note style=background-color:var(--note);font-size:15px;margin-bottom:10px;margin-top:10px;padding:10px;border-radius:10px><i class="fas fa-exclamation-triangle"></i>
To simplify our lab setup, I am not creating any NSG, I'd encourage to embrace Security and add NSG if you are implementing this in a work environment.</div>Because some of our resources are repetitive in nature, and I could confess that I am lazy, I will use tf for_loop instead of copying/pasting similar code.</p><p>Please add the first block of locals in your file.</p><blockquote><p>main.tf</p></blockquote><pre><code class=language-bash>locals {
  org         = &quot;blk&quot;
  rg          = &quot;${local.org}-hub-spoke&quot;
  rg_location = &quot;australiaeast&quot;

  vnets = {
    vnet_hub    = { name = &quot;${local.org}-hub&quot;, address_space = &quot;10.0.0.0/16&quot; }
    vnet_a      = { name = &quot;${local.org}-spoke-a&quot;, address_space = &quot;10.1.0.0/16&quot; }
    vnet_b      = { name = &quot;${local.org}-spoke-b&quot;, address_space = &quot;10.2.0.0/16&quot; }
    vnet_onprem = { name = &quot;${local.org}-spoke-onprem&quot;, address_space = &quot;192.168.1.0/24&quot; }
  }
}
</code></pre><h3 id=quick-explanation-1>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-1>#</a></h3><p>Just wanted to quick go through how I setup the variables for all vNets. The vnets is a map object, essentially a key-value pair structure. If we look at the information of vent name and address spaces as the value of each of the vent properties, then the key would be the reference name of each of the vent property object. The information about each of the vent is grouped in such way that it can be iterated through the map object they form.
when tf uses the vnet variable it would simply go into the value of eac key-value pair and retrieve the data from there.</p><p>Lets add the following to your main.tf and start creating first the resource group and the vNets</p><pre><code class=language-bash>resource &quot;azurerm_resource_group&quot; &quot;hub_spoke&quot; {
  name     = local.rg
  location = local.rg_location
}

resource &quot;azurerm_virtual_network&quot; &quot;hub_spoke&quot; {
  for_each            = local.vnets
  name                = each.value[&quot;name&quot;]
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.hub_spoke.name
  address_space       = [each.value[&quot;address_space&quot;]]
  depends_on          = [azurerm_resource_group.hub_spoke]
}
</code></pre><h3 id=quick-explanation-2>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-2>#</a></h3><p>In the second block we assign local.vnets which is map object to for_each expression, the expression is able to take each of the key-value pair and starting using them in the properties following it. You could then refer to each of the object in map by referring to key words each with syntax as the following:</p><pre><code>name = each.value[&quot;name&quot;]
</code></pre><p>The &ldquo;each&rdquo; keyword effectively points to every single key-value pair, the first one being</p><pre><code>vnet_hub = { name = &quot;${local.org}-hub&quot;, address_space = &quot;10.0.0.0/16&quot; }
</code></pre><p>In this &ldquo;each&rdquo; object, the &ldquo;key&rdquo; is &ldquo;vnet_hub&rdquo; and the &ldquo;value&rdquo; is &ldquo;{ name = &ldquo;${local.org}-hub&rdquo;, address_space = &ldquo;10.0.0.0/16&rdquo; }&rdquo;, therefore if I need the &ldquo;name&rdquo; property in the &ldquo;value&rdquo; inside each of the object, I would have &ldquo;each.value[&ldquo;name&rdquo;]&rdquo;. Please refer to tf <a href=https://www.terraform.io/language/meta-arguments/for_each>doc</a> for more information on for_each.</p><p>With this all set, lets run our tf and deploy the resource for the first time.
Simply go:</p><pre><code>terraform plan -var-file=values.tfvars
</code></pre><p>followed by</p><pre><code class=language-bash>terraform apply -auto-approve -var-file=values.tfvars
</code></pre><p>If all sets up correctly, you should be able to see all the required vNet being created in the resource group.</p><h3 id=12-create-subnets>1.2 Create subnets<a hidden class=anchor aria-hidden=true href=#12-create-subnets>#</a></h3><p>They way subnets are created is pretty much similar to how vents were created, using for_each expression.
add the following to you locals in you main.tf file</p><blockquote><p>main.tf</p></blockquote><pre><code class=language-bash>subnets = {
  hub_gw            = { vnet = &quot;${local.org}-hub&quot;, name = &quot;GatewaySubnet&quot;, address_prefixes = &quot;10.0.1.0/24&quot; }
  hub_firewall      = { vnet = &quot;${local.org}-hub&quot;, name = &quot;AzureFirewallSubnet&quot;, address_prefixes = &quot;10.0.2.0/24&quot; }
  hub_firewall_mgmt = { vnet = &quot;${local.org}-hub&quot;, name = &quot;AzureFirewallManagementSubnet&quot;, address_prefixes = &quot;10.0.3.0/24&quot; }
  hub_jumphost      = { vnet = &quot;${local.org}-hub&quot;, name = &quot;jump&quot;, address_prefixes = &quot;10.0.4.0/24&quot; }
  a_app             = { vnet = &quot;${local.org}-spoke-a&quot;, name = &quot;app&quot;, address_prefixes = &quot;10.1.1.0/24&quot; }
  b_app             = { vnet = &quot;${local.org}-spoke-b&quot;, name = &quot;app&quot;, address_prefixes = &quot;10.2.1.0/24&quot; }
  onprem_gw         = { vnet = &quot;${local.org}-spoke-onprem&quot;, name = &quot;GatewaySubnet&quot;, address_prefixes = &quot;192.168.1.0/26&quot; }
  onprem_app        = { vnet = &quot;${local.org}-spoke-onprem&quot;, name = &quot;app&quot;, address_prefixes = &quot;192.168.1.64/26&quot; }
}
</code></pre><p>Add the following resources to our main.tf as well</p><blockquote><p>main.tf</p></blockquote><pre><code class=language-bash>resource &quot;azurerm_subnet&quot; &quot;hub-spoke&quot; {
  for_each                                       = local.subnets
  name                                           = each.value[&quot;name&quot;]
  resource_group_name                            = local.rg
  virtual_network_name                           = each.value[&quot;vnet&quot;]
  address_prefixes                               = [each.value[&quot;address_prefixes&quot;]]
  enforce_private_link_endpoint_network_policies = true

  depends_on = [azurerm_virtual_network.hub_spoke]
}
</code></pre><p>Again run:</p><pre><code>terraform plan -var-file=values.tfvars
</code></pre><p>followed by</p><pre><code class=language-bash>terraform apply -auto-approve -var-file=values.tfvars
</code></pre><p>If all sets up correctly, you should be able to see all the required subnets created inside our vnets.</p><h3 id=13-create-peerings-between-vnets>1.3 Create peerings between vNets<a hidden class=anchor aria-hidden=true href=#13-create-peerings-between-vnets>#</a></h3><p>Lets create the peerings as well to wire up the spoke vents with hub vnets.
Add the following vars in you locals</p><blockquote><p>main.tf</p></blockquote><pre><code class=language-bash>peering = {
  hub_to_spoke_a = { name = &quot;${local.vnets.vnet_hub.name}-to-${local.vnets.vnet_a.name}&quot;, vnet = &quot;vnet_hub&quot;, remote = &quot;vnet_a&quot;, use_remote_gw = false }
  spoke_a_to_hub = { name = &quot;${local.vnets.vnet_a.name}-to-${local.vnets.vnet_hub.name}&quot;, vnet = &quot;vnet_a&quot;, remote = &quot;vnet_hub&quot;, use_remote_gw = true }
  hub_to_spoke_b = { name = &quot;${local.vnets.vnet_hub.name}-to-${local.vnets.vnet_b.name}&quot;, vnet = &quot;vnet_hub&quot;, remote = &quot;vnet_b&quot;, use_remote_gw = false }
  spoke_b_to_hub = { name = &quot;${local.vnets.vnet_b.name}-to-${local.vnets.vnet_hub.name}&quot;, vnet = &quot;vnet_b&quot;, remote = &quot;vnet_hub&quot;, use_remote_gw = true }
}
</code></pre><p>as well as the following resources:</p><blockquote><p>main.tf</p></blockquote><pre><code class=language-bash>resource &quot;azurerm_virtual_network_peering&quot; &quot;hub_to_spoke_a&quot; {
  for_each                     = local.peering
  name                         = each.value[&quot;name&quot;]
  resource_group_name          = local.rg
  virtual_network_name         = azurerm_virtual_network.hub_spoke[each.value[&quot;vnet&quot;]].name
  remote_virtual_network_id    = azurerm_virtual_network.hub_spoke[each.value[&quot;remote&quot;]].id
  allow_virtual_network_access = true
  allow_forwarded_traffic      = true
  allow_gateway_transit        = true
  use_remote_gateways          = each.value[&quot;use_remote_gw&quot;]
}
</code></pre><p>Let now run terraform plan and terraform apply to create the peering, please follow steps in previous part for the code snippet.
Once done we should be able to validate the peering connection by inspecting the connection status.
<img loading=lazy src=/posts/hub-spoke/peering.png alt=peering></p><h3 id=13-create-onprem-vnet-to-hub-vnet-vpn-connection>1.3 Create onprem vnet to hub vnet VPN connection<a hidden class=anchor aria-hidden=true href=#13-create-onprem-vnet-to-hub-vnet-vpn-connection>#</a></h3><p>VNet peering will help us wiring up spokes with hub, for the part of simulating onprem network connecting to hub, we will need to setup the vNet to vNet VPN connection.
The real world vNet to vNet VPN connection is commonly used to establish connectivity between vNet along side vNet peering. For the use case where use need to decide which one to choose, please refer to this <a href=https://azure.microsoft.com/en-us/blog/vnet-peering-and-vpn-gateways/>doc</a>.
Lets add some vars to our locals block again.</p><pre><code class=language-bash>gw = {
  hub_gw    = { name = &quot;${local.org}-hub-gw&quot;, ip = &quot;${local.org}-hub-gw-ip&quot;, vpn_name = &quot;${local.org}-onprem-hub&quot;, peer = &quot;onprem_gw&quot; }
  onprem_gw = { name = &quot;${local.org}-onprem-gw&quot;, ip = &quot;${local.org}-onprem-gw-ip&quot;, vpn_name = &quot;${local.org}-hub-onprem&quot;, peer = &quot;hub_gw&quot; }
}
</code></pre><h3 id=quick-explanation-3>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-3>#</a></h3><p>Again all the information needed for creating VPN connection is in the key-value pair. We will need to create a public IPs for the gateway resource, and once gateway resources are provisioned we will setup the connection between them.
Add the following resources to your main.tf file</p><pre><code class=language-bash>resource &quot;azurerm_public_ip&quot; &quot;gw_ip&quot; {
  for_each            = local.gw
  name                = each.value[&quot;ip&quot;]
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.hub_spoke.name
  allocation_method   = &quot;Dynamic&quot;
  sku                 = &quot;Basic&quot;
}

resource &quot;azurerm_virtual_network_gateway&quot; &quot;gw&quot; {
  for_each            = local.gw
  name                = each.value[&quot;name&quot;]
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.hub_spoke.name

  type     = &quot;Vpn&quot;
  vpn_type = &quot;RouteBased&quot;

  active_active = false
  enable_bgp    = false
  sku           = &quot;VpnGw1&quot;

  ip_configuration {
    name                          = &quot;vnetGatewayConfig&quot;
    public_ip_address_id          = azurerm_public_ip.gw_ip[each.key].id
    private_ip_address_allocation = &quot;Dynamic&quot;
    subnet_id                     = azurerm_subnet.hub-spoke[each.key].id
  }
}

resource &quot;azurerm_virtual_network_gateway_connection&quot; &quot;hub_onprem&quot; {
  for_each            = local.gw
  name                = each.value[&quot;vpn_name&quot;]
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.hub_spoke.name

  type                            = &quot;Vnet2Vnet&quot;
  virtual_network_gateway_id      = azurerm_virtual_network_gateway.gw[each.key].id
  peer_virtual_network_gateway_id = azurerm_virtual_network_gateway.gw[each.value[&quot;peer&quot;]].id

  shared_key = &quot;microsoft&quot;
}

</code></pre><h3 id=quick-explanation-4>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-4>#</a></h3><p>The first block of resources iteratively define the public IP addresses that are going to be used by the gateway resource.
The second block of resources are the gateways, which will need to be deployed inside the GatewaySubnets of both onprem vNet and hub vNet.
The third block of resources are the VPN connections, please not the the type is &ldquo;Vnet2Vnet&rdquo;. In a real world scenario you would setup your onprem to azure connectivity via site to site VPN or ideally express route.</p><p>At this point we should have our VPN all connected which can be verified by inspecting the connection status in the portal.</p><h3 id=14-create-firewall-instance>1.4 Create firewall instance<a hidden class=anchor aria-hidden=true href=#14-create-firewall-instance>#</a></h3><p>Next up lets create the firewall instance. The firewall instance we created has the forced tunneling option enabled. Please refer to Microsoft <a href=https://docs.microsoft.com/en-us/azure/firewall/forced-tunneling>doc</a> for more information on the tunneling option. With this option you will need a additional subnets in the hub vnet with is designated as the management subnet.
So lets add more vars to our locals block:</p><pre><code class=language-bash>fw_name   = &quot;${local.org}-azfw&quot;
fw_policy = &quot;${local.org}-fw-policy&quot;

fw_ip = {
  fw_ip      = { name = &quot;${local.org}-fw-ip&quot; }
  fw_mgmt_ip = { name = &quot;${local.org}-fw-mgmt-ip&quot; }
}
</code></pre><p>add add the following firewall to our main.tf as well.</p><pre><code class=language-bash>resource &quot;azurerm_firewall_policy&quot; &quot;fw_policy&quot; {
  name                = local.fw_policy
  resource_group_name = azurerm_resource_group.hub_spoke.name
  location            = local.rg_location
}

resource &quot;azurerm_public_ip&quot; &quot;fw_ip&quot; {
  for_each            = local.fw_ip
  name                = each.value[&quot;name&quot;]
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.hub_spoke.name
  allocation_method   = &quot;Static&quot;
  sku                 = &quot;Standard&quot;
}

resource &quot;azurerm_firewall&quot; &quot;fw&quot; {
  name                = local.fw_name
  location            = local.rg_location
  resource_group_name = azurerm_resource_group.hub_spoke.name
  firewall_policy_id  = azurerm_firewall_policy.fw_policy.id

  ip_configuration {
    name                 = &quot;ipconfig&quot;
    subnet_id            = azurerm_subnet.hub-spoke[&quot;hub_firewall&quot;].id
    public_ip_address_id = azurerm_public_ip.fw_ip[&quot;fw_ip&quot;].id
  }

  management_ip_configuration {
    name                 = &quot;mgmt_ipconfig&quot;
    subnet_id            = azurerm_subnet.hub-spoke[&quot;hub_firewall_mgmt&quot;].id
    public_ip_address_id = azurerm_public_ip.fw_ip[&quot;fw_mgmt_ip&quot;].id
  }
}
</code></pre><h3 id=quick-explanation-5>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-5>#</a></h3><p>The first code block create a new firewall policy object.
With The new policy resource sits outside of the firewall resource, if you need to redeploy the firewall for configuration changes, the policy resource remains untouched. Here we decided to create black policy object and attach to the firewall at time of creation.
The second block of resources creates the public IPs used by the firewall.
The third block of resource is obviously the firewall resource with reference to the firewall policy and the public Ips.</p><p>Lets also run terraform plan as well as terraform apply to get the firewall in our vnet.</p><h3 id=15-create-user-defined-route-and-firewall-policies>1.5 Create user defined route and firewall policies<a hidden class=anchor aria-hidden=true href=#15-create-user-defined-route-and-firewall-policies>#</a></h3><p>With all the resources setup and ready to go, the next step would be create some user defined routes and associated them to the vNet and subnets.
For simplicity we will just focus on what needed to get the lab going, in a production environment you will have more complicated routes and firewall policies to meet your organization&rsquo;s networking and security requirements.
Let first add some vars to the locals.
To begin with, we will need to create baseline policy rule collection which denies all traffice. If you need to understand the hierarchical strucutre of policy rules and policy rule collections, please refer to Microsoft <a href=https://docs.microsoft.com/en-us/azure/firewall/rule-processing>doc</a>.</p><pre><code class=language-bash>resource &quot;azurerm_firewall_policy_rule_collection_group&quot; &quot;fw_rules_deny&quot; {
  name               = local.fw_rules_group_deny
  firewall_policy_id = azurerm_firewall_policy.fw_policy.id
  priority           = 1000

  network_rule_collection {
    name     = &quot;deny_netowrk_rule_coll&quot;
    priority = 1000
    action   = &quot;Deny&quot;
    rule {
      name                  = &quot;deny_all&quot;
      protocols             = [&quot;TCP&quot;, &quot;UDP&quot;, &quot;ICMP&quot;]
      source_addresses      = [&quot;*&quot;]
      destination_addresses = [&quot;*&quot;]
      destination_ports     = [&quot;*&quot;]
    }
  }
}
</code></pre><p>secondly lets selectively apply some allow rules for our lab. Lets put our rules in plain words first and then create policy rules based on that. I have the following table to help me formulate my policies provide justification. Again when doing this in your production consult your network and security engineers.</p><table><thead><tr><th style=text-align:left>from</th><th style=text-align:left>to</th><th style=text-align:left>action</th><th style=text-align:left>protocol</th><th style=text-align:left>reasoning</th></tr></thead><tbody><tr><td style=text-align:left>*</td><td style=text-align:left>*</td><td style=text-align:left>deny</td><td style=text-align:left>all</td><td style=text-align:left>Baseline policy to deny all traffic</td></tr><tr><td style=text-align:left>my workstation</td><td style=text-align:left>*</td><td style=text-align:left>allow</td><td style=text-align:left>TCP</td><td style=text-align:left>If I create any jumphost in the hub, we need to be able to rdp into it from our work station</td></tr><tr><td style=text-align:left>jumphost</td><td style=text-align:left>*</td><td style=text-align:left>allow</td><td style=text-align:left>TCP,UDP,ICMP</td><td style=text-align:left>Jumphost need to be able to connect to VMs in the spoke</td></tr><tr><td style=text-align:left>vnet a</td><td style=text-align:left>vnet b/onprem</td><td style=text-align:left>allow</td><td style=text-align:left>TCP,UDP,ICMP</td><td style=text-align:left>This is a arbitrary rule that allow communication from vNet a to vNet b and onprem network</td></tr><tr><td style=text-align:left>vnet b</td><td style=text-align:left>vnet a</td><td style=text-align:left>allow</td><td style=text-align:left>TCP,UDP,ICMP</td><td style=text-align:left>arbitrary rule that allows communication from vnet b to vnet a only</td></tr><tr><td style=text-align:left>onprem</td><td style=text-align:left>*</td><td style=text-align:left>allow</td><td style=text-align:left>TCP,UDP,ICMP</td><td style=text-align:left>rule to allow onprem to connect to all vms in spokes</td></tr></tbody></table><p>with all these in place, lets start building the firewall rules.
Add the following to the main.tf</p><pre><code class=language-bash>resource &quot;azurerm_firewall_policy_rule_collection_group&quot; &quot;fw_rules_allow&quot; {
  name               = local.fw_rules_group_allow
  firewall_policy_id = azurerm_firewall_policy.fw_policy.id
  priority           = 500

  network_rule_collection {
    name     = &quot;allow_network_rule_coll&quot;
    priority = 500
    action   = &quot;Allow&quot;
    rule {
      name                  = &quot;allow_blk&quot;
      protocols             = [&quot;TCP&quot;]
      source_addresses      = [data.external.my_ip.result.ip]
      destination_addresses = [&quot;*&quot;]
      destination_ports     = [&quot;*&quot;]
    }

    rule {
      name                  = &quot;allow_hub_jumphost&quot;
      protocols             = [&quot;TCP&quot;]
      source_addresses      = [local.subnets.hub_jumphost.address_prefixes]
      destination_addresses = [&quot;*&quot;]
      destination_ports     = [&quot;*&quot;]
    }

    rule {
      name                  = &quot;allow_a_to_b_and_onprem&quot;
      protocols             = [&quot;TCP&quot;, &quot;UDP&quot;, &quot;ICMP&quot;]
      source_addresses      = [local.subnets.a_app.address_prefixes]
      destination_addresses = [local.subnets.b_app.address_prefixes, local.subnets.onprem_app.address_prefixes]
      destination_ports     = [&quot;*&quot;]
    }

    rule {
      name                  = &quot;allow_b_to_a&quot;
      protocols             = [&quot;TCP&quot;, &quot;UDP&quot;, &quot;ICMP&quot;]
      source_addresses      = [local.subnets.b_app.address_prefixes]
      destination_addresses = [local.subnets.a_app.address_prefixes]
      destination_ports     = [&quot;*&quot;]
    }

    rule {
      name                  = &quot;allow_onprem_to_all&quot;
      protocols             = [&quot;TCP&quot;, &quot;UDP&quot;, &quot;ICMP&quot;]
      source_addresses      = [local.subnets.onprem_app.address_prefixes]
      destination_addresses = [&quot;*&quot;]
      destination_ports     = [&quot;*&quot;]
    }
  }

  nat_rule_collection {
    name     = &quot;nat_rule_coll&quot;
    priority = 400
    action   = &quot;Dnat&quot;
    rule {
      name                = &quot;jumphost_rdp&quot;
      protocols           = [&quot;TCP&quot;]
      source_addresses    = [data.external.my_ip.result.ip]
      destination_address = &quot;20.227.0.88&quot;
      destination_ports   = [&quot;3387&quot;]
      translated_address  = &quot;10.0.4.4&quot;
      translated_port     = &quot;3389&quot;
    }
  }
}
</code></pre><h3 id=quick-explanation-6>Quick explanation<a hidden class=anchor aria-hidden=true href=#quick-explanation-6>#</a></h3><p>Each of the rule we had in the table will have a rule block to reflect what was planned.
The last block is a nat rule where the firewall IP address can be used to NATted that to the jumphost private IP.</p><p>Lets run terraform plan and terraform apply.</p><h2 id=part-3-create-some-vms-to-validate-the-traffic>Part 3 Create some VMs to validate the traffic<a hidden class=anchor aria-hidden=true href=#part-3-create-some-vms-to-validate-the-traffic>#</a></h2><p>Lets create some vm in each of the app subnets and test the connectivity. For simplicity, create the VM with a public IP and access from your workstation.</p><h3 id=31-create-some-vms>3.1 Create some VMs<a hidden class=anchor aria-hidden=true href=#31-create-some-vms>#</a></h3><p>Manually create the VMs in the app network, for creating VMs with integration to subnet please refer to this <a href=https://docs.microsoft.com/en-us/azure/virtual-machines/windows/quick-create-portal>doc</a>.</p><h3 id=32-ping-tests>3.2 Ping tests<a hidden class=anchor aria-hidden=true href=#32-ping-tests>#</a></h3><h3 id=test-1-connectivity-from-vnet_a>Test 1 Connectivity from vNet_a.<a hidden class=anchor aria-hidden=true href=#test-1-connectivity-from-vnet_a>#</a></h3><p><img loading=lazy src=/posts/hub-spoke/a-to-onprem-b.png alt=atobandonprem>
We are able to that VM in app subnet in vNet_a is able to ping VM in vNet_b and onprem.</p><h3 id=test-2-connectivity-from-onpmrem-network>Test 2 Connectivity from onpmrem network.<a hidden class=anchor aria-hidden=true href=#test-2-connectivity-from-onpmrem-network>#</a></h3><p><img loading=lazy src=/posts/hub-spoke/onprem-to-a.png alt=onpremtoa>
<img loading=lazy src=/posts/hub-spoke/onprem-to-b.png alt=onpremtob></p><h2 id=part-4-azure-log-analytics-analysis>Part 4 Azure log analytics analysis<a hidden class=anchor aria-hidden=true href=#part-4-azure-log-analytics-analysis>#</a></h2><p>We are also able to inspect the traffic once we enable Log Analytics for firewall. Follow <a href=https://docs.microsoft.com/en-us/azure/firewall/firewall-diagnostics#enable-diagnostic-logging-through-the-azure-portal>this</a> to enable diagnostic loggings.
<img loading=lazy src=/posts/hub-spoke/azfirewall.png alt=azfirewall>
As shown on the screen shot we have firewall logging all the traffic including allowed and denied traffic.
VM from vNet_a has connectivity to VMs in vNet_b and VMs in onprem network.
VM from vNet_b has connectivity to VMs in vNet_b but not VMs in onprem network.</p><p>Congrats! you&rsquo;v reached the end of this post, thanks for your patience! Please leave your comments at the bottom if you find this post helpful! Cheers !</p></div><footer class=post-footer><ul class=post-tags><li><i class="fas fa-tags fa-sm"></i></li><li><a href=https://blacklabnz.github.io/tags/terraform/>terraform</a></li><li><a href=https://blacklabnz.github.io/tags/hub-and-spoke/>hub and spoke</a></li><li><a href=https://blacklabnz.github.io/tags/private-endpoints/>private endpoints</a></li><li><a href=https://blacklabnz.github.io/tags/azure-firewall/>azure firewall</a></li><li><a href=https://blacklabnz.github.io/tags/forced-tunneling/>forced tunneling</a></li></ul><nav class=paginav><a class=prev href=https://blacklabnz.github.io/posts/databricks-row-security/><span class=title>« Prev Page</span><br><span>Databrick row and column level security</span></a>
<a class=next href=https://blacklabnz.github.io/posts/databricks-secure/><span class=title>Next Page »</span><br><span>Secure Databricks cluster with vNet injection and access resources via Azure private endpoint</span></a></nav><div class=share-buttons><a>Share via:</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Azure networking: Hub and spoke topology with terraform on twitter" href="https://twitter.com/intent/tweet/?text=Azure%20networking%3a%20Hub%20and%20spoke%20topology%20with%20terraform&url=https%3a%2f%2fblacklabnz.github.io%2fposts%2fhub-spoke%2f&hashtags=terraform%2chubandspoke%2cprivateendpoints%2cazurefirewall%2cforcedtunneling"><i class="fab fa-twitter fa-lg"></i></a>
<a target=_blank rel="noopener noreferrer" aria-label="share Azure networking: Hub and spoke topology with terraform on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fblacklabnz.github.io%2fposts%2fhub-spoke%2f&title=Azure%20networking%3a%20Hub%20and%20spoke%20topology%20with%20terraform&summary=Azure%20networking%3a%20Hub%20and%20spoke%20topology%20with%20terraform&source=https%3a%2f%2fblacklabnz.github.io%2fposts%2fhub-spoke%2f"><i class="fab fa-linkedin-in fa-lg"></i></a></div></footer><div id=page-comments></div><script>let currentTheme=document.getElementsByClassName("dark")[0],utterance=document.createElement("script");utterance.src="https://utteranc.es/client.js",utterance.setAttribute("repo","blacklabnz/blacklabnz.github.io"),utterance.setAttribute("issue-term","pathname"),utterance.setAttribute("crossorigin","anonymous"),utterance.setAttribute("async",''),currentTheme?utterance.setAttribute("theme","photon-dark"):utterance.setAttribute("theme","github-light"),document.querySelector("#page-comments").innerHTML='',document.querySelector("#page-comments").appendChild(utterance)</script><script src=/assets/js/utterance-theme.min.js></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://blacklabnz.github.io/>blacklabnz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><i class="fas fa-chevron-circle-up fa-2x"></i></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>