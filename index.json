[{"content":"Recently I had a chat with one of client regarding on access control of their reports and dashboards. Interestingly it was found out that client is currently doing this by creating similar reports and granting access to people in different security groups. Obviously this is not the best idea because of redundant reports, the ideal solution is to implement row and column level security on the table so that people in different access groups will have visibility to subsets of the rows in the table or view. Since I a big fan of Databricks, so in this lab lets explore row and column level security implementation and find out how easily it is done in Databricks.\nTable setup First lets pretend that we are data engineering some sensitive data, say, salary information in a company. Time to populate some data for our lab. Before doing this, of course, you will need to create interactive cluster so that all the compute would run on it. To find out more on how to create a cluster, please visit here. Lets create the salary table:\nCREATE TABLE test.salary ( id INT, firstName STRING, lastName STRING, gender STRING, position STRING, location STRING, salary INT, viewable_by STRING ) USING DELTA  Now populate some data in it:\nINSERT INTO test.salary VALUES (1, 'Billy', 'Tommie', 'M', 'staff', 'NZ', 55250, 'manager'), (2, 'Judie', 'Williams', 'F', 'manager', 'NZ', 65250, 'snrmgmt'), (3, 'Tom', 'Cruise', 'M', 'staff', 'USA', 75250, 'manager'), (4, 'May', 'Thompson', 'F', 'manager', 'USA', 85250, 'snrmgmt'), (5, 'David', 'Wang', 'M', 'staff', 'AUSSIE', 95250, 'manager'), (6, 'Sammie', 'Willies', 'F', 'staff', 'AUSSIE', 105250, 'manager'), (7, 'Jianguo', 'Liu', 'M', 'staff', 'CHINA', 115250, 'manager'), (8, 'li', 'Wu', 'F', 'cto', 'CHINA', 125250, 'finance')  With the data in place, let dive into it!\nRow level security in Databricks The key function in Databricks to achieve this is \u0026ldquo;is_member()\u0026rdquo;. How does it work ? The is_member(\u0026ldquo;group_name\u0026rdquo;) function takes string parameter as the group name, check if the current user belongs to it and then return true of false accordingly. for example\nSELECT is_member('super_group') as group  This query will check if the current user is a member of \u0026ldquo;super_group\u0026rdquo; and without and access grants, this will return false. We will use this property to achieve row-level security, please follow along!\nCreate groups One of the column in the data we populated is the \u0026ldquo;viewable_by\u0026rdquo;, the value in this column is used to instruct Databricks the row is visible to members in this group, so lets go to databricks access control and create them. To find out more on how to create group in Databricks, please refer to this. Update the query with is_member() function Let update out select query in the way that when our user is assigned to one of the groups created previously, the result set is filtered accordingly.\nSELECT * FROM employee.salary s WHERE is_member(viewable_by)  Quick explanation The is_member function is used to evaluate if the user is member of the group whose name equals to the value in the \u0026ldquo;viewable_by\u0026rdquo; column. Therefore if we did not assign the user to any group then no result will show in the result set.\nLets validate if that is the case by running the query. Sweet! As expected no result returned.\nLet add our current user account to \u0026ldquo;manager\u0026rdquo; group and see if we can filter all the results that \u0026ldquo;viewable_by\u0026rdquo; value is manager. Voilà! we can see all the records that viewable by manager, works as a charm! The \u0026ldquo;is_member()\u0026rdquo; function also works with role inheritance and composition. So lets do the following:\n remove user from \u0026ldquo;manager\u0026rdquo; group. add our current user to \u0026ldquo;snrmgmt\u0026rdquo; and the add \u0026ldquo;snrmgmt\u0026rdquo; to \u0026ldquo;manager\u0026rdquo; group.  We should be see record with \u0026ldquo;viewable_by\u0026rdquo; value of \u0026ldquo;snrmgmt\u0026rdquo;, \u0026ldquo;manager\u0026rdquo;, \u0026ldquo;finance\u0026rdquo;. So lets find it out. Perfect! Again no unwanted surprises what so ever !\nVisualize in PowerBI Let create view for PowerBI with \u0026ldquo;is_member()\u0026rdquo; so that when PowerBI connects it would evaluate the user membership in databricks. Both databricks and PowerBI could use the same Azure Ad account, therefore what works in databrick should work for PowerBI as long as we connect it to the right view.\nCREATE or replace VIEW employee.staff_salary_view AS SELECT * FROM employee.salary s WHERE is_member(viewable_by)  Now create the dataset in PowerBi desktop, and you can find the value for \u0026ldquo;Server Hostname\u0026rdquo; and \u0026ldquo;SQL path\u0026rdquo; in the \u0026ldquo;JDBC/ODBC\u0026rdquo; tab under the \u0026ldquo;Advanced Options\u0026rdquo; of your cluster settings. After authenticate with your Azure account and everything was done correctly, we should be able to see the view we just created. Lets also drop our user account from the finance group just to re-confirm that the row level security is implemented. We should be able to see the following simple table visuals that we could still see records viewable by \u0026ldquo;manager\u0026rdquo; and \u0026ldquo;snrmgmt\u0026rdquo; Pretty straight forward isnt it ? All the complexity of achieving this in corporate environment lies in design of RBAC structure and the databricks implementation is just adding is_member() to your view!\nColumn level security in Databricks With lab success of row level security, lets move on to column level security. Basically it is the same concept, only different is that the location of is_member() function is on the column instead of row using where statement. Remember previously we remove our user from the finance group right ? So lets use this as the filtering condition to hide the salary column.\nSELECT firstName, position, CASE WHEN is_member('finance') THEN salary ELSE 'REDACTED' END AS salary, viewable_by FROM employee.salary  Run this query and we will see now the salary column is redacted from the our current use as it is no long a member of the finance group. Similarly, if we create a view from this query, we would be able to see similar outcome in PowerBI as the previous step.\nAt this point our lab has reached the end, hope you enjoyed the exercise of security your delta tables in databricks.\nIf like the content please leave your comments below, if you find issues with the content, please also comment below. Thanks for your time and patience with me !!\n","permalink":"https://blacklabnz.github.io/posts/databricks-row-security/","summary":"Recently I had a chat with one of client regarding on access control of their reports and dashboards. Interestingly it was found out that client is currently doing this by creating similar reports and granting access to people in different security groups. Obviously this is not the best idea because of redundant reports, the ideal solution is to implement row and column level security on the table so that people in different access groups will have visibility to subsets of the rows in the table or view.","title":"Databricks row and column level security"},{"content":"The hub and spoke topology has been widely adopted for enterprise production deployment. In this lab, let put on our network/infrastructure engineer hat and get our hand dirty on Azure Hub and spoke topology with one of the popular IaC \u0026ndash; Terraform. Lets have a look at the high level architecture first.\nOverall architecture of the lab The essence of the topology is, by the name of it, having all traffic routed to hub before it gets forwarded to spoke. In hub network Azure firewall will be deployed, processing and logging all the traffic. In spoke user defined route is configured to route the traffic to hub, one the traffic arrives hub and processed by firewall, policy will be applied depending on the requirement to either allow or deny. We don\u0026rsquo;t have the luxury to have a on-prem VPN device with a VPN connection or express route, hence we are using another vNet and connect that to the hub via vNet gateway to simulate traffic passing through a gateway. Please refer to the following table for address spaces for each of the vent.\n   vnet address space subnet address prefix subnet name can be customized     blk-hub 10.0.0.0/16 GatewaySubnet 10.0.1.0/24 no     AzureFirewallSubnet 10.0.2.0/24 no     AzureFirewallManagementSubnet 10.0.3.0/24 no     jump 10.0.3.0/24 yes   blk-spoke-a 10.1.0.0/16 app 10.1.1.0/24 yes   blk-spoke-b 10.2.0.0/16 app 10.2.1.0/24 yes   blk-spoke-onprem 192.168.1.0/24 GatewaySubnet 192.168.1.0/26 no     GatewaySubnet 192.168.1.64/26 no     Please note Azure dictates some of the subnet names internally when trying to create resources such as firewalls and gateways. The table will tell you whether the name can be customized or not.  Once we have the network all setup we will manually create some VMs to do some ping tests and the analyze the firewall logs using Azure Log analytics. Without further ado let dive into each part.\nProject setup The following represents the file structures we have in the project.\n├── README.md ├── data.tf ├── main.tf ├── output.tf ├── providers.tf ├── values.tfvars ├── variables.tf └── .gitignore  For simplifying our demo, we are going to put all our terraform resources in the main.tf file. In real world development, the best practice would be modularize any components that are reusable. We are also simplifying the terraform variables with locals, you will find the only the authentication related values are kept in variables whose values get passed in from values.tfvars file.\nFor your convenience, you can download tf module required for this lab from my github repo: networking-hub-spoke\nFirst thing before carry on to the next part is to setup the providers.\n providers.tf\n terraform { required_providers { azurerm = { source = \u0026quot;hashicorp/azurerm\u0026quot; version = \u0026quot;2.97.0\u0026quot; } databricks = { source = \u0026quot;databrickslabs/databricks\u0026quot; version = \u0026quot;0.5.1\u0026quot; } external = { source = \u0026quot;hashicorp/external\u0026quot; version = \u0026quot;2.2.2\u0026quot; } } } provider \u0026quot;azurerm\u0026quot; { features {} subscription_id = var.subscription_id client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id }  Quick explanation We use service principal to authenticate to azure, in here we use variables to represent the value so that later on we can specify the values.tfvars file and pass the value in. This is really handy if you need to deal with multiple Azure environment that has different auth service principals. Refer the official doc here.\nTo be able to use variables we will need to add the following to the variables file:\n variable.tf\n variable \u0026quot;subscription_id\u0026quot; { type = string default = \u0026quot;default\u0026quot; } variable \u0026quot;client_id\u0026quot; { type = string default = \u0026quot;default\u0026quot; } variable \u0026quot;client_secret\u0026quot; { type = string default = \u0026quot;default\u0026quot; } variable \u0026quot;tenant_id\u0026quot; { type = string default = \u0026quot;default\u0026quot; }  Lastly you will need to create a service principal that has contributor access to the subscription that you are about to deploy to. You can find more details in this office doc. Once Service principal is created you will need to create a secret and use that alone side with the application id to authentication to Azure. When finished, please create a values.tfvars file where you could input those required auth information for terraform to pass in as variables. The information required are:\n subscription id service principal client id service principal client secret azure tenant id.   values.tfvars\n subscription_id = \u0026quot;xxxxxxxxx\u0026quot; client_id = \u0026quot;xxxxxxxxx\u0026quot; client_secret = \u0026quot;xxxxxxxxx\u0026quot; tenant_id = \u0026quot;xxxxxxxxx\u0026quot;  At this point we will need to initiate our terraform workspace by running:\nterraform init  With that all set, lets move on to next part of creating actual resources.\nPart 1 Create all vNets and subnets In this part we will create all the vNets and subnets associated with it. We will also create the peerings for each of the vNets.\n1.1 Create vNet First we will need to add some variables in the locals block as mentioned previously.  To simplify our lab setup, I am not creating any NSG, I'd encourage to embrace Security and add NSG if you are implementing this in a work environment.  Because some of our resources are repetitive in nature, and I could confess that I am lazy, I will use tf for_loop instead of copying/pasting similar code.\nPlease add the first block of locals in your file.\n main.tf\n locals { org = \u0026quot;blk\u0026quot; rg = \u0026quot;${local.org}-hub-spoke\u0026quot; rg_location = \u0026quot;australiaeast\u0026quot; vnets = { vnet_hub = { name = \u0026quot;${local.org}-hub\u0026quot;, address_space = \u0026quot;10.0.0.0/16\u0026quot; } vnet_a = { name = \u0026quot;${local.org}-spoke-a\u0026quot;, address_space = \u0026quot;10.1.0.0/16\u0026quot; } vnet_b = { name = \u0026quot;${local.org}-spoke-b\u0026quot;, address_space = \u0026quot;10.2.0.0/16\u0026quot; } vnet_onprem = { name = \u0026quot;${local.org}-spoke-onprem\u0026quot;, address_space = \u0026quot;192.168.1.0/24\u0026quot; } } }  Quick explanation Just wanted to quick go through how I setup the variables for all vNets. The vnets is a map object, essentially a key-value pair structure. If we look at the information of vent name and address spaces as the value of each of the vent properties, then the key would be the reference name of each of the vent property object. The information about each of the vent is grouped in such way that it can be iterated through the map object they form. when tf uses the vnet variable it would simply go into the value of eac key-value pair and retrieve the data from there.\nLets add the following to your main.tf and start creating first the resource group and the vNets\nresource \u0026quot;azurerm_resource_group\u0026quot; \u0026quot;hub_spoke\u0026quot; { name = local.rg location = local.rg_location } resource \u0026quot;azurerm_virtual_network\u0026quot; \u0026quot;hub_spoke\u0026quot; { for_each = local.vnets name = each.value[\u0026quot;name\u0026quot;] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name address_space = [each.value[\u0026quot;address_space\u0026quot;]] depends_on = [azurerm_resource_group.hub_spoke] }  Quick explanation In the second block we assign local.vnets which is map object to for_each expression, the expression is able to take each of the key-value pair and starting using them in the properties following it. You could then refer to each of the object in map by referring to key words each with syntax as the following:\nname = each.value[\u0026quot;name\u0026quot;]  The \u0026ldquo;each\u0026rdquo; keyword effectively points to every single key-value pair, the first one being\nvnet_hub = { name = \u0026quot;${local.org}-hub\u0026quot;, address_space = \u0026quot;10.0.0.0/16\u0026quot; }  In this \u0026ldquo;each\u0026rdquo; object, the \u0026ldquo;key\u0026rdquo; is \u0026ldquo;vnet_hub\u0026rdquo; and the \u0026ldquo;value\u0026rdquo; is \u0026ldquo;{ name = \u0026ldquo;${local.org}-hub\u0026rdquo;, address_space = \u0026ldquo;10.0.0.0/16\u0026rdquo; }\u0026rdquo;, therefore if I need the \u0026ldquo;name\u0026rdquo; property in the \u0026ldquo;value\u0026rdquo; inside each of the object, I would have \u0026ldquo;each.value[\u0026ldquo;name\u0026rdquo;]\u0026rdquo;. Please refer to tf doc for more information on for_each.\nWith this all set, lets run our tf and deploy the resource for the first time. Simply go:\nterraform plan -var-file=values.tfvars  followed by\nterraform apply -auto-approve -var-file=values.tfvars  If all sets up correctly, you should be able to see all the required vNet being created in the resource group.\n1.2 Create subnets They way subnets are created is pretty much similar to how vents were created, using for_each expression. add the following to you locals in you main.tf file\n main.tf\n subnets = { hub_gw = { vnet = \u0026quot;${local.org}-hub\u0026quot;, name = \u0026quot;GatewaySubnet\u0026quot;, address_prefixes = \u0026quot;10.0.1.0/24\u0026quot; } hub_firewall = { vnet = \u0026quot;${local.org}-hub\u0026quot;, name = \u0026quot;AzureFirewallSubnet\u0026quot;, address_prefixes = \u0026quot;10.0.2.0/24\u0026quot; } hub_firewall_mgmt = { vnet = \u0026quot;${local.org}-hub\u0026quot;, name = \u0026quot;AzureFirewallManagementSubnet\u0026quot;, address_prefixes = \u0026quot;10.0.3.0/24\u0026quot; } hub_jumphost = { vnet = \u0026quot;${local.org}-hub\u0026quot;, name = \u0026quot;jump\u0026quot;, address_prefixes = \u0026quot;10.0.4.0/24\u0026quot; } a_app = { vnet = \u0026quot;${local.org}-spoke-a\u0026quot;, name = \u0026quot;app\u0026quot;, address_prefixes = \u0026quot;10.1.1.0/24\u0026quot; } b_app = { vnet = \u0026quot;${local.org}-spoke-b\u0026quot;, name = \u0026quot;app\u0026quot;, address_prefixes = \u0026quot;10.2.1.0/24\u0026quot; } onprem_gw = { vnet = \u0026quot;${local.org}-spoke-onprem\u0026quot;, name = \u0026quot;GatewaySubnet\u0026quot;, address_prefixes = \u0026quot;192.168.1.0/26\u0026quot; } onprem_app = { vnet = \u0026quot;${local.org}-spoke-onprem\u0026quot;, name = \u0026quot;app\u0026quot;, address_prefixes = \u0026quot;192.168.1.64/26\u0026quot; } }  Add the following resources to our main.tf as well\n main.tf\n resource \u0026quot;azurerm_subnet\u0026quot; \u0026quot;hub-spoke\u0026quot; { for_each = local.subnets name = each.value[\u0026quot;name\u0026quot;] resource_group_name = local.rg virtual_network_name = each.value[\u0026quot;vnet\u0026quot;] address_prefixes = [each.value[\u0026quot;address_prefixes\u0026quot;]] enforce_private_link_endpoint_network_policies = true depends_on = [azurerm_virtual_network.hub_spoke] }  Again run:\nterraform plan -var-file=values.tfvars  followed by\nterraform apply -auto-approve -var-file=values.tfvars  If all sets up correctly, you should be able to see all the required subnets created inside our vnets.\n1.3 Create peerings between vNets Lets create the peerings as well to wire up the spoke vents with hub vnets. Add the following vars in you locals\n main.tf\n peering = { hub_to_spoke_a = { name = \u0026quot;${local.vnets.vnet_hub.name}-to-${local.vnets.vnet_a.name}\u0026quot;, vnet = \u0026quot;vnet_hub\u0026quot;, remote = \u0026quot;vnet_a\u0026quot;, use_remote_gw = false } spoke_a_to_hub = { name = \u0026quot;${local.vnets.vnet_a.name}-to-${local.vnets.vnet_hub.name}\u0026quot;, vnet = \u0026quot;vnet_a\u0026quot;, remote = \u0026quot;vnet_hub\u0026quot;, use_remote_gw = true } hub_to_spoke_b = { name = \u0026quot;${local.vnets.vnet_hub.name}-to-${local.vnets.vnet_b.name}\u0026quot;, vnet = \u0026quot;vnet_hub\u0026quot;, remote = \u0026quot;vnet_b\u0026quot;, use_remote_gw = false } spoke_b_to_hub = { name = \u0026quot;${local.vnets.vnet_b.name}-to-${local.vnets.vnet_hub.name}\u0026quot;, vnet = \u0026quot;vnet_b\u0026quot;, remote = \u0026quot;vnet_hub\u0026quot;, use_remote_gw = true } }  as well as the following resources:\n main.tf\n resource \u0026quot;azurerm_virtual_network_peering\u0026quot; \u0026quot;hub_to_spoke_a\u0026quot; { for_each = local.peering name = each.value[\u0026quot;name\u0026quot;] resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.hub_spoke[each.value[\u0026quot;vnet\u0026quot;]].name remote_virtual_network_id = azurerm_virtual_network.hub_spoke[each.value[\u0026quot;remote\u0026quot;]].id allow_virtual_network_access = true allow_forwarded_traffic = true allow_gateway_transit = true use_remote_gateways = each.value[\u0026quot;use_remote_gw\u0026quot;] }  Let now run terraform plan and terraform apply to create the peering, please follow steps in previous part for the code snippet. Once done we should be able to validate the peering connection by inspecting the connection status. 1.3 Create onprem vnet to hub vnet VPN connection VNet peering will help us wiring up spokes with hub, for the part of simulating onprem network connecting to hub, we will need to setup the vNet to vNet VPN connection. The real world vNet to vNet VPN connection is commonly used to establish connectivity between vNet along side vNet peering. For the use case where use need to decide which one to choose, please refer to this doc. Lets add some vars to our locals block again.\ngw = { hub_gw = { name = \u0026quot;${local.org}-hub-gw\u0026quot;, ip = \u0026quot;${local.org}-hub-gw-ip\u0026quot;, vpn_name = \u0026quot;${local.org}-onprem-hub\u0026quot;, peer = \u0026quot;onprem_gw\u0026quot; } onprem_gw = { name = \u0026quot;${local.org}-onprem-gw\u0026quot;, ip = \u0026quot;${local.org}-onprem-gw-ip\u0026quot;, vpn_name = \u0026quot;${local.org}-hub-onprem\u0026quot;, peer = \u0026quot;hub_gw\u0026quot; } }  Quick explanation Again all the information needed for creating VPN connection is in the key-value pair. We will need to create a public IPs for the gateway resource, and once gateway resources are provisioned we will setup the connection between them. Add the following resources to your main.tf file\nresource \u0026quot;azurerm_public_ip\u0026quot; \u0026quot;gw_ip\u0026quot; { for_each = local.gw name = each.value[\u0026quot;ip\u0026quot;] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name allocation_method = \u0026quot;Dynamic\u0026quot; sku = \u0026quot;Basic\u0026quot; } resource \u0026quot;azurerm_virtual_network_gateway\u0026quot; \u0026quot;gw\u0026quot; { for_each = local.gw name = each.value[\u0026quot;name\u0026quot;] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name type = \u0026quot;Vpn\u0026quot; vpn_type = \u0026quot;RouteBased\u0026quot; active_active = false enable_bgp = false sku = \u0026quot;VpnGw1\u0026quot; ip_configuration { name = \u0026quot;vnetGatewayConfig\u0026quot; public_ip_address_id = azurerm_public_ip.gw_ip[each.key].id private_ip_address_allocation = \u0026quot;Dynamic\u0026quot; subnet_id = azurerm_subnet.hub-spoke[each.key].id } } resource \u0026quot;azurerm_virtual_network_gateway_connection\u0026quot; \u0026quot;hub_onprem\u0026quot; { for_each = local.gw name = each.value[\u0026quot;vpn_name\u0026quot;] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name type = \u0026quot;Vnet2Vnet\u0026quot; virtual_network_gateway_id = azurerm_virtual_network_gateway.gw[each.key].id peer_virtual_network_gateway_id = azurerm_virtual_network_gateway.gw[each.value[\u0026quot;peer\u0026quot;]].id shared_key = \u0026quot;microsoft\u0026quot; }  Quick explanation The first block of resources iteratively define the public IP addresses that are going to be used by the gateway resource. The second block of resources are the gateways, which will need to be deployed inside the GatewaySubnets of both onprem vNet and hub vNet. The third block of resources are the VPN connections, please not the the type is \u0026ldquo;Vnet2Vnet\u0026rdquo;. In a real world scenario you would setup your onprem to azure connectivity via site to site VPN or ideally express route.\nAt this point we should have our VPN all connected which can be verified by inspecting the connection status in the portal.\n1.4 Create firewall instance Next up lets create the firewall instance. The firewall instance we created has the forced tunneling option enabled. Please refer to Microsoft doc for more information on the tunneling option. With this option you will need a additional subnets in the hub vnet with is designated as the management subnet. So lets add more vars to our locals block:\nfw_name = \u0026quot;${local.org}-azfw\u0026quot; fw_policy = \u0026quot;${local.org}-fw-policy\u0026quot; fw_ip = { fw_ip = { name = \u0026quot;${local.org}-fw-ip\u0026quot; } fw_mgmt_ip = { name = \u0026quot;${local.org}-fw-mgmt-ip\u0026quot; } }  add add the following firewall to our main.tf as well.\nresource \u0026quot;azurerm_firewall_policy\u0026quot; \u0026quot;fw_policy\u0026quot; { name = local.fw_policy resource_group_name = azurerm_resource_group.hub_spoke.name location = local.rg_location } resource \u0026quot;azurerm_public_ip\u0026quot; \u0026quot;fw_ip\u0026quot; { for_each = local.fw_ip name = each.value[\u0026quot;name\u0026quot;] location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name allocation_method = \u0026quot;Static\u0026quot; sku = \u0026quot;Standard\u0026quot; } resource \u0026quot;azurerm_firewall\u0026quot; \u0026quot;fw\u0026quot; { name = local.fw_name location = local.rg_location resource_group_name = azurerm_resource_group.hub_spoke.name firewall_policy_id = azurerm_firewall_policy.fw_policy.id ip_configuration { name = \u0026quot;ipconfig\u0026quot; subnet_id = azurerm_subnet.hub-spoke[\u0026quot;hub_firewall\u0026quot;].id public_ip_address_id = azurerm_public_ip.fw_ip[\u0026quot;fw_ip\u0026quot;].id } management_ip_configuration { name = \u0026quot;mgmt_ipconfig\u0026quot; subnet_id = azurerm_subnet.hub-spoke[\u0026quot;hub_firewall_mgmt\u0026quot;].id public_ip_address_id = azurerm_public_ip.fw_ip[\u0026quot;fw_mgmt_ip\u0026quot;].id } }  Quick explanation The first code block create a new firewall policy object. With The new policy resource sits outside of the firewall resource, if you need to redeploy the firewall for configuration changes, the policy resource remains untouched. Here we decided to create black policy object and attach to the firewall at time of creation. The second block of resources creates the public IPs used by the firewall. The third block of resource is obviously the firewall resource with reference to the firewall policy and the public Ips.\nLets also run terraform plan as well as terraform apply to get the firewall in our vnet.\n1.5 Create user defined route and firewall policies With all the resources setup and ready to go, the next step would be create some user defined routes and associated them to the vNet and subnets. For simplicity we will just focus on what needed to get the lab going, in a production environment you will have more complicated routes and firewall policies to meet your organization\u0026rsquo;s networking and security requirements. Let first add some vars to the locals. To begin with, we will need to create baseline policy rule collection which denies all traffice. If you need to understand the hierarchical strucutre of policy rules and policy rule collections, please refer to Microsoft doc.\nresource \u0026quot;azurerm_firewall_policy_rule_collection_group\u0026quot; \u0026quot;fw_rules_deny\u0026quot; { name = local.fw_rules_group_deny firewall_policy_id = azurerm_firewall_policy.fw_policy.id priority = 1000 network_rule_collection { name = \u0026quot;deny_netowrk_rule_coll\u0026quot; priority = 1000 action = \u0026quot;Deny\u0026quot; rule { name = \u0026quot;deny_all\u0026quot; protocols = [\u0026quot;TCP\u0026quot;, \u0026quot;UDP\u0026quot;, \u0026quot;ICMP\u0026quot;] source_addresses = [\u0026quot;*\u0026quot;] destination_addresses = [\u0026quot;*\u0026quot;] destination_ports = [\u0026quot;*\u0026quot;] } } }  secondly lets selectively apply some allow rules for our lab. Lets put our rules in plain words first and then create policy rules based on that. I have the following table to help me formulate my policies provide justification. Again when doing this in your production consult your network and security engineers.\n   from to action protocol reasoning     * * deny all Baseline policy to deny all traffic   my workstation * allow TCP If I create any jumphost in the hub, we need to be able to rdp into it from our work station   jumphost * allow TCP,UDP,ICMP Jumphost need to be able to connect to VMs in the spoke   vnet a vnet b/onprem allow TCP,UDP,ICMP This is a arbitrary rule that allow communication from vNet a to vNet b and onprem network   vnet b vnet a allow TCP,UDP,ICMP arbitrary rule that allows communication from vnet b to vnet a only   onprem * allow TCP,UDP,ICMP rule to allow onprem to connect to all vms in spokes    with all these in place, lets start building the firewall rules. Add the following to the main.tf\nresource \u0026quot;azurerm_firewall_policy_rule_collection_group\u0026quot; \u0026quot;fw_rules_allow\u0026quot; { name = local.fw_rules_group_allow firewall_policy_id = azurerm_firewall_policy.fw_policy.id priority = 500 network_rule_collection { name = \u0026quot;allow_network_rule_coll\u0026quot; priority = 500 action = \u0026quot;Allow\u0026quot; rule { name = \u0026quot;allow_blk\u0026quot; protocols = [\u0026quot;TCP\u0026quot;] source_addresses = [data.external.my_ip.result.ip] destination_addresses = [\u0026quot;*\u0026quot;] destination_ports = [\u0026quot;*\u0026quot;] } rule { name = \u0026quot;allow_hub_jumphost\u0026quot; protocols = [\u0026quot;TCP\u0026quot;] source_addresses = [local.subnets.hub_jumphost.address_prefixes] destination_addresses = [\u0026quot;*\u0026quot;] destination_ports = [\u0026quot;*\u0026quot;] } rule { name = \u0026quot;allow_a_to_b_and_onprem\u0026quot; protocols = [\u0026quot;TCP\u0026quot;, \u0026quot;UDP\u0026quot;, \u0026quot;ICMP\u0026quot;] source_addresses = [local.subnets.a_app.address_prefixes] destination_addresses = [local.subnets.b_app.address_prefixes, local.subnets.onprem_app.address_prefixes] destination_ports = [\u0026quot;*\u0026quot;] } rule { name = \u0026quot;allow_b_to_a\u0026quot; protocols = [\u0026quot;TCP\u0026quot;, \u0026quot;UDP\u0026quot;, \u0026quot;ICMP\u0026quot;] source_addresses = [local.subnets.b_app.address_prefixes] destination_addresses = [local.subnets.a_app.address_prefixes] destination_ports = [\u0026quot;*\u0026quot;] } rule { name = \u0026quot;allow_onprem_to_all\u0026quot; protocols = [\u0026quot;TCP\u0026quot;, \u0026quot;UDP\u0026quot;, \u0026quot;ICMP\u0026quot;] source_addresses = [local.subnets.onprem_app.address_prefixes] destination_addresses = [\u0026quot;*\u0026quot;] destination_ports = [\u0026quot;*\u0026quot;] } } nat_rule_collection { name = \u0026quot;nat_rule_coll\u0026quot; priority = 400 action = \u0026quot;Dnat\u0026quot; rule { name = \u0026quot;jumphost_rdp\u0026quot; protocols = [\u0026quot;TCP\u0026quot;] source_addresses = [data.external.my_ip.result.ip] destination_address = \u0026quot;20.227.0.88\u0026quot; destination_ports = [\u0026quot;3387\u0026quot;] translated_address = \u0026quot;10.0.4.4\u0026quot; translated_port = \u0026quot;3389\u0026quot; } } }  Quick explanation Each of the rule we had in the table will have a rule block to reflect what was planned. The last block is a nat rule where the firewall IP address can be used to NATted that to the jumphost private IP.\nLets run terraform plan and terraform apply.\nPart 3 Create some VMs to validate the traffic Lets create some vm in each of the app subnets and test the connectivity. For simplicity, create the VM with a public IP and access from your workstation.\n3.1 Create some VMs Manually create the VMs in the app network, for creating VMs with integration to subnet please refer to this doc.\n3.2 Ping tests Test 1 Connectivity from vNet_a. We are able to that VM in app subnet in vNet_a is able to ping VM in vNet_b and onprem.\nTest 2 Connectivity from onpmrem network. Part 4 Azure log analytics analysis We are also able to inspect the traffic once we enable Log Analytics for firewall. Follow this to enable diagnostic loggings. As shown on the screen shot we have firewall logging all the traffic including allowed and denied traffic. VM from vNet_a has connectivity to VMs in vNet_b and VMs in onprem network. VM from vNet_b has connectivity to VMs in vNet_b but not VMs in onprem network.\nCongrats! you\u0026rsquo;v reached the end of this post, thanks for your patience! Please leave your comments at the bottom if you find this post helpful! Cheers !\n","permalink":"https://blacklabnz.github.io/posts/hub-spoke/","summary":"The hub and spoke topology has been widely adopted for enterprise production deployment. In this lab, let put on our network/infrastructure engineer hat and get our hand dirty on Azure Hub and spoke topology with one of the popular IaC \u0026ndash; Terraform. Lets have a look at the high level architecture first.\nOverall architecture of the lab The essence of the topology is, by the name of it, having all traffic routed to hub before it gets forwarded to spoke.","title":"Azure networking: Hub and spoke topology with terraform"},{"content":"What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.\nThis configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.\nIn this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it. For all infra related deployment I am going to use Terraform as much as possible to avoid any \u0026ldquo;ClickOps\u0026rdquo; efforts. Once Infra is done, we will run a quick notebook to validate the connectivity as well as DNS resolution\nPart 1 Overall Architecture of the lab In this lab we will use databricks with secured connectivity and vNet injection to access some data residing in the ADLS with private endpoint configuration. We will also create keyvault for keeping the secrets involved using private endpoint. In the end we will use a simply python notebook to validate two things against the private Ip of the private endpoint:\n the network connectivity DNS resolution  As revealed on the diagram, we are simulating a well adopted hub and spoke topology. In our case we don\u0026rsquo;t have any hub for this lab, but two spokes that connects to each other via vNet peering. For the storage account private endpoint, well, we could have deployed that into the databricks vNet and they would naturally have connectivity(and secured by NSG if needed). But that removes the fun of dealing with Networking and sometime there are enough reason on the design to keep the private endpoint virtual network separate from other virtual network. We will also create keyvault with private endpoint and use that for a KV backed secret scope in databricks. In a nutshell a secret scope is a way to securely store secrets such as service principal key or a storage account key or any other type of secrets as such, so that we do hard code and secrets but are able to use \u0026ldquo;dbutil\u0026rdquo; to retrieve the secrets in a notebook run. Simple but yet important principle right ? no hard coded secret in your source code !!!!\nProject setup ├── README.md ├── data.tf ├── main.tf ├── output.tf ├── providers.tf ├── values.tfvars ├── variables.tf └── .gitignore  For simplifying our demo, we are going to put all our terraform resources in the main.tf file. In real world development, the best practice would be modularize any components that are reusable. We are also simplifying the terraform variables with locals, you will find the only the authentication related values are kept in variables and passed in with values.tfvars file.\nFor your convenience, you can download tf module required for this lab from my github repo: secure-databricks-terraform\nFirst thing before carry on to the next part is to setup the providers.\n providers.tf\n terraform { required_providers { azurerm = { source = \u0026quot;hashicorp/azurerm\u0026quot; version = \u0026quot;2.97.0\u0026quot; } databricks = { source = \u0026quot;databrickslabs/databricks\u0026quot; version = \u0026quot;0.5.1\u0026quot; } external = { source = \u0026quot;hashicorp/external\u0026quot; version = \u0026quot;2.2.2\u0026quot; } } } provider \u0026quot;azurerm\u0026quot; { features {} subscription_id = var.subscription_id client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id } provider \u0026quot;databricks\u0026quot; { host = format(\u0026quot;https://%s\u0026quot;, data.azurerm_databricks_workspace.dbr.workspace_url) azure_workspace_resource_id = data.azurerm_databricks_workspace.dbr.id azure_client_id = var.client_id azure_client_secret = var.client_secret azure_tenant_id = var.tenant_id }  Quick explanation: line 16 to line 19 we use service principal to authenticate to azure, in here we use variables to represent the value so that later on we can specify the values.tfvars file and pass the value in. This is really handy if you need to deal with multiple Azure environment that has different auth service principals. Refer the official doc here.\nTo be able to use variables we will need to add the following to the variables file:\n variable.tf\n variable \u0026quot;subscription_id\u0026quot; { type = string default = \u0026quot;default\u0026quot; } variable \u0026quot;client_id\u0026quot; { type = string default = \u0026quot;default\u0026quot; } variable \u0026quot;client_secret\u0026quot; { type = string default = \u0026quot;default\u0026quot; } variable \u0026quot;tenant_id\u0026quot; { type = string default = \u0026quot;default\u0026quot; }  line 23 to 27 similar deal here. One thing to note is that we are retrieving the databricks workspace url and resource id by using tf data object. The following needs to be added in the data.tf to achieve this.\n data.tf\n data \u0026quot;azurerm_databricks_workspace\u0026quot; \u0026quot;dbr\u0026quot; { name = azurerm_databricks_workspace.dbr.name resource_group_name = local.rg }  Lastly you will need to create a service principal that has contributor access to the subscription that you are about to deploy to. You can find more details in this office doc. Once Service principal is created you will need to create a secret and use that alone side with the application id to authentication to Azure. When finished, please create a values.tfvars file where you could input those required auth information for terraform to pass in as variables. The information required are:\n subscription id service principal client id service principal client secret azure tenant id.   values.tfvars\n subscription_id = \u0026quot;xxxxxxxxx\u0026quot; client_id = \u0026quot;xxxxxxxxx\u0026quot; client_secret = \u0026quot;xxxxxxxxx\u0026quot; tenant_id = \u0026quot;xxxxxxxxx\u0026quot;  At this point we will need to initiate our terraform workspace by running:\nterraform init  With that all set, lets move on to next part of creating actual resources.\nPart 2 Virtual networks and peering First we will add some variables to main.tf that we can use in the main file to the locals block\n main.tf\n locals { org = \u0026quot;blk\u0026quot; rg = \u0026quot;${local.org}-secure-dbr\u0026quot; rg_location = \u0026quot;australiaeast\u0026quot; vnet_dbr = \u0026quot;${local.org}-spoke-dbr\u0026quot; vnet_resource = \u0026quot;${local.org}-spoke-resource\u0026quot; vnet_dbr_address_space = \u0026quot;10.0.0.0/16\u0026quot; vnet_dbr_subnet_prv = \u0026quot;private\u0026quot; vnet_dbr_subnet_prv_prefix = \u0026quot;10.0.1.0/24\u0026quot; vnet_dbr_subnet_pub = \u0026quot;public\u0026quot; vnet_dbr_subnet_pub_prefix = \u0026quot;10.0.2.0/24\u0026quot; vnet_resource_address_space = \u0026quot;10.1.0.0/16\u0026quot; vnet_resource_subnet_stor = \u0026quot;stor\u0026quot; vnet_resource_subnet_stor_prefix = \u0026quot;10.1.1.0/24\u0026quot; vnet_resource_subnet_kv = \u0026quot;keyvault\u0026quot; vnet_resource_subnet_kv_prefix = \u0026quot;10.1.2.0/24\u0026quot; }  Line 2 to line 6 defines some local variables that we can reuse to build up the name of vNets. Line 8 to 14 defines the databricks vNet. Line 16 to 22 defines the resource vNet which is used by storage account and keyvault.\n2.1 Spoke vNet for databricks Pretty standard configuration here, one thing to note is that the subnets name is dictated to be private and public. There are also requirements to the minimum size of the subnets, please refere to this doc for more details.\nAll we need to do is to add the following to main.tf next to the locals block.\n main.tf\n resource \u0026quot;azurerm_resource_group\u0026quot; \u0026quot;secure_dbr\u0026quot; { name = local.rg location = local.rg_location } resource \u0026quot;azurerm_virtual_network\u0026quot; \u0026quot;vnet_dbr\u0026quot; { name = local.vnet_dbr location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name address_space = [local.vnet_dbr_address_space] depends_on = [azurerm_resource_group.secure_dbr] } resource \u0026quot;azurerm_subnet\u0026quot; \u0026quot;dbr_prv\u0026quot; { name = local.vnet_dbr_subnet_prv resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name address_prefixes = [local.vnet_dbr_subnet_prv_prefix] delegation { name = \u0026quot;dbr_prv_dlg\u0026quot; service_delegation { name = \u0026quot;Microsoft.Databricks/workspaces\u0026quot; actions = [ \u0026quot;Microsoft.Network/virtualNetworks/subnets/join/action\u0026quot;, \u0026quot;Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\u0026quot;, \u0026quot;Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action\u0026quot;, ] } } depends_on = [azurerm_virtual_network.vnet_dbr] } resource \u0026quot;azurerm_subnet\u0026quot; \u0026quot;dbr_pub\u0026quot; { name = local.vnet_dbr_subnet_pub resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name address_prefixes = [local.vnet_dbr_subnet_pub_prefix] delegation { name = \u0026quot;dbr_pub_dlg\u0026quot; service_delegation { name = \u0026quot;Microsoft.Databricks/workspaces\u0026quot; actions = [ \u0026quot;Microsoft.Network/virtualNetworks/subnets/join/action\u0026quot;, \u0026quot;Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\u0026quot;, \u0026quot;Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action\u0026quot;, ] } } depends_on = [azurerm_virtual_network.vnet_dbr] }  Quick explanation: Line 1-4 defines the resource group. Line 6-13 defines the dbr vNet. Line 15-53 defines the private and public subnets for the clusters.  Note in each of the subnet there is subnet delegation properties that needs to be added. The delegation block is not mandatory as Azure will take care of that, but if you don't add that, next time when you do terraform plan, the delegation property will be recognized as change even though there is nothing changed. \n2.2 Spoke vNet for storage account and keyvault Nothing complicated here, just need a separate subnets for storage account and keyvault private endpoints. Add the following to main.tf file.\nresource \u0026quot;azurerm_virtual_network\u0026quot; \u0026quot;vnet_resource\u0026quot; { name = local.vnet_resource location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name address_space = [local.vnet_resource_address_space] depends_on = [azurerm_resource_group.secure_dbr] } resource \u0026quot;azurerm_subnet\u0026quot; \u0026quot;resource_stor\u0026quot; { name = local.vnet_resource_subnet_stor resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name address_prefixes = [local.vnet_resource_subnet_stor_prefix] enforce_private_link_endpoint_network_policies = true depends_on = [azurerm_virtual_network.vnet_resource] } resource \u0026quot;azurerm_subnet\u0026quot; \u0026quot;resource_kv\u0026quot; { name = local.vnet_resource_subnet_kv resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name address_prefixes = [local.vnet_resource_subnet_kv_prefix] enforce_private_link_endpoint_network_policies = true depends_on = [azurerm_virtual_network.vnet_resource] }  Quick explanation Note the difference on the subnet compare to the above. Subnets provisioned for storage account and keyvault does not require subnet delegation. However it need the \u0026ldquo;Private endpoint network policy\u0026rdquo; to be enabled before any private endpoint can be deployed in it.\n2.3 vNet peering Virtual network peering ensures the connectivity between two virtual networks. In some environments direct spoke to spoke communication via peering is disable, all traffic between vNet peers goes via hub network, filtered and processed by firewall. However purpose of our lab is to focus on Databricks and private endpoints, we will use peering, a simpler setup to achieve spoke to spoke connectivity. I will write a separate article to talk about best practice of Hub-Spoke topology.\nresource \u0026quot;azurerm_virtual_network_peering\u0026quot; \u0026quot;dbr_to_resource\u0026quot; { name = \u0026quot;dbr-vent-to-resource-vnet\u0026quot; resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name remote_virtual_network_id = azurerm_virtual_network.vnet_resource.id } resource \u0026quot;azurerm_virtual_network_peering\u0026quot; \u0026quot;resource_to_dbr\u0026quot; { name = \u0026quot;resource-vent-to-dbr-vnet\u0026quot; resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name remote_virtual_network_id = azurerm_virtual_network.vnet_dbr.id }  Quick explanation The vNet peering needs to be setup from both directions, hence you find that there are two peering blocks required to achieve this.\nOnce you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars  followed by:\nterraform apply -auto-approve -var-file=values.tfvars  Let quickly go to the portal and validate that our vNets are created and peering is all connected. Part 3 Databricks workspace and cluster Lets create Databricks workspace and clusters in this part.\n3.1 Databricks secure connectivity + vNet injection To remove the exposure to public internet traffic, clusters can be deployed with no-pubip configuration and deployed into pre-defined vNet. First need to add the following to locals block:\ndbr = \u0026quot;${local.org}-secure-dbr\u0026quot; dbr_sku = \u0026quot;premium\u0026quot; dbr_mgmt_rg = \u0026quot;${local.dbr}-mgmt-rg\u0026quot; dbr_nsg = \u0026quot;${local.org}-nsg\u0026quot; dbr_cluster = \u0026quot;${local.org}-cluster\u0026quot;  Then add the following to main.tf\nresource \u0026quot;azurerm_network_security_group\u0026quot; \u0026quot;dbr_nsg\u0026quot; { name = local.dbr_nsg location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name } resource \u0026quot;azurerm_subnet_network_security_group_association\u0026quot; \u0026quot;dbr_prv\u0026quot; { subnet_id = azurerm_subnet.dbr_prv.id network_security_group_id = azurerm_network_security_group.dbr_nsg.id } resource \u0026quot;azurerm_subnet_network_security_group_association\u0026quot; \u0026quot;dbr_pub\u0026quot; { subnet_id = azurerm_subnet.dbr_pub.id network_security_group_id = azurerm_network_security_group.dbr_nsg.id } resource \u0026quot;azurerm_databricks_workspace\u0026quot; \u0026quot;dbr\u0026quot; { name = local.dbr resource_group_name = local.rg location = local.rg_location sku = local.dbr_sku managed_resource_group_name = local.dbr_mgmt_rg custom_parameters { no_public_ip = true virtual_network_id = azurerm_virtual_network.vnet_dbr.id public_subnet_name = azurerm_subnet.dbr_pub.name private_subnet_name = azurerm_subnet.dbr_prv.name public_subnet_network_security_group_association_id = azurerm_subnet_network_security_group_association.dbr_pub.id private_subnet_network_security_group_association_id = azurerm_subnet_network_security_group_association.dbr_prv.id } }  Quick explanation Line 1-15 defines the network security group and the association, this is mandatory for databricks cluster vNet injection. Line 17-34 defines the databricks workspace with existing vNet and subnets. Line 26 configures databricks cluster to have not public IP.  Please customize your NSG if you are using this in higher environments, current configuration is minimal so that we could focus on the lab. Please always go with Security first principle. \n3.2 Cluster creation We will use terraform to create clusters, note we will need to add databricks provider to terraform which was already performed in earlier steps.\nresource \u0026quot;databricks_cluster\u0026quot; \u0026quot;cluster\u0026quot; { cluster_name = local.dbr_cluster spark_version = data.databricks_spark_version.latest.id node_type_id = data.databricks_node_type.smallest.id autotermination_minutes = 20 spark_conf = { \u0026quot;spark.databricks.cluster.profile\u0026quot; : \u0026quot;singleNode\u0026quot; \u0026quot;spark.master\u0026quot; : \u0026quot;local[*]\u0026quot; } custom_tags = { \u0026quot;ResourceClass\u0026quot; = \u0026quot;SingleNode\u0026quot; } }  you will also need to add the following to the data.tf file so that databricks provide can retrieve some information dynamically for the provisioning.\n data.tf\n data \u0026quot;databricks_spark_version\u0026quot; \u0026quot;latest\u0026quot; {} data \u0026quot;databricks_node_type\u0026quot; \u0026quot;smallest\u0026quot; { local_disk = true }  Quick explanation The cluster we provisioned is single cluster which is capable enough to help to complete the lab.\nOnce you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars  followed by:\nterraform apply -auto-approve -var-file=values.tfvars  Lets login to our databricks instance and validate the cluster is in place. Part 4 ADLS + Keyvault with Private endpoint Lets create ADLS account and Keyvault in this part with private endpoints and private DNS zones. private DNS zone is required so that a Azure private link DNS address could be resolved to the private IP address for the private endpoint.\n4.1 Storage account with private endpoint and private DNS zone This part shouldnt be too bad, a storage account with private endpoint deployed to the spoke network.\nFirst need to add the following to the locals block:\nstor = \u0026quot;${local.org}stor\u0026quot; stor_pe = \u0026quot;${local.stor}pe\u0026quot; stor_prv_con = \u0026quot;${local.stor}prvcon\u0026quot; kv = \u0026quot;${local.org}-kv\u0026quot; kv_pe = \u0026quot;${local.kv}pe\u0026quot; kv_prv_con = \u0026quot;${local.kv}prvcon\u0026quot;  Then add the following to main.tf file：\nresource \u0026quot;azurerm_storage_account\u0026quot; \u0026quot;stor\u0026quot; { name = local.stor resource_group_name = local.rg location = local.rg_location account_tier = \u0026quot;Standard\u0026quot; account_replication_type = \u0026quot;LRS\u0026quot; is_hns_enabled = true network_rules { default_action = \u0026quot;Deny\u0026quot; ip_rules = [data.external.my_ip.result.ip] } } resource \u0026quot;azurerm_storage_container\u0026quot; \u0026quot;container\u0026quot; { name = \u0026quot;land\u0026quot; storage_account_name = azurerm_storage_account.stor.name container_access_type = \u0026quot;private\u0026quot; } resource \u0026quot;azurerm_private_endpoint\u0026quot; \u0026quot;stor_pe\u0026quot; { name = local.stor_pe location = local.rg_location resource_group_name = local.rg subnet_id = azurerm_subnet.resource_stor.id private_service_connection { name = local.stor_prv_con private_connection_resource_id = azurerm_storage_account.stor.id is_manual_connection = false subresource_names = [\u0026quot;dfs\u0026quot;] } } resource \u0026quot;azurerm_private_dns_zone\u0026quot; \u0026quot;stor_dfs\u0026quot; { name = \u0026quot;privatelink.dfs.core.windows.net\u0026quot; resource_group_name = local.rg } resource \u0026quot;azurerm_private_dns_zone_virtual_network_link\u0026quot; \u0026quot;dbr_vnet_link_stor\u0026quot; { name = \u0026quot;dbr_vnet_link\u0026quot; resource_group_name = local.rg private_dns_zone_name = azurerm_private_dns_zone.stor_dfs.name virtual_network_id = azurerm_virtual_network.vnet_dbr.id } resource \u0026quot;azurerm_private_dns_a_record\u0026quot; \u0026quot;storpe_dns\u0026quot; { name = local.stor zone_name = azurerm_private_dns_zone.stor_dfs.name resource_group_name = local.rg ttl = 300 records = [azurerm_private_endpoint.stor_pe.private_service_connection.0.private_ip_address] }  Quick explanation Line 11-14 defined the firewall rules so that we are not open our storage account to public by default. Note the \u0026ldquo;data.external.my_ip.result.ip\u0026rdquo;, because we still want to be operate the storage account from out local machine hence the data block retrieve your current public ip and inject that to network policy. You will also need to add the following to your data.tf file：\ndata \u0026quot;external\u0026quot; \u0026quot;my_ip\u0026quot; { program = [\u0026quot;curl\u0026quot;, \u0026quot;https://api.ipify.org?format=json\u0026quot;] }  Line 23-35 created the private endpoint targeting the storage account resource. Line 37-47 creates a Azure private dns zone for ADLS, and link that to the dbr network, so that DNS request from the dbr network could use this for name resolution. Line 38 specifies the top level domain name for the storage account in the private DNZ zone. Line 49-55 creates a private DNS record using the private ip address of the network interface as part of private endpoint provisioning.  Note when creating private endpoint, there will be a network interface attached to it which gets a private ip address allocated in the subnet range. \n4.2 Keyvault with private endpoint and private DNS zone This one is not too bad either, keyvualt with private endpoint deployed to the same spoke network with similar setup on the private DNS zone, vNet linkage and dns records\nresource \u0026quot;azurerm_key_vault\u0026quot; \u0026quot;kv\u0026quot; { name = local.kv location = local.rg_location resource_group_name = local.rg enabled_for_disk_encryption = true tenant_id = data.azurerm_client_config.current.tenant_id soft_delete_retention_days = 7 purge_protection_enabled = false sku_name = \u0026quot;standard\u0026quot; network_acls { default_action = \u0026quot;Deny\u0026quot; bypass = \u0026quot;AzureServices\u0026quot; ip_rules = [data.external.my_ip.result.ip] } } resource \u0026quot;azurerm_private_endpoint\u0026quot; \u0026quot;kv_pe\u0026quot; { name = local.kv_pe location = local.rg_location resource_group_name = local.rg subnet_id = azurerm_subnet.resource_kv.id private_service_connection { name = local.kv_prv_con private_connection_resource_id = azurerm_key_vault.kv.id is_manual_connection = false subresource_names = [\u0026quot;Vault\u0026quot;] } } resource \u0026quot;azurerm_private_dns_zone\u0026quot; \u0026quot;kv\u0026quot; { name = \u0026quot;privatelink.vaultcore.azure.net\u0026quot; resource_group_name = local.rg } resource \u0026quot;azurerm_private_dns_zone_virtual_network_link\u0026quot; \u0026quot;dbr_vnet_link_kv\u0026quot; { name = \u0026quot;dbr_vnet_link_kv\u0026quot; resource_group_name = local.rg private_dns_zone_name = azurerm_private_dns_zone.kv.name virtual_network_id = azurerm_virtual_network.vnet_dbr.id } resource \u0026quot;azurerm_private_dns_a_record\u0026quot; \u0026quot;kvpe_dns\u0026quot; { name = local.kv zone_name = azurerm_private_dns_zone.kv.name resource_group_name = local.rg ttl = 300 records = [azurerm_private_endpoint.kv_pe.private_service_connection.0.private_ip_address] }  Once you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars  followed by:\nterraform apply -auto-approve -var-file=values.tfvars  Part 5 Databricks configuration - Keyvault backed secret scope To create the keyvault backed secret scope, terraform provides a handy resource to do this. add the following to the main.tf files to achieve this.\nresource \u0026quot;databricks_secret_scope\u0026quot; \u0026quot;kv\u0026quot; { name = \u0026quot;keyvault-managed\u0026quot; keyvault_metadata { resource_id = azurerm_key_vault.kv.id dns_name = azurerm_key_vault.kv.vault_uri } }  The tricky part here is that according to terraform this is only supported by azure cli authentication but NOT with service principal authentication. You will see error like this if service principal auth is used: what you will need to do is to comment out the azurerm authentication part in the providers.tf and then login to Azure using\naz login  If you haven\u0026rsquo;t installed Az cli following this to install and read more details of this limitation here.\nPart 6 Run notebook to validate Let put some python code in and test the connectivity and DNS resolutions. The first and most important task is to validate the connectivity and DNS resolution. You can run this code snippet in a notebook to check them.\nimport socket stor = socket.gethostbyname_ex(\u0026quot;blkstor.dfs.core.windows.net\u0026quot;) print (\u0026quot;\\n\\nThe IP Address of the Domain Name is: \u0026quot; + repr(stor)) kv = socket.gethostbyname_ex(\u0026quot;blk-kv.vault.azure.net\u0026quot;) print (\u0026quot;\\n\\nThe IP Address of the Domain Name is: \u0026quot; + repr(kv))  If all setup correctly you should be able to see that the DNS name is resolved with a private IP address. If you are interest in further testing to read some file from ADLS you could use the following snippet to do so.\ndbutils.widgets.text(\u0026quot;adls_name\u0026quot;, \u0026quot;blkstor\u0026quot;, \u0026quot;adls_name\u0026quot;) dbutils.widgets.text(\u0026quot;adls_container\u0026quot;, \u0026quot;land\u0026quot;, \u0026quot;adls_container\u0026quot;) # COMMAND ---------- adls_name = dbutils.widgets.get(\u0026quot;adls_name\u0026quot;) adls_container = dbutils.widgets.get(\u0026quot;adls_container\u0026quot;) # COMMAND ---------- spark.conf.set(\u0026quot;fs.azure.account.auth.type\u0026quot;, \u0026quot;OAuth\u0026quot;) spark.conf.set(\u0026quot;fs.azure.account.oauth.provider.type\u0026quot;, \u0026quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\u0026quot;) spark.conf.set(\u0026quot;fs.azure.account.oauth2.client.id\u0026quot;, dbutils.secrets.get(scope=\u0026quot;keyvault-managed\u0026quot;,key=\u0026quot;sp-id\u0026quot;)) spark.conf.set(\u0026quot;fs.azure.account.oauth2.client.secret\u0026quot;, dbutils.secrets.get(scope=\u0026quot;keyvault-managed\u0026quot;,key=\u0026quot;sp-secret\u0026quot;)) spark.conf.set(\u0026quot;fs.azure.account.oauth2.client.endpoint\u0026quot;, \u0026quot;https://login.microsoftonline.com/xxxx-xxx-xxxxxx-xxxxxx/oauth2/token\u0026quot;) # COMMAND ---------- df = spark.read.json(f\u0026quot;abfss://{adls_container}@{adls_name}.dfs.core.windows.net/pubapis.json\u0026quot;) # COMMAND ---------- df.show()  What this notebook does is quite simple, it grabs the secret of a service principal from keyvault secret scope and use that to auth to Azure. Once authorized it will read a json file into dataframe. Please seethe outcome of the notebook run in the following screen shot. To be able to achieve this, you will need the following:\n another service principal that has the role \u0026ldquo;Storage blob contributor\u0026rdquo; on the storage account having the application id and secret saved in the Keyvault we created as \u0026ldquo;sp-id\u0026rdquo; and \u0026ldquo;sp-secret\u0026rdquo;  I have included these two notebooks in the repo, you will find them in the notebooks folder.\nTo deploy the notebooks to our databricks workspace, you could add the following to your main.tf file as part of the deployment.\nresource \u0026quot;databricks_notebook\u0026quot; \u0026quot;notbooks\u0026quot; { for_each = fileset(\u0026quot;${path.module}/notebooks\u0026quot;, \u0026quot;*\u0026quot;) source = \u0026quot;${path.module}/notebooks/${each.key}\u0026quot; path = \u0026quot;/validation/${element(split(\u0026quot;.\u0026quot;, each.key), 0)}\u0026quot; language = \u0026quot;PYTHON\u0026quot; }  This block iterate through the notebooks folder and deploy the notebook in a folder called \u0026ldquo;validated\u0026rdquo; in the workspace.  Note usually you would not deploy your notebooks as part of IaC efforts, these would otherwise be regarded as the \"application\" artifacts. But for simplicity of our demo, they are deployed in the same IaC code flow. \nAt this point our lab has reached the end, hope you enjoyed the extra efforts needed to make your databricks cluster more secure.\nIf like the content please leave your comments below, if you find issues with the content, please also comment below. Thanks for your time and patience with me !!\n","permalink":"https://blacklabnz.github.io/posts/databricks-secure/","summary":"What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.\nThis configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.\nIn this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it.","title":"Secure Databricks cluster with vNet injection and access resources via Azure private endpoint"},{"content":"Hi there, great to meet you here. My name is Neil，currently working as a Data Engineer operating in the cloud.\nPreviously I was working in DevOps capacity, focusing on building useful CICD pipelines, automations, APIs and else.\nThrough out my career, I\u0026rsquo;d like to share some tips and tricks which I hope you find helpful !\n","permalink":"https://blacklabnz.github.io/about/","summary":"Hi there, great to meet you here. My name is Neil，currently working as a Data Engineer operating in the cloud.\nPreviously I was working in DevOps capacity, focusing on building useful CICD pipelines, automations, APIs and else.\nThrough out my career, I\u0026rsquo;d like to share some tips and tricks which I hope you find helpful !","title":"About"},{"content":"Recently I was tasked with consuming data from websocket, analyze it and then send data to Prometheus. The theory is pretty straight forward: getting data from websocket API in a stream and analyze and take the data points and send it to prometheus for visualization. In this blog you will have all the steps and code needed to reproduce this flow. With this in mind, I decided using python to achieve all these.\nPart 1. Websocket VS Rest reminder Before we start, I would like to have a bit of revision on Websocket API and how it is different from REST API\nThe diagram I took from internet explains it quite well. In simple term, you interact with REST API with a request and response fashion whereas in websocket there is a two way connection established during interaction lifecycle therefore you don\u0026rsquo;t need to constantly send request to server for retrieving data. At the end of the interaction, the two way connection is close.\nPart 2. Consuming a websocket API You could easily find some publicly available websocket API, the one I used for this blog is from Binance, one of the platform used by coin traders. Though myself is not doing any coin trading nor receiving any sponsorship from them. They have very detailed API documentation on their Spot API.\nThe following code snippet can be used to connect to websocket API:\nimport json import websocket socket = \u0026quot;wss://stream.binance.com:9443/ws/bnbusdt@kline_1m\u0026quot; def on_open(ws): print(\u0026quot;Opened connection\u0026quot;) def on_message(ws, message): data = json.loads(message) print(data) def on_open(ws): print(\u0026quot;Opened connection\u0026quot;) def on_error(ws, error): print(error) ws = websocket.WebSocketApp( socket, on_open=on_open, on_message=on_message, on_error=on_error, on_close=on_close) ws.run_forever()  Quick explanation Line 4 defines the websocket url, the details of this endpoint can be found here. \u0026ldquo;/bnbusdt@kline_1m\u0026rdquo; means retrieving data from the kline stream for bnb vs usdt, what is so called a symbol, these are crypto coin terminologies which you can fine richer explanations else where.\nLine 6 to line 17 defines the callback function with minimal functionality when message is received from the server. More details can be found in websocket-client official documentation.\nLine 19 and 10 creates a websocket instance and start connection and run forever.\nRun the above python code will give you this console output:\nOpened connection {\u0026quot;e\u0026quot;: \u0026quot;kline\u0026quot;, \u0026quot;E\u0026quot;: 1647220248756, \u0026quot;s\u0026quot;: \u0026quot;BNBUSDT\u0026quot;, \u0026quot;k\u0026quot;: {\u0026quot;t\u0026quot;: 1647220200000, \u0026quot;T\u0026quot;: 1647220259999, \u0026quot;s\u0026quot;: \u0026quot;BNBUSDT\u0026quot;, \u0026quot;i\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;f\u0026quot;: 527449288, \u0026quot;L\u0026quot;: 527449503, \u0026quot;o\u0026quot;: \u0026quot;364.60000000\u0026quot;, \u0026quot;c\u0026quot;: \u0026quot;364.30000000\u0026quot;, \u0026quot;h\u0026quot;: \u0026quot;364.60000000\u0026quot;, \u0026quot;l\u0026quot;: \u0026quot;364.20000000\u0026quot;, \u0026quot;v\u0026quot;: \u0026quot;458.54400000\u0026quot;, \u0026quot;n\u0026quot;: 216, \u0026quot;x\u0026quot;: False, \u0026quot;q\u0026quot;: \u0026quot;167076.68030000\u0026quot;, \u0026quot;V\u0026quot;: \u0026quot;183.99400000\u0026quot;, \u0026quot;Q\u0026quot;: \u0026quot;67043.53500000\u0026quot;, \u0026quot;B\u0026quot;: \u0026quot;0\u0026quot;}} {\u0026quot;e\u0026quot;: \u0026quot;kline\u0026quot;, \u0026quot;E\u0026quot;: 1647220251279, \u0026quot;s\u0026quot;: \u0026quot;BNBUSDT\u0026quot;, \u0026quot;k\u0026quot;: {\u0026quot;t\u0026quot;: 1647220200000, \u0026quot;T\u0026quot;: 1647220259999, \u0026quot;s\u0026quot;: \u0026quot;BNBUSDT\u0026quot;, \u0026quot;i\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;f\u0026quot;: 527449288, \u0026quot;L\u0026quot;: 527449507, \u0026quot;o\u0026quot;: \u0026quot;364.60000000\u0026quot;, \u0026quot;c\u0026quot;: \u0026quot;364.30000000\u0026quot;, \u0026quot;h\u0026quot;: \u0026quot;364.60000000\u0026quot;, \u0026quot;l\u0026quot;: \u0026quot;364.20000000\u0026quot;, \u0026quot;v\u0026quot;: \u0026quot;460.67000000\u0026quot;, \u0026quot;n\u0026quot;: 220, \u0026quot;x\u0026quot;: False, \u0026quot;q\u0026quot;: \u0026quot;167851.03680000\u0026quot;, \u0026quot;V\u0026quot;: \u0026quot;184.66700000\u0026quot;, \u0026quot;Q\u0026quot;: \u0026quot;67288.70890000\u0026quot;, \u0026quot;B\u0026quot;: \u0026quot;0\u0026quot;}} {\u0026quot;e\u0026quot;: \u0026quot;kline\u0026quot;, \u0026quot;E\u0026quot;: 1647220253583, \u0026quot;s\u0026quot;: \u0026quot;BNBUSDT\u0026quot;, \u0026quot;k\u0026quot;: {\u0026quot;t\u0026quot;: 1647220200000, \u0026quot;T\u0026quot;: 1647220259999, \u0026quot;s\u0026quot;: \u0026quot;BNBUSDT\u0026quot;, \u0026quot;i\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;f\u0026quot;: 527449288, \u0026quot;L\u0026quot;: 527449513, \u0026quot;o\u0026quot;: \u0026quot;364.60000000\u0026quot;, \u0026quot;c\u0026quot;: \u0026quot;364.20000000\u0026quot;, \u0026quot;h\u0026quot;: \u0026quot;364.60000000\u0026quot;, \u0026quot;l\u0026quot;: \u0026quot;364.20000000\u0026quot;, \u0026quot;v\u0026quot;: \u0026quot;461.78000000\u0026quot;, \u0026quot;n\u0026quot;: 226, \u0026quot;x\u0026quot;: False, \u0026quot;q\u0026quot;: \u0026quot;168255.38670000\u0026quot;, \u0026quot;V\u0026quot;: \u0026quot;185.54600000\u0026quot;, \u0026quot;Q\u0026quot;: \u0026quot;67608.92860000\u0026quot;, \u0026quot;B\u0026quot;: \u0026quot;0\u0026quot;}} Connection closed  Futher explore the API Previous step works fine with a single symbol, what if in the websocket I need data from more symbol or even in any websockets ?? The websocket API kindly offers subscription mode, with which you can subscribe multiple symbols and get the data back within the same websocket connection. refer to the following code for this:\nimport json import websocket socket = \u0026quot;wss://stream.binance.com:9443/ws\u0026quot; ws = websocket.create_connection(socket) ws.send(json.dumps({ \u0026quot;method\u0026quot;: \u0026quot;SUBSCRIBE\u0026quot;, \u0026quot;params\u0026quot;: [ \u0026quot;btcusdt@kline_1m\u0026quot;, \u0026quot;bnbusdt@kline_1m\u0026quot;, \u0026quot;ethusdt@kline_1m\u0026quot;, \u0026quot;dogeusdt@kline_1m\u0026quot;, \u0026quot;xrpusdt@kline_1m\u0026quot; ], \u0026quot;id\u0026quot;: 1 })) def on_message(message): data = json.loads(message) print(data) while True: result = ws.recv() on_message(result) ws.close()  Quick explanation Line 6-17 creates a websocket connection, the first action is to send a message to endpoint to subscribe to steams that are of interests. The payload of the subscription is like this:\n{ \u0026quot;method\u0026quot;: \u0026quot;SUBSCRIBE\u0026quot;, \u0026quot;params\u0026quot;: [ \u0026quot;btcusdt@kline_1m\u0026quot;, \u0026quot;bnbusdt@kline_1m\u0026quot;, \u0026quot;ethusdt@kline_1m\u0026quot;, \u0026quot;dogeusdt@kline_1m\u0026quot;, \u0026quot;xrpusdt@kline_1m\u0026quot; ], \u0026quot;id\u0026quot;: 1 }  According to the API doc, the response of the first ws.sent(payload) call is\n{ \u0026quot;result\u0026quot;: null, \u0026quot;id\u0026quot;: 1 }  When we inspect the console output when running the above python code we also get the following which indicates we are NOT doing something crazy !\n{\u0026quot;result\u0026quot;: \u0026quot;None\u0026quot;, \u0026quot;id\u0026quot;: 1} {\u0026quot;e\u0026quot;: \u0026quot;kline\u0026quot;, \u0026quot;E\u0026quot;: 1647220380568, \u0026quot;s\u0026quot;: \u0026quot;XRPUSDT\u0026quot;, \u0026quot;k\u0026quot;: {\u0026quot;t\u0026quot;: 1647220320000, \u0026quot;T\u0026quot;: 1647220379999, \u0026quot;s\u0026quot;: \u0026quot;XRPUSDT\u0026quot;, \u0026quot;i\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;f\u0026quot;: 429501724, \u0026quot;L\u0026quot;: 429501859, \u0026quot;o\u0026quot;: \u0026quot;0.75860000\u0026quot;, \u0026quot;c\u0026quot;: \u0026quot;0.75920000\u0026quot;, \u0026quot;h\u0026quot;: \u0026quot;0.75920000\u0026quot;, \u0026quot;l\u0026quot;: \u0026quot;0.75860000\u0026quot;, \u0026quot;v\u0026quot;: \u0026quot;81089.00000000\u0026quot;, \u0026quot;n\u0026quot;: 136, \u0026quot;x\u0026quot;: true, \u0026quot;q\u0026quot;: \u0026quot;61541.60570000\u0026quot;, \u0026quot;V\u0026quot;: \u0026quot;63081.00000000\u0026quot;, \u0026quot;Q\u0026quot;: \u0026quot;47874.45210000\u0026quot;, \u0026quot;B\u0026quot;: \u0026quot;0\u0026quot;}} {\u0026quot;e\u0026quot;: \u0026quot;kline\u0026quot;, \u0026quot;E\u0026quot;: 1647220382238, \u0026quot;s\u0026quot;: \u0026quot;BTCUSDT\u0026quot;, \u0026quot;k\u0026quot;: {\u0026quot;t\u0026quot;: 1647220380000, \u0026quot;T\u0026quot;: 1647220439999, \u0026quot;s\u0026quot;: \u0026quot;BTCUSDT\u0026quot;, \u0026quot;i\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;f\u0026quot;: 1291349545, \u0026quot;L\u0026quot;: 1291349561, \u0026quot;o\u0026quot;: \u0026quot;38183.79000000\u0026quot;, \u0026quot;c\u0026quot;: \u0026quot;38183.79000000\u0026quot;, \u0026quot;h\u0026quot;: \u0026quot;38183.80000000\u0026quot;, \u0026quot;l\u0026quot;: \u0026quot;38183.79000000\u0026quot;, \u0026quot;v\u0026quot;: \u0026quot;0.25519000\u0026quot;, \u0026quot;n\u0026quot;: 17, \u0026quot;x\u0026quot;: false, \u0026quot;q\u0026quot;: \u0026quot;9744.12173000\u0026quot;, \u0026quot;V\u0026quot;: \u0026quot;0.03599000\u0026quot;, \u0026quot;Q\u0026quot;: \u0026quot;1374.23496200\u0026quot;, \u0026quot;B\u0026quot;: \u0026quot;0\u0026quot;}} {\u0026quot;e\u0026quot;: \u0026quot;kline\u0026quot;, \u0026quot;E\u0026quot;: 1647220382372, \u0026quot;s\u0026quot;: \u0026quot;BNBUSDT\u0026quot;, \u0026quot;k\u0026quot;: {\u0026quot;t\u0026quot;: 1647220380000, \u0026quot;T\u0026quot;: 1647220439999, \u0026quot;s\u0026quot;: \u0026quot;BNBUSDT\u0026quot;, \u0026quot;i\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;f\u0026quot;: 527450005, \u0026quot;L\u0026quot;: 527450009, \u0026quot;o\u0026quot;: \u0026quot;363.80000000\u0026quot;, \u0026quot;c\u0026quot;: \u0026quot;363.70000000\u0026quot;, \u0026quot;h\u0026quot;: \u0026quot;363.80000000\u0026quot;, \u0026quot;l\u0026quot;: \u0026quot;363.70000000\u0026quot;, \u0026quot;v\u0026quot;: \u0026quot;2.58200000\u0026quot;, \u0026quot;n\u0026quot;: 5, \u0026quot;x\u0026quot;: false, \u0026quot;q\u0026quot;: \u0026quot;939.30620000\u0026quot;, \u0026quot;V\u0026quot;: \u0026quot;2.32800000\u0026quot;, \u0026quot;Q\u0026quot;: \u0026quot;846.92640000\u0026quot;, \u0026quot;B\u0026quot;: \u0026quot;0\u0026quot;}}  As you could see from the screenshot, the first response from the server is the acknowledgement of the subscription. The data following the acknowledgement is the data for the symbols(remember the crypto trading jargon?), and indeed we are receiving data for all the symbols we have subscribed to !\nPart 3. Add prometheus-client and create metric endpoint To start with part, first we want to add prometheus client to our code so that we could see metric being sent.\nRecall the follow symbol data output from our previous step:\n{ \u0026quot;e\u0026quot;: \u0026quot;kline\u0026quot;, // Event type \u0026quot;E\u0026quot;: 123456789, // Event time \u0026quot;s\u0026quot;: \u0026quot;BNBBTC\u0026quot;, // Symbol \u0026quot;k\u0026quot;: { \u0026quot;t\u0026quot;: 123400000, // Kline start time \u0026quot;T\u0026quot;: 123460000, // Kline close time \u0026quot;s\u0026quot;: \u0026quot;BNBBTC\u0026quot;, // Symbol \u0026quot;i\u0026quot;: \u0026quot;1m\u0026quot;, // Interval \u0026quot;f\u0026quot;: 100, // First trade ID \u0026quot;L\u0026quot;: 200, // Last trade ID \u0026quot;o\u0026quot;: \u0026quot;0.0010\u0026quot;, // Open price \u0026quot;c\u0026quot;: \u0026quot;0.0020\u0026quot;, // Close price \u0026quot;h\u0026quot;: \u0026quot;0.0025\u0026quot;, // High price \u0026quot;l\u0026quot;: \u0026quot;0.0015\u0026quot;, // Low price \u0026quot;v\u0026quot;: \u0026quot;1000\u0026quot;, // Base asset volume \u0026quot;n\u0026quot;: 100, // Number of trades \u0026quot;x\u0026quot;: false, // Is this kline closed? \u0026quot;q\u0026quot;: \u0026quot;1.0000\u0026quot;, // Quote asset volume \u0026quot;V\u0026quot;: \u0026quot;500\u0026quot;, // Taker buy base asset volume \u0026quot;Q\u0026quot;: \u0026quot;0.500\u0026quot;, // Taker buy quote asset volume \u0026quot;B\u0026quot;: \u0026quot;123456\u0026quot; // Ignore } }  With reference to the doc, for our case we are interested in \u0026ldquo;l\u0026rdquo; and \u0026ldquo;h\u0026rdquo; in the \u0026ldquo;k\u0026rdquo; property for the low price and high price. Obviously we are interested in the \u0026ldquo;s\u0026rdquo; property as well for the name of the symbol. Once we understand what we need to do with our data, now it time to update the callback function accordingly to do our \u0026ldquo;analysis\u0026rdquo;. Our analysis here is as simple as just reading some properties and this is not very important here as it is on the business side in the real world, our goal in this blog is to retrieve the data and send to Prometheus and we can just send some properties as if it is our \u0026ldquo;analysis\u0026rdquo;. So let\u0026quot;s go\ndef on_message(message): data = json.loads(message) if \u0026quot;k\u0026quot; not in data: pass else: print(data[\u0026quot;s\u0026quot;], data[\u0026quot;k\u0026quot;][\u0026quot;l\u0026quot;], data[\u0026quot;k\u0026quot;][\u0026quot;h\u0026quot;])  Once call back function is updated we should see the following output that captures properties we are interested\nBTCUSDT 38155.15000000 38170.16000000 ETHUSDT 2534.36000000 2535.02000000 XRPUSDT 0.76010000 0.76040000 BNBUSDT 364.40000000 364.50000000 DOGEUSDT 0.11180000 0.11180000 BTCUSDT 38153.01000000 38170.16000000  Now lets start ingesting data to Prometheus using prometheus-client. Obviously we can spent our whole night trying to understand the nitty gritty of prometheus-client, but that is not the purpose right ? Our purpose here is to the things going end to end from websocket to prometheus. So let\u0026quot;s grab just what we need for this exercise !\nimport json import websocket from prometheus_client import Gauge, start_http_server g = Gauge(\u0026quot;SymbolPrice\u0026quot;, \u0026quot;Symbol high and low price\u0026quot;, [\u0026quot;symbols\u0026quot;]) start_http_server(8000) def on_message(message): data = json.loads(message) if \u0026quot;k\u0026quot; not in data: pass else: print(data[\u0026quot;s\u0026quot;], data[\u0026quot;k\u0026quot;][\u0026quot;l\u0026quot;], data[\u0026quot;k\u0026quot;][\u0026quot;h\u0026quot;]) g.labels(f\u0026quot;{data[\u0026quot;s\u0026quot;]}-high\u0026quot;).set(data[\u0026quot;k\u0026quot;][\u0026quot;h\u0026quot;]) g.labels(f\u0026quot;{data[\u0026quot;s\u0026quot;]}-low\u0026quot;).set(data[\u0026quot;k\u0026quot;][\u0026quot;l\u0026quot;])  Quick explanation Line 5 create a Gauge object, a Gauge is a type of metrics to record a value. Like mentioned before there are other types of metrics worth exploring, sounds like a place to spend our \u0026ldquo;tech time\u0026rdquo;.\nLine 6 create a Prometheus endpoint where you could see the metrics at http://localhost:8080.\nWhen you run python code with above updates, you could start the metrics endpoint from http://localhost:8080 and see the following output. When you refresh the page you should be able to see symbol high and low price being updated! yay ! 😀\n# HELP python_gc_objects_collected_total Objects collected during gc # TYPE python_gc_objects_collected_total counter python_gc_objects_collected_total{generation=\u0026quot;0\u0026quot;} 311.0 python_gc_objects_collected_total{generation=\u0026quot;1\u0026quot;} 71.0 python_gc_objects_collected_total{generation=\u0026quot;2\u0026quot;} 0.0 # HELP python_gc_objects_uncollectable_total Uncollectable object found during GC # TYPE python_gc_objects_uncollectable_total counter python_gc_objects_uncollectable_total{generation=\u0026quot;0\u0026quot;} 0.0 python_gc_objects_uncollectable_total{generation=\u0026quot;1\u0026quot;} 0.0 python_gc_objects_uncollectable_total{generation=\u0026quot;2\u0026quot;} 0.0 # HELP python_gc_collections_total Number of times this generation was collected # TYPE python_gc_collections_total counter python_gc_collections_total{generation=\u0026quot;0\u0026quot;} 41.0 python_gc_collections_total{generation=\u0026quot;1\u0026quot;} 3.0 python_gc_collections_total{generation=\u0026quot;2\u0026quot;} 0.0 # HELP python_info Python platform information # TYPE python_info gauge python_info{implementation=\u0026quot;CPython\u0026quot;,major=\u0026quot;3\u0026quot;,minor=\u0026quot;9\u0026quot;,patchlevel=\u0026quot;0\u0026quot;,version=\u0026quot;3.9.0\u0026quot;} 1.0 # HELP SymbolPrice Symbol high and low price # TYPE SymbolPrice gauge SymbolPrice{symbols=\u0026quot;BNBUSDT-high\u0026quot;} 363.2 SymbolPrice{symbols=\u0026quot;BNBUSDT-low\u0026quot;} 363.0 SymbolPrice{symbols=\u0026quot;XRPUSDT-high\u0026quot;} 0.7599 SymbolPrice{symbols=\u0026quot;XRPUSDT-low\u0026quot;} 0.7591 SymbolPrice{symbols=\u0026quot;ETHUSDT-high\u0026quot;} 2525.16 SymbolPrice{symbols=\u0026quot;ETHUSDT-low\u0026quot;} 2523.81 SymbolPrice{symbols=\u0026quot;BTCUSDT-high\u0026quot;} 37984.81 SymbolPrice{symbols=\u0026quot;BTCUSDT-low\u0026quot;} 37971.19 SymbolPrice{symbols=\u0026quot;DOGEUSDT-high\u0026quot;} 0.1113 SymbolPrice{symbols=\u0026quot;DOGEUSDT-low\u0026quot;} 0.1112  Part 4. Create Prometheus server and receive metrics data for visualization In this part you will need docker installed for creating Prometheus server in container. Please refer to official documentation for setup.\nObviously you could run docker cmd for this, I have also got a docker-compose.yml here as well.\nversion: \u0026quot;3.9\u0026quot; services: prometheus: image: prom/prometheus ports: - \u0026quot;9090:9090\u0026quot; volumes: - /pathofown/prometheus.yml:/etc/prometheus/prometheus.yml  Before running this you will also need a prometheus configuration file with the name of \u0026ldquo;prometheus.yml\u0026rdquo;, in a nutshell in this file you will specify the metrics endpoint from where the server is going to collect data from. When running the Prometheus container, you need to mount the volume for you Prometheus container so that this config file is place in \u0026ldquo;/etc/prometheus/prometheus.yml\u0026rdquo; at container runtime.\nglobal: scrape_interval: 5s evaluation_interval: 5s scrape_configs: - job_name: \u0026quot;prome-local\u0026quot; static_configs: - targets: [\u0026quot;localhost:9090\u0026quot;] - job_name: \u0026quot;TestJob\u0026quot; static_configs: - targets: [\u0026quot;host.docker.internal:8000\u0026quot;]  Quick explanation Line 8 specifies the port of Prometheus server running locally Line 11 specifies the port and DNS name of the metric endpoint created in Part 3. Note the dns is \u0026ldquo;host.docker.internal\u0026rdquo;, this worked for me when running containers with Docker Desktop.\n Note the Prometheus server configuration could be way more complicated than what we are doing here.Again the purpose of this blog is not to setup prometheus for production, we only what to tip our teo on the surface and have a feel on it!  Lets start Docker Desktop and run Prometheus server locally either using docker cmd or docker-compose at choice of yours. When navigating to http://localhost:9090/targets，voilà ！Both the Prometheus server and the Metric endpoints for the websocket are up and running !\nAs you can see both of the jobs that you specified in the prometheus.yml file are running !\nWhen navigate back to the graph page, you can easily enter \u0026ldquo;symbolPrice\u0026rdquo; in the search box and hit execute. You then should be able to see a graph like the following and you can highlight different labels to see the price change for each of the symbols. In may case I selected \u0026ldquo;BNBUSDT-high\u0026rdquo;\nCongratulations! You\u0026quot;v just reach the end of this blog, I know right ? It is a rather long blog to read, but at least I found the exercise pretty interesting and when you see the graph in Prometheus, it somewhat feeling really comforting !\nThanks for you patience, see you at my next blog !!\n","permalink":"https://blacklabnz.github.io/posts/websocket-prometheus/","summary":"Recently I was tasked with consuming data from websocket, analyze it and then send data to Prometheus. The theory is pretty straight forward: getting data from websocket API in a stream and analyze and take the data points and send it to prometheus for visualization. In this blog you will have all the steps and code needed to reproduce this flow. With this in mind, I decided using python to achieve all these.","title":"Consume Websocket stream and send to Prometheus in Python"}]