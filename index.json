[{"content":"Recently I was tasked with consuming data from websocket, analyze it and then send data to Prometheus. The theory is pretty straight forward: getting data from websocket API in a stream and analyze and take the data points and send it to prometheus for visualization. In this blog you will have all the steps and code needed to reproduce this flow. With this in mind, I decided using python to achieve all these.\nPart 1. Websocket VS Rest reminder Before we start, I would like to have a bit of revision on Websocket API and how it is different from REST API\nThe diagram I took from internet explains it quite well. In simple term, you interact with REST API with a request and response fashion whereas in websocket there is a two way connection established during interaction lifecycle therefore you don\u0026rsquo;t need to constantly send request to server for retrieving data. At the end of the interaction, the two way connection is close.\nPart 2. Consuming a websocket API You could easily find some publicly available websocket API, the one I used for this blog is from Binance, one of the platform used by coin traders. Though myself is not doing any coin trading nor receiving any sponsorship from them. They have very detailed API documentation on their Spot API.\nThe following code snippet can be used to connect to websocket API:\nimport json import websocket socket = \u0026#34;wss://stream.binance.com:9443/ws/bnbusdt@kline_1m\u0026#34; def on_open(ws): print(\u0026#34;Opened connection\u0026#34;) def on_message(ws, message): data = json.loads(message) print(data) def on_open(ws): print(\u0026#34;Opened connection\u0026#34;) def on_error(ws, error): print(error) ws = websocket.WebSocketApp( socket, on_open=on_open, on_message=on_message, on_error=on_error, on_close=on_close) ws.run_forever() Quick explanation Line 4 defines the websocket url, the details of this endpoint can be found here. \u0026ldquo;/bnbusdt@kline_1m\u0026rdquo; means retrieving data from the kline stream for bnb vs usdt, what is so called a symbol, these are crypto coin terminologies which you can fine richer explanations else where.\nLine 6 to line 17 defines the callback function with minimal functionality when message is received from the server. More details can be found in websocket-client official documentation.\nLine 19 and 10 creates a websocket instance and start connection and run forever.\nRun the above python code will give you this console output:\nOpened connection {\u0026#34;e\u0026#34;: \u0026#34;kline\u0026#34;, \u0026#34;E\u0026#34;: 1647220248756, \u0026#34;s\u0026#34;: \u0026#34;BNBUSDT\u0026#34;, \u0026#34;k\u0026#34;: {\u0026#34;t\u0026#34;: 1647220200000, \u0026#34;T\u0026#34;: 1647220259999, \u0026#34;s\u0026#34;: \u0026#34;BNBUSDT\u0026#34;, \u0026#34;i\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;f\u0026#34;: 527449288, \u0026#34;L\u0026#34;: 527449503, \u0026#34;o\u0026#34;: \u0026#34;364.60000000\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;364.30000000\u0026#34;, \u0026#34;h\u0026#34;: \u0026#34;364.60000000\u0026#34;, \u0026#34;l\u0026#34;: \u0026#34;364.20000000\u0026#34;, \u0026#34;v\u0026#34;: \u0026#34;458.54400000\u0026#34;, \u0026#34;n\u0026#34;: 216, \u0026#34;x\u0026#34;: False, \u0026#34;q\u0026#34;: \u0026#34;167076.68030000\u0026#34;, \u0026#34;V\u0026#34;: \u0026#34;183.99400000\u0026#34;, \u0026#34;Q\u0026#34;: \u0026#34;67043.53500000\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;0\u0026#34;}} {\u0026#34;e\u0026#34;: \u0026#34;kline\u0026#34;, \u0026#34;E\u0026#34;: 1647220251279, \u0026#34;s\u0026#34;: \u0026#34;BNBUSDT\u0026#34;, \u0026#34;k\u0026#34;: {\u0026#34;t\u0026#34;: 1647220200000, \u0026#34;T\u0026#34;: 1647220259999, \u0026#34;s\u0026#34;: \u0026#34;BNBUSDT\u0026#34;, \u0026#34;i\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;f\u0026#34;: 527449288, \u0026#34;L\u0026#34;: 527449507, \u0026#34;o\u0026#34;: \u0026#34;364.60000000\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;364.30000000\u0026#34;, \u0026#34;h\u0026#34;: \u0026#34;364.60000000\u0026#34;, \u0026#34;l\u0026#34;: \u0026#34;364.20000000\u0026#34;, \u0026#34;v\u0026#34;: \u0026#34;460.67000000\u0026#34;, \u0026#34;n\u0026#34;: 220, \u0026#34;x\u0026#34;: False, \u0026#34;q\u0026#34;: \u0026#34;167851.03680000\u0026#34;, \u0026#34;V\u0026#34;: \u0026#34;184.66700000\u0026#34;, \u0026#34;Q\u0026#34;: \u0026#34;67288.70890000\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;0\u0026#34;}} {\u0026#34;e\u0026#34;: \u0026#34;kline\u0026#34;, \u0026#34;E\u0026#34;: 1647220253583, \u0026#34;s\u0026#34;: \u0026#34;BNBUSDT\u0026#34;, \u0026#34;k\u0026#34;: {\u0026#34;t\u0026#34;: 1647220200000, \u0026#34;T\u0026#34;: 1647220259999, \u0026#34;s\u0026#34;: \u0026#34;BNBUSDT\u0026#34;, \u0026#34;i\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;f\u0026#34;: 527449288, \u0026#34;L\u0026#34;: 527449513, \u0026#34;o\u0026#34;: \u0026#34;364.60000000\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;364.20000000\u0026#34;, \u0026#34;h\u0026#34;: \u0026#34;364.60000000\u0026#34;, \u0026#34;l\u0026#34;: \u0026#34;364.20000000\u0026#34;, \u0026#34;v\u0026#34;: \u0026#34;461.78000000\u0026#34;, \u0026#34;n\u0026#34;: 226, \u0026#34;x\u0026#34;: False, \u0026#34;q\u0026#34;: \u0026#34;168255.38670000\u0026#34;, \u0026#34;V\u0026#34;: \u0026#34;185.54600000\u0026#34;, \u0026#34;Q\u0026#34;: \u0026#34;67608.92860000\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;0\u0026#34;}} Connection closed Futher explore the API Previous step works fine with a single symbol, what if in the websocket I need data from more symbol or even in any websockets ?? The websocket API kindly offers subscription mode, with which you can subscribe multiple symbols and get the data back within the same websocket connection. refer to the following code for this:\nimport json import websocket socket = \u0026#34;wss://stream.binance.com:9443/ws\u0026#34; ws = websocket.create_connection(socket) ws.send(json.dumps({ \u0026#34;method\u0026#34;: \u0026#34;SUBSCRIBE\u0026#34;, \u0026#34;params\u0026#34;: [ \u0026#34;btcusdt@kline_1m\u0026#34;, \u0026#34;bnbusdt@kline_1m\u0026#34;, \u0026#34;ethusdt@kline_1m\u0026#34;, \u0026#34;dogeusdt@kline_1m\u0026#34;, \u0026#34;xrpusdt@kline_1m\u0026#34; ], \u0026#34;id\u0026#34;: 1 })) def on_message(message): data = json.loads(message) print(data) while True: result = ws.recv() on_message(result) ws.close() Quick explanation Line 6-17 creates a websocket connection, the first action is to send a message to endpoint to subscribe to steams that are of interests. The payload of the subscription is like this:\n{ \u0026#34;method\u0026#34;: \u0026#34;SUBSCRIBE\u0026#34;, \u0026#34;params\u0026#34;: [ \u0026#34;btcusdt@kline_1m\u0026#34;, \u0026#34;bnbusdt@kline_1m\u0026#34;, \u0026#34;ethusdt@kline_1m\u0026#34;, \u0026#34;dogeusdt@kline_1m\u0026#34;, \u0026#34;xrpusdt@kline_1m\u0026#34; ], \u0026#34;id\u0026#34;: 1 } According to the API doc, the response of the first ws.sent(payload) call is\n{ \u0026#34;result\u0026#34;: null, \u0026#34;id\u0026#34;: 1 } When we inspect the console output when running the above python code we also get the following which indicates we are NOT doing something crazy !\n{\u0026#34;result\u0026#34;: \u0026#34;None\u0026#34;, \u0026#34;id\u0026#34;: 1} {\u0026#34;e\u0026#34;: \u0026#34;kline\u0026#34;, \u0026#34;E\u0026#34;: 1647220380568, \u0026#34;s\u0026#34;: \u0026#34;XRPUSDT\u0026#34;, \u0026#34;k\u0026#34;: {\u0026#34;t\u0026#34;: 1647220320000, \u0026#34;T\u0026#34;: 1647220379999, \u0026#34;s\u0026#34;: \u0026#34;XRPUSDT\u0026#34;, \u0026#34;i\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;f\u0026#34;: 429501724, \u0026#34;L\u0026#34;: 429501859, \u0026#34;o\u0026#34;: \u0026#34;0.75860000\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;0.75920000\u0026#34;, \u0026#34;h\u0026#34;: \u0026#34;0.75920000\u0026#34;, \u0026#34;l\u0026#34;: \u0026#34;0.75860000\u0026#34;, \u0026#34;v\u0026#34;: \u0026#34;81089.00000000\u0026#34;, \u0026#34;n\u0026#34;: 136, \u0026#34;x\u0026#34;: true, \u0026#34;q\u0026#34;: \u0026#34;61541.60570000\u0026#34;, \u0026#34;V\u0026#34;: \u0026#34;63081.00000000\u0026#34;, \u0026#34;Q\u0026#34;: \u0026#34;47874.45210000\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;0\u0026#34;}} {\u0026#34;e\u0026#34;: \u0026#34;kline\u0026#34;, \u0026#34;E\u0026#34;: 1647220382238, \u0026#34;s\u0026#34;: \u0026#34;BTCUSDT\u0026#34;, \u0026#34;k\u0026#34;: {\u0026#34;t\u0026#34;: 1647220380000, \u0026#34;T\u0026#34;: 1647220439999, \u0026#34;s\u0026#34;: \u0026#34;BTCUSDT\u0026#34;, \u0026#34;i\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;f\u0026#34;: 1291349545, \u0026#34;L\u0026#34;: 1291349561, \u0026#34;o\u0026#34;: \u0026#34;38183.79000000\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;38183.79000000\u0026#34;, \u0026#34;h\u0026#34;: \u0026#34;38183.80000000\u0026#34;, \u0026#34;l\u0026#34;: \u0026#34;38183.79000000\u0026#34;, \u0026#34;v\u0026#34;: \u0026#34;0.25519000\u0026#34;, \u0026#34;n\u0026#34;: 17, \u0026#34;x\u0026#34;: false, \u0026#34;q\u0026#34;: \u0026#34;9744.12173000\u0026#34;, \u0026#34;V\u0026#34;: \u0026#34;0.03599000\u0026#34;, \u0026#34;Q\u0026#34;: \u0026#34;1374.23496200\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;0\u0026#34;}} {\u0026#34;e\u0026#34;: \u0026#34;kline\u0026#34;, \u0026#34;E\u0026#34;: 1647220382372, \u0026#34;s\u0026#34;: \u0026#34;BNBUSDT\u0026#34;, \u0026#34;k\u0026#34;: {\u0026#34;t\u0026#34;: 1647220380000, \u0026#34;T\u0026#34;: 1647220439999, \u0026#34;s\u0026#34;: \u0026#34;BNBUSDT\u0026#34;, \u0026#34;i\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;f\u0026#34;: 527450005, \u0026#34;L\u0026#34;: 527450009, \u0026#34;o\u0026#34;: \u0026#34;363.80000000\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;363.70000000\u0026#34;, \u0026#34;h\u0026#34;: \u0026#34;363.80000000\u0026#34;, \u0026#34;l\u0026#34;: \u0026#34;363.70000000\u0026#34;, \u0026#34;v\u0026#34;: \u0026#34;2.58200000\u0026#34;, \u0026#34;n\u0026#34;: 5, \u0026#34;x\u0026#34;: false, \u0026#34;q\u0026#34;: \u0026#34;939.30620000\u0026#34;, \u0026#34;V\u0026#34;: \u0026#34;2.32800000\u0026#34;, \u0026#34;Q\u0026#34;: \u0026#34;846.92640000\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;0\u0026#34;}} As you could see from the screenshot, the first response from the server is the acknowledgement of the subscription. The data following the acknowledgement is the data for the symbols(remember the crypto trading jargon?), and indeed we are receiving data for all the symbols we have subscribed to !\nPart 3. Add prometheus-client and create metric endpoint To start with part, first we want to add prometheus client to our code so that we could see metric being sent.\nRecall the follow symbol data output from our previous step:\n{ \u0026#34;e\u0026#34;: \u0026#34;kline\u0026#34;, // Event type  \u0026#34;E\u0026#34;: 123456789, // Event time  \u0026#34;s\u0026#34;: \u0026#34;BNBBTC\u0026#34;, // Symbol  \u0026#34;k\u0026#34;: { \u0026#34;t\u0026#34;: 123400000, // Kline start time  \u0026#34;T\u0026#34;: 123460000, // Kline close time  \u0026#34;s\u0026#34;: \u0026#34;BNBBTC\u0026#34;, // Symbol  \u0026#34;i\u0026#34;: \u0026#34;1m\u0026#34;, // Interval  \u0026#34;f\u0026#34;: 100, // First trade ID  \u0026#34;L\u0026#34;: 200, // Last trade ID  \u0026#34;o\u0026#34;: \u0026#34;0.0010\u0026#34;, // Open price  \u0026#34;c\u0026#34;: \u0026#34;0.0020\u0026#34;, // Close price  \u0026#34;h\u0026#34;: \u0026#34;0.0025\u0026#34;, // High price  \u0026#34;l\u0026#34;: \u0026#34;0.0015\u0026#34;, // Low price  \u0026#34;v\u0026#34;: \u0026#34;1000\u0026#34;, // Base asset volume  \u0026#34;n\u0026#34;: 100, // Number of trades  \u0026#34;x\u0026#34;: false, // Is this kline closed?  \u0026#34;q\u0026#34;: \u0026#34;1.0000\u0026#34;, // Quote asset volume  \u0026#34;V\u0026#34;: \u0026#34;500\u0026#34;, // Taker buy base asset volume  \u0026#34;Q\u0026#34;: \u0026#34;0.500\u0026#34;, // Taker buy quote asset volume  \u0026#34;B\u0026#34;: \u0026#34;123456\u0026#34; // Ignore  } } With reference to the doc, for our case we are interested in \u0026ldquo;l\u0026rdquo; and \u0026ldquo;h\u0026rdquo; in the \u0026ldquo;k\u0026rdquo; property for the low price and high price. Obviously we are interested in the \u0026ldquo;s\u0026rdquo; property as well for the name of the symbol. Once we understand what we need to do with our data, now it time to update the callback function accordingly to do our \u0026ldquo;analysis\u0026rdquo;. Our analysis here is as simple as just reading some properties and this is not very important here as it is on the business side in the real world, our goal in this blog is to retrieve the data and send to Prometheus and we can just send some properties as if it is our \u0026ldquo;analysis\u0026rdquo;. So let\u0026quot;s go\ndef on_message(message): data = json.loads(message) if \u0026#34;k\u0026#34; not in data: pass else: print(data[\u0026#34;s\u0026#34;], data[\u0026#34;k\u0026#34;][\u0026#34;l\u0026#34;], data[\u0026#34;k\u0026#34;][\u0026#34;h\u0026#34;]) Once call back function is updated we should see the following output that captures properties we are interested\nBTCUSDT 38155.15000000 38170.16000000 ETHUSDT 2534.36000000 2535.02000000 XRPUSDT 0.76010000 0.76040000 BNBUSDT 364.40000000 364.50000000 DOGEUSDT 0.11180000 0.11180000 BTCUSDT 38153.01000000 38170.16000000 Now lets start ingesting data to Prometheus using prometheus-client. Obviously we can spent our whole night trying to understand the nitty gritty of prometheus-client, but that is not the purpose right ? Our purpose here is to the things going end to end from websocket to prometheus. So let\u0026quot;s grab just what we need for this exercise !\nimport json import websocket from prometheus_client import Gauge, start_http_server g = Gauge(\u0026#34;SymbolPrice\u0026#34;, \u0026#34;Symbol high and low price\u0026#34;, [\u0026#34;symbols\u0026#34;]) start_http_server(8000) def on_message(message): data = json.loads(message) if \u0026#34;k\u0026#34; not in data: pass else: print(data[\u0026#34;s\u0026#34;], data[\u0026#34;k\u0026#34;][\u0026#34;l\u0026#34;], data[\u0026#34;k\u0026#34;][\u0026#34;h\u0026#34;]) g.labels(f\u0026#34;{data[\u0026#34;s\u0026#34;]}-high\u0026#34;).set(data[\u0026#34;k\u0026#34;][\u0026#34;h\u0026#34;]) g.labels(f\u0026#34;{data[\u0026#34;s\u0026#34;]}-low\u0026#34;).set(data[\u0026#34;k\u0026#34;][\u0026#34;l\u0026#34;]) Quick explanation Line 5 create a Gauge object, a Gauge is a type of metrics to record a value. Like mentioned before there are other types of metrics worth exploring, sounds like a place to spend our \u0026ldquo;tech time\u0026rdquo;.\nLine 6 create a Prometheus endpoint where you could see the metrics at http://localhost:8080.\nWhen you run python code with above updates, you could start the metrics endpoint from http://localhost:8080 and see the following output. When you refresh the page you should be able to see symbol high and low price being updated! yay ! üòÄ\n# HELP python_gc_objects_collected_total Objects collected during gc # TYPE python_gc_objects_collected_total counter python_gc_objects_collected_total{generation=\u0026#34;0\u0026#34;} 311.0 python_gc_objects_collected_total{generation=\u0026#34;1\u0026#34;} 71.0 python_gc_objects_collected_total{generation=\u0026#34;2\u0026#34;} 0.0 # HELP python_gc_objects_uncollectable_total Uncollectable object found during GC # TYPE python_gc_objects_uncollectable_total counter python_gc_objects_uncollectable_total{generation=\u0026#34;0\u0026#34;} 0.0 python_gc_objects_uncollectable_total{generation=\u0026#34;1\u0026#34;} 0.0 python_gc_objects_uncollectable_total{generation=\u0026#34;2\u0026#34;} 0.0 # HELP python_gc_collections_total Number of times this generation was collected # TYPE python_gc_collections_total counter python_gc_collections_total{generation=\u0026#34;0\u0026#34;} 41.0 python_gc_collections_total{generation=\u0026#34;1\u0026#34;} 3.0 python_gc_collections_total{generation=\u0026#34;2\u0026#34;} 0.0 # HELP python_info Python platform information # TYPE python_info gauge python_info{implementation=\u0026#34;CPython\u0026#34;,major=\u0026#34;3\u0026#34;,minor=\u0026#34;9\u0026#34;,patchlevel=\u0026#34;0\u0026#34;,version=\u0026#34;3.9.0\u0026#34;} 1.0 # HELP SymbolPrice Symbol high and low price # TYPE SymbolPrice gauge SymbolPrice{symbols=\u0026#34;BNBUSDT-high\u0026#34;} 363.2 SymbolPrice{symbols=\u0026#34;BNBUSDT-low\u0026#34;} 363.0 SymbolPrice{symbols=\u0026#34;XRPUSDT-high\u0026#34;} 0.7599 SymbolPrice{symbols=\u0026#34;XRPUSDT-low\u0026#34;} 0.7591 SymbolPrice{symbols=\u0026#34;ETHUSDT-high\u0026#34;} 2525.16 SymbolPrice{symbols=\u0026#34;ETHUSDT-low\u0026#34;} 2523.81 SymbolPrice{symbols=\u0026#34;BTCUSDT-high\u0026#34;} 37984.81 SymbolPrice{symbols=\u0026#34;BTCUSDT-low\u0026#34;} 37971.19 SymbolPrice{symbols=\u0026#34;DOGEUSDT-high\u0026#34;} 0.1113 SymbolPrice{symbols=\u0026#34;DOGEUSDT-low\u0026#34;} 0.1112 Part 4. Create Prometheus server and receive metrics data for visualization In this part you will need docker installed for creating Prometheus server in container. Please refer to official documentation for setup.\nObviously you could run docker cmd for this, I have also got a docker-compose.yml here as well.\nversion:\u0026#34;3.9\u0026#34;services:prometheus:image:prom/prometheusports:- \u0026#34;9090:9090\u0026#34;volumes:- /pathofown/prometheus.yml:/etc/prometheus/prometheus.ymlBefore running this you will also need a prometheus configuration file with the name of \u0026ldquo;prometheus.yml\u0026rdquo;, in a nutshell in this file you will specify the metrics endpoint from where the server is going to collect data from. When running the Prometheus container, you need to mount the volume for you Prometheus container so that this config file is place in \u0026ldquo;/etc/prometheus/prometheus.yml\u0026rdquo; at container runtime.\nglobal:scrape_interval:5sevaluation_interval:5sscrape_configs:- job_name:\u0026#34;prome-local\u0026#34;static_configs:- targets:[\u0026#34;localhost:9090\u0026#34;]- job_name:\u0026#34;TestJob\u0026#34;static_configs:- targets:[\u0026#34;host.docker.internal:8000\u0026#34;]Quick explanation Line 8 specifies the port of Prometheus server running locally Line 11 specifies the port and DNS name of the metric endpoint created in Part 3. Note the dns is \u0026ldquo;host.docker.internal\u0026rdquo;, this worked for me when running containers with Docker Desktop.\n Note the Prometheus server configuration could be way more complicated than what we are doing here.Again the purpose of this blog is not to setup prometheus for production, we only what to tip our teo on the surface and have a feel on it!  Lets start Docker Desktop and run Prometheus server locally either using docker cmd or docker-compose at choice of yours. When navigating to http://localhost:9090/targetsÔºåvoil√† ÔºÅBoth the Prometheus server and the Metric endpoints for the websocket are up and running !\nAs you can see both of the jobs that you specified in the prometheus.yml file are running !\nWhen navigate back to the graph page, you can easily enter \u0026ldquo;symbolPrice\u0026rdquo; in the search box and hit execute. You then should be able to see a graph like the following and you can highlight different labels to see the price change for each of the symbols. In may case I selected \u0026ldquo;BNBUSDT-high\u0026rdquo;\nCongratulations! You\u0026quot;v just reach the end of this blog, I know right ? It is a rather long blog to read, but at least I found the exercise pretty interesting and when you see the graph in Prometheus, it somewhat feeling really comforting !\nThanks for you patience, see you at my next blog !!\n","permalink":"https://blacklabnz.github.io/posts/websocket-prometheus/","summary":"Recently I was tasked with consuming data from websocket, analyze it and then send data to Prometheus. The theory is pretty straight forward: getting data from websocket API in a stream and analyze and take the data points and send it to prometheus for visualization. In this blog you will have all the steps and code needed to reproduce this flow. With this in mind, I decided using python to achieve all these.","title":"Consume Websocket stream and send to Prometheus in Python"},{"content":"What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.\nThis configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.\nIn this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it. For all infra related deployment I am going to use Terraform as much as possible to avoid any \u0026ldquo;ClickOps\u0026rdquo; efforts. Once Infra is done, we will run a quick notebook to validate the connectivity as well as DNS resolution\nPart 1 Overall Architecture of the lab In this lab we will use databricks with secured connectivity and vNet injection to access some data residing in the ADLS with private endpoint configuration. We will also create keyvault for keeping the secrets involved using private endpoint. In the end we will use a simply python notebook to validate two things against the private Ip of the private endpoint:\n the network connectivity DNS resolution  As revealed on the diagram, we are simulating a well adopted hub and spoke topology. In our case we don\u0026rsquo;t have any hub for this lab, but two spokes that connects to each other via vNet peering. For the storage account private endpoint, well, we could have deployed that into the databricks vNet and they would naturally have connectivity(and secured by NSG if needed). But that removes the fun of dealing with Networking and sometime there are enough reason on the design to keep the private endpoint virtual network separate from other virtual network. We will also create keyvault with private endpoint and use that for a KV backed secret scope in databricks. In a nutshell a secret scope is a way to securely store secrets such as service principal key or a storage account key or any other type of secrets as such, so that we do hard code and secrets but are able to use \u0026ldquo;dbutil\u0026rdquo; to retrieve the secrets in a notebook run. Simple but yet important principle right ? no hard coded secret in your source code !!!!\nProject setup ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ data.tf ‚îú‚îÄ‚îÄ main.tf ‚îú‚îÄ‚îÄ output.tf ‚îú‚îÄ‚îÄ providers.tf ‚îú‚îÄ‚îÄ values.tfvars ‚îú‚îÄ‚îÄ variables.tf ‚îî‚îÄ‚îÄ .gitignore For simplifying our demo, we are going to put all our terraform resources in the main.tf file. In real world development, the best practice would be modularize any components that are reusable. We are also simplifying the terraform variables with locals, you will find the only the authentication related values are kept in variables and passed in with values.tfvars file.\nFor your convenience, you can download tf module required for this lab from my github repo: secure-databricks-terraform\nFirst thing before carry on to the next part is to setup the providers.\nproviders.tf\nterraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;2.97.0\u0026#34; } databricks = { source = \u0026#34;databrickslabs/databricks\u0026#34; version = \u0026#34;0.5.1\u0026#34; } external = { source = \u0026#34;hashicorp/external\u0026#34; version = \u0026#34;2.2.2\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} subscription_id = var.subscription_id client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id } provider \u0026#34;databricks\u0026#34; { host = format(\u0026#34;https://%s\u0026#34;, data.azurerm_databricks_workspace.dbr.workspace_url) azure_workspace_resource_id = data.azurerm_databricks_workspace.dbr.id azure_client_id = var.client_id azure_client_secret = var.client_secret azure_tenant_id = var.tenant_id } Quick explanation: line 16 to line 19 we use service principal to authenticate to azure, in here we use variables to represent the value so that later on we can specify the values.tfvars file and pass the value in. This is really handy if you need to deal with multiple Azure environment that has different auth service principals. Refer the official doc here.\nTo be able to use variables we will need to add the following to the variables file:\nvariable.tf\nvariable \u0026#34;subscription_id\u0026#34; { type = string default = \u0026#34;default\u0026#34; } variable \u0026#34;client_id\u0026#34; { type = string default = \u0026#34;default\u0026#34; } variable \u0026#34;client_secret\u0026#34; { type = string default = \u0026#34;default\u0026#34; } variable \u0026#34;tenant_id\u0026#34; { type = string default = \u0026#34;default\u0026#34; } line 23 to 27 similar deal here. One thing to note is that we are retrieving the databricks workspace url and resource id by using tf data object. The following needs to be added in the data.tf to achieve this.\ndata.tf\ndata \u0026#34;azurerm_databricks_workspace\u0026#34; \u0026#34;dbr\u0026#34; { name = azurerm_databricks_workspace.dbr.name resource_group_name = local.rg } Lastly you will need to create a service principal that has contributor access to the subscription that you are about to deploy to. You can find more details in this office doc. Once Service principal is created you will need to create a secret and use that alone side with the application id to authentication to Azure. When finished, please create a values.tfvars file where you could input those required auth information for terraform to pass in as variables. The information required are:\n subscription id service principal client id service principal client secret azure tenant id.  values.tfvars\nsubscription_id = \u0026#34;xxxxxxxxx\u0026#34; client_id = \u0026#34;xxxxxxxxx\u0026#34; client_secret = \u0026#34;xxxxxxxxx\u0026#34; tenant_id = \u0026#34;xxxxxxxxx\u0026#34; At this point we will need to initiate our terraform workspace by running:\nterraform init With that all set, lets move on to next part of creating actual resources.\nPart 2 Virtual networks and peering First we will add some variables to main.tf that we can use in the main file to the locals block\nmain.tf\nlocals { org = \u0026#34;blk\u0026#34; rg = \u0026#34;${local.org}-secure-dbr\u0026#34; rg_location = \u0026#34;australiaeast\u0026#34; vnet_dbr = \u0026#34;${local.org}-spoke-dbr\u0026#34; vnet_resource = \u0026#34;${local.org}-spoke-resource\u0026#34; vnet_dbr_address_space = \u0026#34;10.0.0.0/16\u0026#34; vnet_dbr_subnet_prv = \u0026#34;private\u0026#34; vnet_dbr_subnet_prv_prefix = \u0026#34;10.0.1.0/24\u0026#34; vnet_dbr_subnet_pub = \u0026#34;public\u0026#34; vnet_dbr_subnet_pub_prefix = \u0026#34;10.0.2.0/24\u0026#34; vnet_resource_address_space = \u0026#34;10.1.0.0/16\u0026#34; vnet_resource_subnet_stor = \u0026#34;stor\u0026#34; vnet_resource_subnet_stor_prefix = \u0026#34;10.1.1.0/24\u0026#34; vnet_resource_subnet_kv = \u0026#34;keyvault\u0026#34; vnet_resource_subnet_kv_prefix = \u0026#34;10.1.2.0/24\u0026#34; } Line 2 to line 6 defines some local variables that we can reuse to build up the name of vNets. Line 8 to 14 defines the databricks vNet. Line 16 to 22 defines the resource vNet which is used by storage account and keyvault.\n2.1 Spoke vNet for databricks Pretty standard configuration here, one thing to note is that the subnets name is dictated to be private and public. There are also requirements to the minimum size of the subnets, please refere to this doc for more details.\nAll we need to do is to add the following to main.tf next to the locals block.\nmain.tf\nresource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;secure_dbr\u0026#34; { name = local.rg location = local.rg_location } resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vnet_dbr\u0026#34; { name = local.vnet_dbr location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name address_space = [local.vnet_dbr_address_space] depends_on = [azurerm_resource_group.secure_dbr] } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;dbr_prv\u0026#34; { name = local.vnet_dbr_subnet_prv resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name address_prefixes = [local.vnet_dbr_subnet_prv_prefix] delegation { name = \u0026#34;dbr_prv_dlg\u0026#34; service_delegation { name = \u0026#34;Microsoft.Databricks/workspaces\u0026#34; actions = [ \u0026#34;Microsoft.Network/virtualNetworks/subnets/join/action\u0026#34;, \u0026#34;Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\u0026#34;, \u0026#34;Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action\u0026#34;, ] } } depends_on = [azurerm_virtual_network.vnet_dbr] } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;dbr_pub\u0026#34; { name = local.vnet_dbr_subnet_pub resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name address_prefixes = [local.vnet_dbr_subnet_pub_prefix] delegation { name = \u0026#34;dbr_pub_dlg\u0026#34; service_delegation { name = \u0026#34;Microsoft.Databricks/workspaces\u0026#34; actions = [ \u0026#34;Microsoft.Network/virtualNetworks/subnets/join/action\u0026#34;, \u0026#34;Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\u0026#34;, \u0026#34;Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action\u0026#34;, ] } } depends_on = [azurerm_virtual_network.vnet_dbr] } Quick explanation: Line 1-4 defines the resource group. Line 6-13 defines the dbr vNet. Line 15-53 defines the private and public subnets for the clusters.  Note in each of the subnet there is subnet delegation properties that needs to be added. The delegation block is not mandatory as Azure will take care of that, but if you don't add that, next time when you do terraform plan, the delegation property will be recognized as change even though there is nothing changed. \n2.2 Spoke vNet for storage account and keyvault Nothing complicated here, just need a separate subnets for storage account and keyvault private endpoints. Add the following to main.tf file.\nresource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vnet_resource\u0026#34; { name = local.vnet_resource location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name address_space = [local.vnet_resource_address_space] depends_on = [azurerm_resource_group.secure_dbr] } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;resource_stor\u0026#34; { name = local.vnet_resource_subnet_stor resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name address_prefixes = [local.vnet_resource_subnet_stor_prefix] enforce_private_link_endpoint_network_policies = true depends_on = [azurerm_virtual_network.vnet_resource] } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;resource_kv\u0026#34; { name = local.vnet_resource_subnet_kv resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name address_prefixes = [local.vnet_resource_subnet_kv_prefix] enforce_private_link_endpoint_network_policies = true depends_on = [azurerm_virtual_network.vnet_resource] } Quick explanation Note the difference on the subnet compare to the above. Subnets provisioned for storage account and keyvault does not require subnet delegation. However it need the \u0026ldquo;Private endpoint network policy\u0026rdquo; to be enabled before any private endpoint can be deployed in it.\n2.3 vNet peering Virtual network peering ensures the connectivity between two virtual networks. In some environments direct spoke to spoke communication via peering is disable, all traffic between vNet peers goes via hub network, filtered and processed by firewall. However purpose of our lab is to focus on Databricks and private endpoints, we will use peering, a simpler setup to achieve spoke to spoke connectivity. I will write a separate article to talk about best practice of Hub-Spoke topology.\nresource \u0026#34;azurerm_virtual_network_peering\u0026#34; \u0026#34;dbr_to_resource\u0026#34; { name = \u0026#34;dbr-vent-to-resource-vnet\u0026#34; resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_dbr.name remote_virtual_network_id = azurerm_virtual_network.vnet_resource.id } resource \u0026#34;azurerm_virtual_network_peering\u0026#34; \u0026#34;resource_to_dbr\u0026#34; { name = \u0026#34;resource-vent-to-dbr-vnet\u0026#34; resource_group_name = local.rg virtual_network_name = azurerm_virtual_network.vnet_resource.name remote_virtual_network_id = azurerm_virtual_network.vnet_dbr.id } Quick explanation The vNet peering needs to be setup from both directions, hence you find that there are two peering blocks required to achieve this.\nOnce you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars followed by:\nterraform apply -auto-approve -var-file=values.tfvars Let quickly go to the portal and validate that our vNets are created and peering is all connected. Part 3 Databricks workspace and cluster Lets create Databricks workspace and clusters in this part.\n3.1 Databricks secure connectivity + vNet injection To remove the exposure to public internet traffic, clusters can be deployed with no-pubip configuration and deployed into pre-defined vNet. First need to add the following to locals block:\ndbr = \u0026#34;${local.org}-secure-dbr\u0026#34; dbr_sku = \u0026#34;premium\u0026#34; dbr_mgmt_rg = \u0026#34;${local.dbr}-mgmt-rg\u0026#34; dbr_nsg = \u0026#34;${local.org}-nsg\u0026#34; dbr_cluster = \u0026#34;${local.org}-cluster\u0026#34; Then add the following to main.tf\nresource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;dbr_nsg\u0026#34; { name = local.dbr_nsg location = local.rg_location resource_group_name = azurerm_resource_group.secure_dbr.name } resource \u0026#34;azurerm_subnet_network_security_group_association\u0026#34; \u0026#34;dbr_prv\u0026#34; { subnet_id = azurerm_subnet.dbr_prv.id network_security_group_id = azurerm_network_security_group.dbr_nsg.id } resource \u0026#34;azurerm_subnet_network_security_group_association\u0026#34; \u0026#34;dbr_pub\u0026#34; { subnet_id = azurerm_subnet.dbr_pub.id network_security_group_id = azurerm_network_security_group.dbr_nsg.id } resource \u0026#34;azurerm_databricks_workspace\u0026#34; \u0026#34;dbr\u0026#34; { name = local.dbr resource_group_name = local.rg location = local.rg_location sku = local.dbr_sku managed_resource_group_name = local.dbr_mgmt_rg custom_parameters { no_public_ip = true virtual_network_id = azurerm_virtual_network.vnet_dbr.id public_subnet_name = azurerm_subnet.dbr_pub.name private_subnet_name = azurerm_subnet.dbr_prv.name public_subnet_network_security_group_association_id = azurerm_subnet_network_security_group_association.dbr_pub.id private_subnet_network_security_group_association_id = azurerm_subnet_network_security_group_association.dbr_prv.id } } Quick explanation Line 1-15 defines the network security group and the association, this is mandatory for databricks cluster vNet injection. Line 17-34 defines the databricks workspace with existing vNet and subnets. Line 26 configures databricks cluster to have not public IP.  Please customize your NSG if you are using this in higher environments, current configuration is minimal so that we could focus on the lab. Please always go with Security first principle. \n3.2 Cluster creation We will use terraform to create clusters, note we will need to add databricks provider to terraform which was already performed in earlier steps.\nresource \u0026#34;databricks_cluster\u0026#34; \u0026#34;cluster\u0026#34; { cluster_name = local.dbr_cluster spark_version = data.databricks_spark_version.latest.id node_type_id = data.databricks_node_type.smallest.id autotermination_minutes = 20 spark_conf = { \u0026#34;spark.databricks.cluster.profile\u0026#34; : \u0026#34;singleNode\u0026#34; \u0026#34;spark.master\u0026#34; : \u0026#34;local[*]\u0026#34; } custom_tags = { \u0026#34;ResourceClass\u0026#34; = \u0026#34;SingleNode\u0026#34; } } you will also need to add the following to the data.tf file so that databricks provide can retrieve some information dynamically for the provisioning. data.tf\ndata \u0026#34;databricks_spark_version\u0026#34; \u0026#34;latest\u0026#34; {} data \u0026#34;databricks_node_type\u0026#34; \u0026#34;smallest\u0026#34; { local_disk = true } Quick explanation The cluster we provisioned is single cluster which is capable enough to help to complete the lab.\nOnce you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars followed by:\nterraform apply -auto-approve -var-file=values.tfvars Lets login to our databricks instance and validate the cluster is in place. Part 4 ADLS + Keyvault with Private endpoint Lets create ADLS account and Keyvault in this part with private endpoints and private DNS zones. private DNS zone is required so that a Azure private link DNS address could be resolved to the private IP address for the private endpoint.\n4.1 Storage account with private endpoint and private DNS zone This part shouldnt be too bad, a storage account with private endpoint deployed to the spoke network.\nFirst need to add the following to the locals block:\nstor = \u0026#34;${local.org}stor\u0026#34; stor_pe = \u0026#34;${local.stor}pe\u0026#34; stor_prv_con = \u0026#34;${local.stor}prvcon\u0026#34; kv = \u0026#34;${local.org}-kv\u0026#34; kv_pe = \u0026#34;${local.kv}pe\u0026#34; kv_prv_con = \u0026#34;${local.kv}prvcon\u0026#34; Then add the following to main.tf fileÔºö\nresource \u0026#34;azurerm_storage_account\u0026#34; \u0026#34;stor\u0026#34; { name = local.stor resource_group_name = local.rg location = local.rg_location account_tier = \u0026#34;Standard\u0026#34; account_replication_type = \u0026#34;LRS\u0026#34; is_hns_enabled = true network_rules { default_action = \u0026#34;Deny\u0026#34; ip_rules = [data.external.my_ip.result.ip] } } resource \u0026#34;azurerm_storage_container\u0026#34; \u0026#34;container\u0026#34; { name = \u0026#34;land\u0026#34; storage_account_name = azurerm_storage_account.stor.name container_access_type = \u0026#34;private\u0026#34; } resource \u0026#34;azurerm_private_endpoint\u0026#34; \u0026#34;stor_pe\u0026#34; { name = local.stor_pe location = local.rg_location resource_group_name = local.rg subnet_id = azurerm_subnet.resource_stor.id private_service_connection { name = local.stor_prv_con private_connection_resource_id = azurerm_storage_account.stor.id is_manual_connection = false subresource_names = [\u0026#34;dfs\u0026#34;] } } resource \u0026#34;azurerm_private_dns_zone\u0026#34; \u0026#34;stor_dfs\u0026#34; { name = \u0026#34;privatelink.dfs.core.windows.net\u0026#34; resource_group_name = local.rg } resource \u0026#34;azurerm_private_dns_zone_virtual_network_link\u0026#34; \u0026#34;dbr_vnet_link_stor\u0026#34; { name = \u0026#34;dbr_vnet_link\u0026#34; resource_group_name = local.rg private_dns_zone_name = azurerm_private_dns_zone.stor_dfs.name virtual_network_id = azurerm_virtual_network.vnet_dbr.id } resource \u0026#34;azurerm_private_dns_a_record\u0026#34; \u0026#34;storpe_dns\u0026#34; { name = local.stor zone_name = azurerm_private_dns_zone.stor_dfs.name resource_group_name = local.rg ttl = 300 records = [azurerm_private_endpoint.stor_pe.private_service_connection.0.private_ip_address] } Quick explanation Line 11-14 defined the firewall rules so that we are not open our storage account to public by default. Note the \u0026ldquo;data.external.my_ip.result.ip\u0026rdquo;, because we still want to be operate the storage account from out local machine hence the data block retrieve your current public ip and inject that to network policy. You will also need to add the following to your data.tf fileÔºö\ndata \u0026#34;external\u0026#34; \u0026#34;my_ip\u0026#34; { program = [\u0026#34;curl\u0026#34;, \u0026#34;https://api.ipify.org?format=json\u0026#34;] } Line 23-35 created the private endpoint targeting the storage account resource. Line 37-47 creates a Azure private dns zone for ADLS, and link that to the dbr network, so that DNS request from the dbr network could use this for name resolution. Line 38 specifies the top level domain name for the storage account in the private DNZ zone. Line 49-55 creates a private DNS record using the private ip address of the network interface as part of private endpoint provisioning.  Note when creating private endpoint, there will be a network interface attached to it which gets a private ip address allocated in the subnet range. \n4.2 Keyvault with private endpoint and private DNS zone This one is not too bad either, keyvualt with private endpoint deployed to the same spoke network with similar setup on the private DNS zone, vNet linkage and dns records\nresource \u0026#34;azurerm_key_vault\u0026#34; \u0026#34;kv\u0026#34; { name = local.kv location = local.rg_location resource_group_name = local.rg enabled_for_disk_encryption = true tenant_id = data.azurerm_client_config.current.tenant_id soft_delete_retention_days = 7 purge_protection_enabled = false sku_name = \u0026#34;standard\u0026#34; network_acls { default_action = \u0026#34;Deny\u0026#34; bypass = \u0026#34;AzureServices\u0026#34; ip_rules = [data.external.my_ip.result.ip] } } resource \u0026#34;azurerm_private_endpoint\u0026#34; \u0026#34;kv_pe\u0026#34; { name = local.kv_pe location = local.rg_location resource_group_name = local.rg subnet_id = azurerm_subnet.resource_kv.id private_service_connection { name = local.kv_prv_con private_connection_resource_id = azurerm_key_vault.kv.id is_manual_connection = false subresource_names = [\u0026#34;Vault\u0026#34;] } } resource \u0026#34;azurerm_private_dns_zone\u0026#34; \u0026#34;kv\u0026#34; { name = \u0026#34;privatelink.vaultcore.azure.net\u0026#34; resource_group_name = local.rg } resource \u0026#34;azurerm_private_dns_zone_virtual_network_link\u0026#34; \u0026#34;dbr_vnet_link_kv\u0026#34; { name = \u0026#34;dbr_vnet_link_kv\u0026#34; resource_group_name = local.rg private_dns_zone_name = azurerm_private_dns_zone.kv.name virtual_network_id = azurerm_virtual_network.vnet_dbr.id } resource \u0026#34;azurerm_private_dns_a_record\u0026#34; \u0026#34;kvpe_dns\u0026#34; { name = local.kv zone_name = azurerm_private_dns_zone.kv.name resource_group_name = local.rg ttl = 300 records = [azurerm_private_endpoint.kv_pe.private_service_connection.0.private_ip_address] } Once you have all the code in, lets create resources in a iterative fashion by running\nterraform plan -var-file=values.tfvars followed by:\nterraform apply -auto-approve -var-file=values.tfvars Part 5 Databricks configuration - Keyvault backed secret scope To create the keyvault backed secret scope, terraform provides a handy resource to do this. add the following to the main.tf files to achieve this.\nresource \u0026#34;databricks_secret_scope\u0026#34; \u0026#34;kv\u0026#34; { name = \u0026#34;keyvault-managed\u0026#34; keyvault_metadata { resource_id = azurerm_key_vault.kv.id dns_name = azurerm_key_vault.kv.vault_uri } } The tricky part here is that according to terraform this is only supported by azure cli authentication but NOT with service principal authentication. You will see error like this if service principal auth is used: what you will need to do is to comment out the azurerm authentication part in the providers.tf and then login to Azure using\naz login If you haven\u0026rsquo;t installed Az cli following this to install and read more details of this limitation here.\nPart 6 Run notebook to validate Let put some python code in and test the connectivity and DNS resolutions. The first and most important task is to validate the connectivity and DNS resolution. You can run this code snippet in a notebook to check them.\nimport socket stor = socket.gethostbyname_ex(\u0026#34;blkstor.dfs.core.windows.net\u0026#34;) print (\u0026#34;\\n\\nThe IP Address of the Domain Name is: \u0026#34; + repr(stor)) kv = socket.gethostbyname_ex(\u0026#34;blk-kv.vault.azure.net\u0026#34;) print (\u0026#34;\\n\\nThe IP Address of the Domain Name is: \u0026#34; + repr(kv)) If all setup correctly you should be able to see that the DNS name is resolved with a private IP address. If you are interest in further testing to read some file from ADLS you could use the following snippet to do so.\ndbutils.widgets.text(\u0026#34;adls_name\u0026#34;, \u0026#34;blkstor\u0026#34;, \u0026#34;adls_name\u0026#34;) dbutils.widgets.text(\u0026#34;adls_container\u0026#34;, \u0026#34;land\u0026#34;, \u0026#34;adls_container\u0026#34;) # COMMAND ---------- adls_name = dbutils.widgets.get(\u0026#34;adls_name\u0026#34;) adls_container = dbutils.widgets.get(\u0026#34;adls_container\u0026#34;) # COMMAND ---------- spark.conf.set(\u0026#34;fs.azure.account.auth.type\u0026#34;, \u0026#34;OAuth\u0026#34;) spark.conf.set(\u0026#34;fs.azure.account.oauth.provider.type\u0026#34;, \u0026#34;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\u0026#34;) spark.conf.set(\u0026#34;fs.azure.account.oauth2.client.id\u0026#34;, dbutils.secrets.get(scope=\u0026#34;keyvault-managed\u0026#34;,key=\u0026#34;sp-id\u0026#34;)) spark.conf.set(\u0026#34;fs.azure.account.oauth2.client.secret\u0026#34;, dbutils.secrets.get(scope=\u0026#34;keyvault-managed\u0026#34;,key=\u0026#34;sp-secret\u0026#34;)) spark.conf.set(\u0026#34;fs.azure.account.oauth2.client.endpoint\u0026#34;, \u0026#34;https://login.microsoftonline.com/xxxx-xxx-xxxxxx-xxxxxx/oauth2/token\u0026#34;) # COMMAND ---------- df = spark.read.json(f\u0026#34;abfss://{adls_container}@{adls_name}.dfs.core.windows.net/pubapis.json\u0026#34;) # COMMAND ---------- df.show() What this notebook does is quite simple, it grabs the secret of a service principal from keyvault secret scope and use that to auth to Azure. Once authorized it will read a json file into dataframe. Please seethe outcome of the notebook run in the following screen shot. To be able to achieve this, you will need the following:\n another service principal that has the role \u0026ldquo;Storage blob contributor\u0026rdquo; on the storage account having the application id and secret saved in the Keyvault we created as \u0026ldquo;sp-id\u0026rdquo; and \u0026ldquo;sp-secret\u0026rdquo;  I have included these two notebooks in the repo, you will find them in the notebooks folder.\nTo deploy the notebooks to our databricks workspace, you could add the following to your main.tf file as part of the deployment.\nresource \u0026#34;databricks_notebook\u0026#34; \u0026#34;notbooks\u0026#34; { for_each = fileset(\u0026#34;${path.module}/notebooks\u0026#34;, \u0026#34;*\u0026#34;) source = \u0026#34;${path.module}/notebooks/${each.key}\u0026#34; path = \u0026#34;/validation/${element(split(\u0026#34;.\u0026#34;, each.key), 0)}\u0026#34; language = \u0026#34;PYTHON\u0026#34; } This block iterate through the notebooks folder and deploy the notebook in a folder called \u0026ldquo;validated\u0026rdquo; in the workspace.  Note usually you would not deploy your notebooks as part of IaC efforts, these would otherwise be regarded as the \"application\" artifacts. But for simplicity of our demo, they are deployed in the same IaC code flow. \nAt this point our lab has reached the end, hope you enjoyed the extra efforts needed to make your databricks cluster more secure.\nIf like the content please leave your comments below, if you fine issues with the content, please also comment below. Thanks for your time and patience with me !!\n","permalink":"https://blacklabnz.github.io/posts/databricks-secure/","summary":"What an interesting topic I had recently regarding on security hardening Databricks using Secure cluster connectivity + vNet injection.\nThis configuration will allow the cluster to access Azure Data Lake Storage (I know right ?! what a popular combination!) and keyvault with private endpoint.\nIn this post, in a lab environment, we will find out how we can put Databricks cluster inside existing Azure virtual network and access private endpoint deployed inside it.","title":"Secure Databricks cluster with vNet injection and access resources via Azure private endpoint"},{"content":"Hi there, great to meet you here. My name is NeilÔºåcurrently working as a Data Engineer operating in the cloud.\nPreviously I was working in DevOps capacity, focusing on building useful CICD pipelines, automations, APIs and else.\nThrough out my career, I\u0026rsquo;d like to share some tips and tricks which I hope you find helpful !\n","permalink":"https://blacklabnz.github.io/about/","summary":"Hi there, great to meet you here. My name is NeilÔºåcurrently working as a Data Engineer operating in the cloud.\nPreviously I was working in DevOps capacity, focusing on building useful CICD pipelines, automations, APIs and else.\nThrough out my career, I\u0026rsquo;d like to share some tips and tricks which I hope you find helpful !","title":"About"}]